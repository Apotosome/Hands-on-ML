{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import time\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def initialization(seed=42):\n",
    "    keras.backend.clear_session()\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001-01-01\n",
      "01, 0001\n",
      "0001-01-01\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    " \n",
    "dt=date.fromordinal( 1 )\n",
    "print(dt)\n",
    " \n",
    "print( dt.strftime( \"%d, %Y\" ) )\n",
    "print(  dt.isoformat() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    " \n",
    "# cannot use strftime()'s %B format since it depends on the locale\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "def random_dates( n_dates ):\n",
    "    min_date = date(1000,1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "    \n",
    "    ordinals = np.random.randint( max_date-min_date, size=n_dates ) + min_date\n",
    "    dates = [ date.fromordinal( ordinal) for ordinal in ordinals ]\n",
    "    x = [ MONTHS[dt.month-1] + \" \" + dt.strftime( \"%d, %Y\" ) for dt in dates ]\n",
    "    y = [ dt.isoformat() for dt in dates ]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                    Target                   \n",
      "--------------------------------------------------\n",
      "September 20, 7075       7075-09-20               \n",
      "May 15, 8579             8579-05-15               \n",
      "January 11, 7103         7103-01-11               \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    " \n",
    "n_dates = 3\n",
    "x_example, y_example = random_dates( n_dates )\n",
    "print( \"{:25s}{:25s}\".format(\"Input\", \"Target\") )\n",
    "print( \"-\"*50 )\n",
    "for idx in range(n_dates):\n",
    "    print( \"{:25s}{:25s}\".format(x_example[idx], y_example[idx]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ,0123456789ADFJMNOSabceghilmnoprstuvy'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_CHARS = \"\".join( sorted( set( \"\".join(MONTHS) \n",
    "                                    + \"0123456789, \" )\n",
    "                             ) )\n",
    "INPUT_CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 23, 31, 34, 23, 28, 21, 23, 32, 0, 4, 2, 1, 0, 9, 2, 9, 7]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def date_str_to_ids( date_str, chars=INPUT_CHARS ):\n",
    "    return [ chars.index(c) for c in date_str ]\n",
    " \n",
    "date_str_to_ids(x_example[0], INPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 0, 7, 5, 10, 0, 9, 10, 2, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_CHARS = \"0123456789-\"\n",
    "date_str_to_ids( y_example[0], OUTPUT_CHARS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs( date_strs, chars=INPUT_CHARS ):            #ragg #veriable length\n",
    "    X_ids = [ date_str_to_ids(dt, chars) for dt in date_strs ]# [[nested_list_veriable_length],[nested_list]...]\n",
    "    X = tf.ragged.constant( X_ids, ragged_rank=1 )\n",
    "    return (X+1).to_tensor() # +1 for id start from 1\n",
    " \n",
    "def create_dataset( n_dates ):\n",
    "    x,y = random_dates(n_dates)\n",
    "    return prepare_date_strs(x, INPUT_CHARS),\\\n",
    "           prepare_date_strs(y, OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 8,  1,  8,  6, 11,  1, 10, 11,  3,  1], dtype=int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    " \n",
    "X_train, Y_train = create_dataset( 10000 )\n",
    "X_valid, Y_valid = create_dataset( 2000 )\n",
    "X_test, Y_test = create_dataset( 2000 )\n",
    " \n",
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000, 10), dtype=int32, numpy=\n",
       "array([[ 8,  1,  8, ..., 11,  3,  1],\n",
       "       [ 9,  6,  8, ..., 11,  2,  6],\n",
       "       [ 8,  2,  1, ..., 11,  2,  2],\n",
       "       ...,\n",
       "       [10,  8,  7, ..., 11,  4,  1],\n",
       "       [ 2,  2,  3, ..., 11,  3,  8],\n",
       "       [ 8,  9,  4, ..., 11,  3, 10]], dtype=int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1 #==12\n",
    " \n",
    "def shifted_output_sequences(Y):\n",
    "    sos_tokens = tf.fill( dims=(len(Y),1), \n",
    "                          value=sos_id )\n",
    "    return tf.concat([ sos_tokens, Y[:,:-1] ],\n",
    "                       axis=1 )\n",
    " \n",
    "X_train_decoder = shifted_output_sequences(Y_train)\n",
    "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
    "X_test_decoder = shifted_output_sequences(Y_test)\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 32)     1248        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 32)     416         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 128), (None, 82432       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "basic_decoder (BasicDecoder)    (BasicDecoderOutput( 83980       embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 12)     0           basic_decoder[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 168,076\n",
      "Trainable params: 168,076\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# pip install tensorflow-addons\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "units = 128\n",
    "\n",
    "################################# encoder\n",
    "encoder_inputs = keras.layers.Input(shape=[None],\n",
    "                                    dtype=np.int32)  # None: num_time_steps\n",
    "\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "# INPUT_CHARS = ' ,0123456789ADFJMNOSabceghilmnoprstuvy'\n",
    "# len(INPUT_CHARS) = 38\n",
    "encoder_embeddings = keras.layers.Embedding(\n",
    "    input_dim=len(INPUT_CHARS) +\n",
    "    1,  #+1 since (X+1).to_tensor() #+1 for id start from 1\n",
    "    output_dim=encoder_embedding_size)(encoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(units, return_state=True)  # return_sequences=False\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "################################# decoder\n",
    "# OUTPUT_CHARS = '0123456789-'\n",
    "# len(OUTPUT_CHARS) = 11\n",
    "decoder_inputs = keras.layers.Input(shape=[None],\n",
    "                                    dtype=np.int32)  # None: num_time_steps\n",
    "decoder_embedding_layer = keras.layers.Embedding(  # +1 again for 'SOS'\n",
    "    input_dim=len(OUTPUT_CHARS) + 2,  # +1 for id start from 1 \n",
    "    output_dim=decoder_embedding_size)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "# why uses keras.layers.LSTMCell? During inference, we use one step output as next step input\n",
    "# keras.layers.LSTMCell processes one step within the whole time sequence input\n",
    "decoder_cell = keras.layers.LSTMCell(units)  # one step or one word\n",
    "#+1 since (X+1).to_tensor() # +1 for id start from 1 and we don't need to +1 again for predicting 'sos' with 0 probability\n",
    "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "# https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq/TrainingSampler\n",
    "# A training sampler that simply reads its inputs.\n",
    "# its role is to tell the decoder at each step what it should pretend the\n",
    "# previous output was.\n",
    "# During inference, this should be the embedding of the token that was actually output\n",
    "# During training, it should be the embedding of the previous target token\n",
    "# time_major : Python bool. Whether the tensors in inputs are time major.\n",
    "#              If False (default), they are assumed to be batch major.\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "# In tfa.seq2seq.BasicDecoder\n",
    "# The tfa.seq2seq.Sampler instance passed as argument is responsible to\n",
    "# sample from the output distribution and\n",
    "# produce the input for the next decoding step.\n",
    "# https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq/BasicDecoder\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n",
    "                                                 sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings,\n",
    "    initial_state=encoder_state,\n",
    "    # sequence_length = sequence_lengths\n",
    ")\n",
    "\n",
    "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n",
    "#final_outputs.rnn_outputs access to the logits ==>\"softmax\" for normalization==>Y_proba\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                           outputs=[Y_proba])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 18)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 18, 32)       1248        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 10, 32)       416         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 82432       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "basic_decoder_1 (BasicDecoder)  (BasicDecoderOutput( 83980       embedding_3[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 12)     0           basic_decoder_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 168,076\n",
      "Trainable params: 168,076\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# pip install tensorflow-addons\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    " \n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    " \n",
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "units = 128\n",
    " \n",
    "################################# encoder \n",
    "encoder_inputs = keras.layers.Input( shape=[18], dtype=np.int32 )# 18: num_time_steps\n",
    " \n",
    "sequence_lengths = keras.layers.Input( shape=[], dtype=np.int32 )\n",
    "# INPUT_CHARS = ' ,0123456789ADFJMNOSabceghilmnoprstuvy'\n",
    "# len(INPUT_CHARS) = 38\n",
    "encoder_embeddings = keras.layers.Embedding(\n",
    "                        input_dim = len(INPUT_CHARS)+1, #+1 since (X+1).to_tensor() #+1 for id start from 1\n",
    "                        output_dim=encoder_embedding_size\n",
    "                     )(encoder_inputs)\n",
    " \n",
    "encoder = keras.layers.LSTM(units, return_state=True) # return_sequences=False\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    " \n",
    " \n",
    "################################# decoder\n",
    "# OUTPUT_CHARS = '0123456789-'\n",
    "# len(OUTPUT_CHARS) = 11\n",
    "decoder_inputs = keras.layers.Input( shape=[10], dtype=np.int32 )# 10: num_time_steps\n",
    " \n",
    "decoder_embedding_layer = keras.layers.Embedding(           # +1 again for 'SOS'\n",
    "                            input_dim = len(OUTPUT_CHARS)+2,# +1 for id start from 1 \n",
    "                            output_dim=decoder_embedding_size\n",
    "                          )\n",
    "decoder_embeddings = decoder_embedding_layer( decoder_inputs )\n",
    " \n",
    "# why uses keras.layers.LSTMCell? During inference, we use one step output as next step input\n",
    "# keras.layers.LSTMCell processes one step within the whole time sequence input\n",
    "decoder_cell = keras.layers.LSTMCell(units) # one step or one word\n",
    "#+1 since (X+1).to_tensor() # +1 for id start from 1 and we don't need to +1 again for predicting 'sos' with 0 probability\n",
    "output_layer = keras.layers.Dense( len(OUTPUT_CHARS)+1 )\n",
    "# https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq/TrainingSampler\n",
    "# A training sampler that simply reads its inputs.\n",
    "# its role is to tell the decoder at each step what it should pretend the \n",
    "# previous output was. \n",
    "# During inference, this should be the embedding of the token that was actually output \n",
    "# During training, it should be the embedding of the previous target token\n",
    "# time_major : Python bool. Whether the tensors in inputs are time major. \n",
    "#              If False (default), they are assumed to be batch major.\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "# In tfa.seq2seq.BasicDecoder\n",
    "# The tfa.seq2seq.Sampler instance passed as argument is responsible to \n",
    "# sample from the output distribution and \n",
    "# produce the input for the next decoding step. \n",
    "# https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq/BasicDecoder\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder( decoder_cell,\n",
    "                                                  sampler,\n",
    "                                                  output_layer=output_layer )\n",
    "final_outputs, final_state, final_sequence_lengths = decoder( decoder_embeddings,\n",
    "                                                              initial_state=encoder_state,\n",
    "                                                              # sequence_length = sequence_lengths\n",
    "                                                            )\n",
    " \n",
    "Y_proba = keras.layers.Activation( \"softmax\" )( final_outputs.rnn_output )\n",
    " \n",
    " \n",
    "model = keras.models.Model( inputs=[encoder_inputs, decoder_inputs],\n",
    "                            outputs=[Y_proba] )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "313/313 [==============================] - 8s 17ms/step - loss: 1.6827 - accuracy: 0.3734 - val_loss: 1.4085 - val_accuracy: 0.4687\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.2088 - accuracy: 0.5491 - val_loss: 0.9161 - val_accuracy: 0.6637\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.6643 - accuracy: 0.7592 - val_loss: 0.3778 - val_accuracy: 0.8862\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2364 - accuracy: 0.9402 - val_loss: 0.1413 - val_accuracy: 0.9717\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 0.0733 - accuracy: 0.9917 - val_loss: 0.0433 - val_accuracy: 0.9980\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.0718 - accuracy: 0.9885 - val_loss: 0.0290 - val_accuracy: 0.9990\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0180 - accuracy: 0.9998 - val_loss: 0.0139 - val_accuracy: 0.9998\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.0104 - accuracy: 0.9999 - val_loss: 0.0110 - val_accuracy: 0.9997\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.0388 - accuracy: 0.9925 - val_loss: 0.0207 - val_accuracy: 0.9989\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.0085 - accuracy: 0.9999 - val_loss: 0.0063 - val_accuracy: 0.9999\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9999\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 0.9999\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 0.9999\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile( loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "               metrics=[\"accuracy\"] )\n",
    "history = model.fit( [X_train, X_train_decoder], Y_train, epochs=15,\n",
    "                     validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ids_to_date_strs( ids, chars=OUTPUT_CHARS ):\n",
    "                      # \" \" since (X+1).to_tensor() # +1 for id start from 1\n",
    "    return [ \"\".join([ (\" \"+chars)[index] for index in sequence ])\n",
    "             for sequence in ids ]\n",
    "             \n",
    "# since we use X = tf.ragged.constant( X_ids, ragged_rank=1 ) # 内部非均匀\n",
    "max_input_length = X_train.shape[1] # 18 \n",
    " \n",
    "def prepare_date_strs_padded( date_strs ):\n",
    "    X = prepare_date_strs( date_strs )\n",
    "    if X.shape[1] <max_input_length:\n",
    "        X = tf.pad(X, [ [ 0, 0 ], # not to fill the batch_size dimension\n",
    "                        [ 0, max_input_length-X.shape[1] ] # fill the sequences dimension(veriable length)\n",
    "                      ])\n",
    "    return X\n",
    " \n",
    "max_output_length = Y_train.shape[1] #10\n",
    "def predict_date_strs(date_strs): # during inference\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
    "    for index in range(max_output_length):\n",
    "        pad_size = max_output_length - Y_pred.shape[1]\n",
    "        X_decoder = tf.pad(Y_pred, [[0, 0], \n",
    "[0, pad_size]])\n",
    "        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n",
    "        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n",
    "        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n",
    "    return ids_to_date_strs(Y_pred[:, 1:])\n",
    " \n",
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ during inference\n",
    " \n",
    "inference_sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
    "                        embedding_fn = decoder_embedding_layer\n",
    "                      )\n",
    "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "                        decoder_cell, \n",
    "                        inference_sampler,                    ##########\n",
    "                        output_layer=output_layer,\n",
    "                        maximum_iterations = max_output_length##########\n",
    "                    )\n",
    "batch_size = tf.shape(encoder_inputs)[:1]\n",
    "start_tokens = tf.fill( dims=batch_size, value=sos_id )##############\n",
    "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
    "    start_tokens,# decoder_cell # keras.layers.LSTMCell(units) # one step or one word\n",
    "    initial_state = encoder_state,\n",
    "    start_tokens=start_tokens,\n",
    "    end_token=0\n",
    ")\n",
    "# Y_proba = keras.layers.Activation( \"softmax\" )( final_outputs.rnn_output )\n",
    "# final_outputs.rnn_outputs access to the logits ==>\"softmax\" for normalization==>Y_proba\n",
    "# sample_id is the argmax of the rnn_output\n",
    "inference_model = keras.models.Model( inputs=[encoder_inputs],\n",
    "                                      outputs=[final_outputs.sample_id]#######not outputs=[Y_proba]\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
    "    # \" \" since we are using 0 as the padding token ID\n",
    "    return [\n",
    "        \"\".join([(\" \" + chars)[index] for index in sequence])\n",
    "        for sequence in ids\n",
    "    ]\n",
    "\n",
    "\n",
    "# since we use X = tf.ragged.constant( X_ids, ragged_rank=1 ) # 内部非均匀\n",
    "max_input_length = X_train.shape[1]  # 18\n",
    "\n",
    "\n",
    "def prepare_date_strs_padded(date_strs):\n",
    "    X = prepare_date_strs(date_strs)\n",
    "    if X.shape[1] < max_input_length:\n",
    "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
    "    return X\n",
    "\n",
    "\n",
    "### max_output_length = Y_train.shape[1] #10\n",
    "\n",
    "\n",
    "def fast_predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = inference_model.predict(X)\n",
    "    return ids_to_date_strs(Y_pred)\n",
    "\n",
    "\n",
    "fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "目录",
   "title_sidebar": "目录",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
