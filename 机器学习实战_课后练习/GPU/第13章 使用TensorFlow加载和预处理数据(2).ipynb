{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习题 Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:为什么要使用`Data API` ？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ingesting a large dataset and preprocessing it** efficiently can be a complex engineering challenge. **The Data API makes it fairly simple.** It offers many features, including loading data from various sources (such as text or binary files), reading data in parallel from multiple sources, transforming it, interleaving the records, shuffling the data, batching it, and prefetching it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:将大数据分成多个文件有什么好处？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Splitting a large dataset into multiple files makes it possible to **shuffle it at a coarse level before shuffling it at a finer level using a shuffling buffer.** \n",
    "- It also makes it possible to **handle huge datasets** that do not fit on a single machine. It’s also simpler to manipulate thousands of small files rather than one huge file; for example, it’s easier to split the data into multiple subsets. \n",
    "- Lastly, if **the data is split across multiple files spread across multiple servers**, it is possible to download several files from different servers simultaneously, which **improves the bandwidth usage.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:训练中，如何断定输入流水线是瓶颈？如何处理瓶颈？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can **use TensorBoard to visualize profiling data**: if the GPU is not fully utilized then your input pipeline is likely to be the bottleneck. You can fix it by making sure it **reads and preprocesses the data in multiple threads in parallel, and ensuring it prefetches a few batches.** If this is insufficient to get your GPU to 100% usage during training, make sure your **preprocessing code is optimized.** You can also try **saving the dataset into multiple TFRecord files**, and if necessary **perform some of the preprocessing ahead of time** so that it does not need to be done on the fly during training (TF Transform can help with this). If necessary, **use a machine with more CPU and RAM**, and ensure that the GPU bandwidth is large enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:可以将任何二进制数据存入TFRecord文件吗，还是只能保存序列化的协议缓冲区？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Waiting for replenishment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:为什么要将所有数据转换为`Example protobuf格式`？为什么不使用自己的`protobuf`定义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Waiting for replenishment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:使用`TFRecord`时，什么时候要压缩？为什么不系统化的做？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Waiting for replenishment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:对数据预处理,可以在编写数据文件时，或在`tf.data`流水线内，或在模型内的预处理层中，或使用`TF Transform`。这几种方法各有什么优缺点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Let’s look at the pros and cons of each preprocessing option:\n",
    "- If you preprocess the data **when creating the data files**.\n",
    "    - the training script will  **run faster** , since it will not have to perform preprocessing on the fly. In some cases, the preprocessed data will also be much smaller than the orig inal data, so you **can save some space and speed up downloads**. It may also be **helpful to materialize the preprocessed data**, for example to  inspect it or archive it. \n",
    "    - However, this approach has a few cons. First, **it’s not easy to experiment with various preprocessing logics** if you need to generate a preprocessed dataset for each variant. Second, if you want to perform data augmentation, you have to materialize many variants of your dataset, which **will use a large amount of disk space and take a lot of time to generate.** Lastly, the trained model will expect preprocessed data, so you will **have to add preprocessing code in your application before it calls the model.**\n",
    "- If the data is preprocessed with the **`tf.data` pipeline**\n",
    "    - **it’s much easier to tweak the preprocessing logic and apply data augmentation.** Also, tf.data makes it easy to **build highly efficient preprocessing pipelines** (e.g., with multithreading and prefetching). \n",
    "    - However, preprocessing the data this way will **slow down training.** Moreover, each training instance will be preprocessed once per epoch rather than just once if the data was preprocessed when creating the data files. Lastly, **the trained model will still expect preprocessed data.**\n",
    "- If you **add preprocessing layers to your model**\n",
    "    - you will **only have to write the preprocessing code once** for both training and inference. If your model needs to be deployed to many different platforms, you will not need to write the preprocessing code multiple times. Plus, you **will not run the risk of using the wrong preprocessing logic for your model**, since it will be part of the model. \n",
    "    - On the downside, preprocessing the data will **slow down training**, and each training instance will be preprocessed once per epoch. Moreover, by default the preprocessing operations will run on the GPU for the current batch (you will not benefit from parallel preprocessing on the CPU, and prefetching). Fortunately, the upcoming Keras preprocessing layers should be able to lift the preprocessing operations from the preprocessing layers and run them as part of the tf.data pipeline, so you will benefit from multithreaded execution on the CPU and prefetching.\n",
    "- Lastly, using TF Transform for preprocessing \n",
    "    - gives you many of the benefits from the previous options: the preprocessed data is materialized, each instance is preprocessed just once (**speeding up training**), and **preprocessing layers get generated automatically** so you only need to write the preprocessing code once. \n",
    "    - The main drawback is the fact that **you need to learn how to use this tool.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:列举一些可用于编码分类特征的常用技术。文本呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Let’s look at how to encode categorical features and text: \n",
    "- To encode a categorical feature that has a natural order, such as a movie rating (e.g., “bad,” “average,” “good”), the simplest option is to use ordinal encoding: sort the categories in their natural order and map each category to its rank (e.g., “bad” maps to 0, “average” maps to 1, and “good” maps to 2). However, most categorical features don’t have such a natural order. For example, there’s no natural order for professions or countries. In this case, you can use one-hot encoding or, if there are many categories, embeddings.\n",
    "- For text, one option is to use a bag-of-words representation: a sentence is represented by a vector counting the counts of each possible word. Since common words are usually not very important, you’ll want to use TF-IDF to reduce their weight. Instead of counting words, it is also common to count n-grams, which are sequences of n consecutive words—nice and simple. Alternatively, you can encode each word using word embeddings, possibly pretrained. Rather than encoding words, it is also possible to encode each letter, or subword tokens (e.g., splitting “smartest” into “smart” and “est”). These last two options are discussed in Chapter 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:完成以下任务\n",
    ">1. 加载 `Fashion MNIST`数据集；将其分为训练集、验证集和测试集.打乱训练集次序：\n",
    "2. 将毎个数据集保存到多个 `TFRecord`文件中。 每个记录都 应该是具有两个特征的序列化 `Example protobuf`：序列化的图像(使用`tf.io.serialize_tensor()`序列化每个图像)和标签。\n",
    "3. 使用`tf.data`为每个集合 创建一个有效的数据集。\n",
    "4. 使用 `Keras`模型训练这些数据集，包括预处理层来标准化每个输入特征。尝试使输入流水线尽可能高效，使用 `Tensorboard`来可视化数据分析."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def initialization():\n",
    "    keras.backend.clear_session()\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the `Fashion MNIST` dataset;split it into a training set, a validation set, and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_MNIST = keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full,y_train_full), (X_test,y_test) = fashion_MNIST\n",
    "\n",
    "X_val, y_val = X_train_full[:5000], y_train_full[:5000]\n",
    "X_train, y_train = X_train_full[5000:], y_train_full[5000:]\n",
    "\n",
    "train_set = (X_train,y_train)\n",
    "val_set = (X_val, y_val)\n",
    "test_set = (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Shuffle the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 10:30:23.421267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 10:30:23.443495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 10:30:23.443625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 10:30:23.444138: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-26 10:30:23.444622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 10:30:23.444732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 10:30:23.444826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 10:30:23.755864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 10:30:23.756017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 10:30:23.756122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 10:30:23.756207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3067 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "initialization()\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices(train_set).shuffle(len(X_train))\n",
    "val_set = tf.data.Dataset.from_tensor_slices(val_set)\n",
    "test_set = tf.data.Dataset.from_tensor_slices(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Save each dataset to multiple TFRecord files.Each record should be a serialized `Example` protobuf with two features: the serialized image (use `tf.io.serialize_tensor()` to serialize each image), and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BytesList = tf.train.BytesList\n",
    "FloatList = tf.train.FloatList\n",
    "Int64List = tf.train.Int64List\n",
    "Feature = tf.train.Feature\n",
    "Features = tf.train.Features\n",
    "Example = tf.train.Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(image, label):\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "    #image_data = tf.io.encode_jpeg(image[..., np.newaxis])\n",
    "    return Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
    "                \"label\": Feature(int64_list=Int64List(value=[label])),\n",
    "            }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"image\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"\\010\\004\\022\\010\\022\\002\\010\\034\\022\\002\\010\\034\\\"\\220\\006\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\rI\\000\\000\\001\\004\\000\\000\\000\\000\\001\\001\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\003\\000$\\210\\177>6\\000\\000\\000\\001\\003\\004\\000\\000\\003\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\006\\000f\\314\\260\\206\\220{\\027\\000\\000\\000\\000\\014\\n\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\233\\354\\317\\262k\\234\\241m@\\027M\\202H\\017\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000E\\317\\337\\332\\330\\330\\243\\177yz\\222\\215X\\254B\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\001\\001\\000\\310\\350\\350\\351\\345\\337\\337\\327\\325\\244\\177{\\304\\345\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\267\\341\\330\\337\\344\\353\\343\\340\\336\\340\\335\\337\\365\\255\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\301\\344\\332\\325\\306\\264\\324\\322\\323\\325\\337\\334\\363\\312\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\003\\000\\014\\333\\334\\324\\332\\300\\251\\343\\320\\332\\340\\324\\342\\305\\3214\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\006\\000c\\364\\336\\334\\332\\313\\306\\335\\327\\325\\336\\334\\365w\\2478\\000\\000\\000\\000\\000\\000\\000\\000\\000\\004\\000\\0007\\354\\344\\346\\344\\360\\350\\325\\332\\337\\352\\331\\331\\321\\\\\\000\\000\\000\\001\\004\\006\\007\\002\\000\\000\\000\\000\\000\\355\\342\\331\\337\\336\\333\\336\\335\\330\\337\\345\\327\\332\\377M\\000\\000\\003\\000\\000\\000\\000\\000\\000\\000>\\221\\314\\344\\317\\325\\335\\332\\320\\323\\332\\340\\337\\333\\327\\340\\364\\237\\000\\000\\000\\000\\000\\022,Rk\\275\\344\\334\\336\\331\\342\\310\\315\\323\\346\\340\\352\\260\\274\\372\\370\\351\\356\\327\\000\\0009\\273\\320\\340\\335\\340\\320\\314\\326\\320\\321\\310\\237\\365\\301\\316\\337\\377\\377\\335\\352\\335\\323\\334\\350\\366\\000\\003\\312\\344\\340\\335\\323\\323\\326\\315\\315\\315\\334\\360P\\226\\377\\345\\335\\274\\232\\277\\322\\314\\321\\336\\344\\341\\000b\\351\\306\\322\\336\\345\\345\\352\\371\\334\\302\\327\\331\\361AIju\\250\\333\\335\\327\\331\\337\\337\\340\\345\\035K\\314\\324\\314\\301\\315\\323\\341\\330\\271\\305\\316\\306\\325\\360\\303\\343\\365\\357\\337\\332\\324\\321\\336\\334\\335\\346C0\\313\\267\\302\\325\\305\\271\\276\\302\\300\\312\\326\\333\\335\\334\\354\\341\\330\\307\\316\\272\\265\\261\\254\\265\\315\\316s\\000z\\333\\301\\263\\253\\267\\304\\314\\322\\325\\317\\323\\322\\310\\304\\302\\277\\303\\277\\306\\300\\260\\234\\247\\261\\322\\\\\\000\\000J\\275\\324\\277\\257\\254\\257\\265\\271\\274\\275\\274\\301\\306\\314\\321\\322\\322\\323\\274\\274\\302\\300\\330\\252\\000\\002\\000\\000\\000B\\310\\336\\355\\357\\362\\366\\363\\364\\335\\334\\301\\277\\263\\266\\266\\265\\260\\246\\250c:\\000\\000\\000\\000\\000\\000\\000\\000\\000(=,H)#\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"label\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 9\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for image, label in val_set.take(1):\n",
    "    print(create_example(image, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function saves a given dataset to a set of TFRecord files. \n",
    "\n",
    "The examples are written to the files in a round-robin fashion. To do this, we enumerate all the examples using the `dataset.enumerate()` method, and we compute `index % n_shards` to decide which file to write to. We use the standard `contextlib.ExitStack` class to make sure that all writers are properly closed whether or not an I/O error occurs while writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import ExitStack\n",
    "\n",
    "def write_tfrecords(name, dataset, n_shards=10):\n",
    "    paths = [\"{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_shards)\n",
    "             for index in range(n_shards)]\n",
    "    with ExitStack() as stack:\n",
    "        writers = [stack.enter_context(tf.io.TFRecordWriter(path))\n",
    "                   for path in paths]\n",
    "        for index, (image, label) in dataset.enumerate():\n",
    "            shard = index % n_shards\n",
    "            example = create_example(image, label)\n",
    "            writers[shard].write(example.SerializeToString())\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = write_tfrecords(\"../TFRecords/my_fashion_mnist.train\", train_set)\n",
    "val_filepaths = write_tfrecords(\"../TFRecords/my_fashion_mnist.valid\", val_set)\n",
    "test_filepaths = write_tfrecords(\"../TFRecords/my_fashion_mnist.test\", test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use `tf.data` to create an efficient dataset for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tfrecord):\n",
    "    feature_descriptions = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n",
    "    #image = tf.io.decode_jpeg(example[\"image\"])\n",
    "    image = tf.reshape(image, shape=[28, 28])\n",
    "    return image, example[\"label\"]\n",
    "\n",
    "def mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None,\n",
    "                  n_parse_threads=5, batch_size=32, cache=True):\n",
    "    dataset = tf.data.TFRecordDataset(filepaths,\n",
    "                                      num_parallel_reads=n_read_threads)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\n",
    "val_set = mnist_dataset(val_filepaths)\n",
    "test_set = mnist_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for X, y in train_set.take(1):\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(X[i].numpy(), cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(str(y[i].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use a Keras model to train these datasets, including a preprocessing layer to standardize each input feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs-self.means_) / (self.stds_+keras.backend.epsilon())\n",
    "    \n",
    "standardization = Standardization(input_shape=[28,28])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image_batches = train_set.take(100).map(\n",
    "                        lambda img, label : img)\n",
    "# sample_images = np.concatenate(\n",
    "#                     list(sample_image_batches.as_numpy_iterator()),\n",
    "#                     axis=0).astype(np.float32)\n",
    "# standardization.adapt(sample_images)\n",
    "\n",
    "# model = keras.models.Sequential([\n",
    "#     standardization,\n",
    "#     keras.layers.Flatten(),\n",
    "#     keras.layers.Dense(100, activation=\"relu\"),\n",
    "#     keras.layers.Dense(10, activation=\"softmax\")\n",
    "# ])\n",
    "# model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "#               optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# logs = os.path.join(os.curdir, \"my_logs\",\n",
    "#                     \"run_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "# tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "#     log_dir=logs, histogram_freq=1, profile_batch=10)\n",
    "\n",
    "# model.fit(train_set, epochs=5, validation_data=val_set,\n",
    "#           callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 48780), started 0:24:25 ago. (Use '!kill 48780' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4ce800964c5cc181\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4ce800964c5cc181\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:在本练习中,你将下载一个数据集,对其进行拆分,创建一个`tf.data.Dataset` 来加载并对其进行有效的预处理,然后构建和训练一个包含`Embedding`层的二进制分类模型.\n",
    ">1. 下载大型电影评论数据集,其中包含来自 Internet电影数据库的50 000条电影评论。数据有train和test两个目录,毎个目录包含一个带有12 500个肯定评论的pos子目录和一个带有12 500个否定评论的neg子目录。毎个评论均存储在单独的文本文件中。还有其他文件和文件夹(包括顶处理的词袋),但是在本练习中我们将忽略它.\n",
    "2. 将测试集分为一个验证集(15 000)和一个测试集(1 0000)\n",
    "3. 使用 `tf.data`为每个集合创建一个有效的数据集。\n",
    "4. 创建一个二进制分类模型,使用 `Textvectorization` 层对每个评论进行预处理。如果` Textvectorization`层尚不可用(或者你想挑战),请尝试创建自定义预处理层：你可以使用`tf.strings`包中的函数，例如，`1ower()`来使所有内容变为小写， `regex_replace()`将标点符号替换为空格， `split()`在空格上拆分单词。你应该使用査找表输出单词素引，该索引必在 `adapt()`方法中准备.\n",
    "5. 添加一个 `Embedding`层并计算每个评论的平均嵌入值，乘以单词数的平方根。然后将这种重新缩放的均值嵌入传递给模型的其余部分。 \n",
    "6. 训练模型，看看可以得到什么准确率。尝试优化你的流水线以使训练速度更快.\n",
    "7. 使用`TFDS`可以更轻松地加載相同的数据集：`tfds.1oad(\"imdb_reviews\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Download the Large Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84125825/84125825 [==============================] - 17s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/sora/.keras/datasets/aclImdb')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
    "FILENAME = \"aclImdb_v1.tar.gz\"\n",
    "filepath = keras.utils.get_file(fname=FILENAME, origin=DOWNLOAD_ROOT+FILENAME, extract=True)\n",
    "path = Path(filepath).parent / \"aclImdb\"   # 数据集根目录地址 \n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb/\n",
      "    README\n",
      "    imdb.vocab\n",
      "    imdbEr.txt\n",
      "    test/\n",
      "        labeledBow.feat\n",
      "        urls_neg.txt\n",
      "        urls_pos.txt\n",
      "        neg/\n",
      "            0_2.txt\n",
      "            10000_4.txt\n",
      "            10001_1.txt\n",
      "            ...\n",
      "        pos/\n",
      "            0_10.txt\n",
      "            10000_7.txt\n",
      "            10001_9.txt\n",
      "            ...\n",
      "    train/\n",
      "        labeledBow.feat\n",
      "        unsupBow.feat\n",
      "        urls_neg.txt\n",
      "        ...\n",
      "        unsup/\n",
      "            0_0.txt\n",
      "            10000_0.txt\n",
      "            10001_0.txt\n",
      "            ...\n",
      "        neg/\n",
      "            0_3.txt\n",
      "            10000_4.txt\n",
      "            10001_4.txt\n",
      "            ...\n",
      "        pos/\n",
      "            0_9.txt\n",
      "            10000_8.txt\n",
      "            10001_10.txt\n",
      "            ...\n"
     ]
    }
   ],
   "source": [
    "# 查看该数据集的树形目录\n",
    "for name, subdirs, files in os.walk(path):\n",
    "    indent = len(Path(name).parts) - len(path.parts)   # 2 -> 'train', 'pos'\n",
    "    print(\"    \" * indent + Path(name).parts[-1] + os.sep)  # os.sep -> '/'\n",
    "    for index, filename in enumerate(sorted(files)):\n",
    "        if index == 3:\n",
    "            print(\"    \" * (indent + 1) + \"...\")\n",
    "            break\n",
    "        print(\"    \" * (indent + 1) + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500, 12500, 12500)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def review_paths(dirpath):\n",
    "    return [str(path) for path in dirpath.glob(\"*.txt\")]   # 遍历目录\n",
    "\n",
    "train_pos = review_paths(path / \"train\" / \"pos\")\n",
    "train_neg = review_paths(path / \"train\" / \"neg\")\n",
    "test_val_pos = review_paths(path / \"test\" / \"pos\")\n",
    "test_val_neg = review_paths(path / \"test\" / \"neg\")\n",
    "\n",
    "len(train_pos), len(train_neg), len(test_val_pos), len(test_val_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. Split the test set into a validation set (15,000) and a test set (10,000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(test_val_pos)\n",
    "np.random.shuffle(test_val_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证集:15 000 ( pos 和 neg 各7 500)\n",
    "val_pos = test_val_pos[5000:]\n",
    "val_neg = test_val_neg[5000:]\n",
    "\n",
    "# 测试集:10 000 ( pos 和 neg 各5 000)\n",
    "test_pos = test_val_pos[:5000]\n",
    "test_neg = test_val_neg[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 5000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_pos), len(test_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Use `tf.data` to create an efficient dataset for each set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  If the dataset fits in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_dataset(filepaths_pos, filepaths_neg):\n",
    "    reviews = []\n",
    "    labels = []  \n",
    "    \n",
    "    # pos评论label标记为1 neg评价label为0\n",
    "    postive = (filepaths_pos, 1)\n",
    "    negative = (filepaths_neg, 0)\n",
    "    \n",
    "    for filepaths, label in (negative, postive):\n",
    "        for filepath in filepaths:\n",
    "            with open(filepath) as review_file:\n",
    "                reviews.append(review_file.read())   # 提取评论数据\n",
    "            labels.append(label)                     # 提取标签\n",
    "    \n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "                (tf.constant(reviews), tf.constant(labels))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for review, label in imdb_dataset(train_pos, train_neg):\n",
    "#    print(review)\n",
    "#    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for review, label in imdb_dataset(train_pos, train_neg).repeat(20): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遍历20次大概需要花费38秒."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  If the dataset does not fit in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于每条评论只占一行(使用`<br />`换行),因此我们可以使用`TextLineDataset`读取评论.否则,我们将不得不使用预处理输入文件(转化为`TFRcords`)\n",
    "\n",
    "对于非常大的数据集,使用`Apache Beam`等工具."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_dataset(filepaths_pos, filepaths_neg, n_read_threads=tf.data.AUTOTUNE):\n",
    "    dataset_neg = tf.data.TextLineDataset(filenames=filepaths_neg,\n",
    "                                          num_parallel_reads=n_read_threads)\n",
    "    dataset_neg = dataset_neg.map(lambda review:(review, 0))\n",
    "    \n",
    "    dataset_pos = tf.data.TextLineDataset(filenames=filepaths_pos,\n",
    "                                          num_parallel_reads=n_read_threads)\n",
    "    dataset_pos = dataset_pos.map(lambda review:(review, 1))\n",
    "    \n",
    "#     return tf.data.Dataset.from_tensor_slices(\n",
    "#                 (dataset_pos, dataset_neg)\n",
    "#             )    \n",
    "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.7 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for review, label in imdb_dataset(train_pos, train_neg).repeat(20): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，遍历数据集 20 次大约需要 43 秒。这要前面的慢得多，主要是因为数据集没有缓存在 RAM 中，因此必须在每个时期重新加载。\n",
    "\n",
    "在 `.repeat(10)` 之前添加 `.cache()`，耗时将和前面的一样快."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.7 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for review, label in imdb_dataset(train_pos, train_neg).cache().repeat(20): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- apply on the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\n",
    "val_set = imdb_dataset(val_pos, val_neg).batch(batch_size).prefetch(1)\n",
    "test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. Create a binary classification model, using a `TextVectorization` layer to preprocess each review. If the `TextVectorization` layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: you can use the functions in the `tf.strings` package, for example `lower()` to make everything lowercase, `regex_replace()` to replace punctuation with spaces, and `split()` to split words on spaces. You should use a lookup table to output word indices, which must be prepared in the `adapt()` method._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们先写一个函数对评论进行预处理，将它们裁剪为300个字符，将它们转换为小写，然后将`<br />`和所有非字母字符替换为空格，将评论拆分为单词，最后填充或裁剪每个评论所以它最终得到了恰好达到`n_words `长度."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_example = tf.constant([\"It's a great, great movie! I loved it.\",\n",
    "                         \"It was terrible, run away!!!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 2, 50], dtype=int32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(X_example) * tf.constant([1, 0]) + tf.constant([0,50])  # shape=[2,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, n_words=50):\n",
    "    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0,n_words])\n",
    "    Z = tf.strings.substr(X_batch, pos=0, len=300)  # 子字符串的操作\n",
    "    Z = tf.strings.lower(Z)  # 转换为小写\n",
    "    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \") # 正则表达式进行字符串替换\n",
    "    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n",
    "    Z = tf.strings.split(Z)  # 拆分为单词\n",
    "    return Z.to_tensor(shape=shape, default_value=b\"<pad>\") # 不足的位置由<pad>填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=string, numpy=\n",
       "array([[b'it', b's', b'a', b'great', b'great', b'movie'],\n",
       "       [b'it', b'was', b'terrible', b'run', b'away', b'<pad>']],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(X_example, n_words=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 现在让我们编写第二个函数，该函数将采用与`preprocess()`函数的输出格式相同的数据样本，并将输出最高 `max_size` 最常用词的列表，确保填充标记是第一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_vocabulary(data_sample, max_size=1000):\n",
    "    preprocessed_reviews = preprocess(data_sample).numpy()\n",
    "    counter = Counter()  # 初始化计数器\n",
    "    \n",
    "    for words in preprocessed_reviews:\n",
    "        for word in words:\n",
    "            if word !=  b'<pad>':\n",
    "                counter[word] += 1  \n",
    "    #print({word:count for word, count in counter.most_common()})\n",
    "    return [b\"<pad>\"] + [word for word, count in counter.most_common(max_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<pad>',\n",
       " b'it',\n",
       " b'great',\n",
       " b's',\n",
       " b'a',\n",
       " b'movie',\n",
       " b'i',\n",
       " b'loved',\n",
       " b'was',\n",
       " b'terrible',\n",
       " b'run',\n",
       " b'away']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vocabulary(X_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 现在我们准备好创建`TextVectorization`层。它的构造函数只保存超参数（`max_vocabulary_size`和 `n_oov_buckets`）。 `adapt()` 使用 `get_vocabulary()` 函数计算词汇数量，然后它构建一个`StaticVocabularyTable`（更多细节参见第 16 章）。 `call()` 预处理评论以获得每个评论的填充单词列表，然后它使用`StaticVocabularyTable`查找词汇表中每个单词的索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorization(keras.layers.Layer):\n",
    "    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        self.max_vocabulary_size = max_vocabulary_size\n",
    "        self.n_oov_buckets = n_oov_buckets\n",
    "\n",
    "    def adapt(self, data_sample):\n",
    "        self.vocab = get_vocabulary(data_sample, max_size=self.max_vocabulary_size)\n",
    "        words = tf.constant(self.vocab)\n",
    "        \n",
    "        word_ids = tf.range(len(self.vocab), dtype=tf.int64)  # X_example -> 12\n",
    "        vocab_init = tf.lookup.KeyValueTensorInitializer(keys=words, values=word_ids) # 为查找表创建一个初始化程序 \n",
    "        self.table = tf.lookup.StaticVocabularyTable(\n",
    "                        initializer=vocab_init, \n",
    "                        num_oov_buckets=self.n_oov_buckets) # 创建查找表\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        preprocessed_inputs = preprocess(inputs)\n",
    "        return self.table.lookup(preprocessed_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
       "array([[ 1,  3,  4,  2,  2,  5,  6,  7,  1,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 1,  8,  9, 10, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0]])>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization = TextVectorization()\n",
    "\n",
    "text_vectorization.adapt(data_sample=X_example)\n",
    "text_vectorization(X_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每条评论都经过清理和标记，然后每个单词都被编码为词汇表中的索引（所有 0 对应于 `<pad>` ）。\n",
    "\n",
    "\n",
    "现在让我们创建另一个`TextVectorization`层，让我们将其调整到完整的 IMDB 训练集（如果训练集不适合 RAM，我们可以通过调用 `train_set.take(500)` 使用较小的训练集样本）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocabulary_size = 1000\n",
    "n_oov_buckets = 100\n",
    "\n",
    "sample_review_batches = train_set.map(lambda review, label: review)\n",
    "sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),\n",
    "                                axis=0)\n",
    "\n",
    "text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,\n",
    "                                       input_shape=[])\n",
    "text_vectorization.adapt(sample_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
       "array([[  9,  14,   2,  64,  64,  12,   5, 256,   9,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  9,  13, 269, 531, 334,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization(X_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此可见，在`X_example`上词汇的`IDs`更大，因为词汇表中的词汇量更大."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<pad>', b'the', b'a', b'of', b'and', b'i', b'to', b'is', b'this', b'it']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.vocab[:10]  # 在评论中最常见的词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在要构建我们的模型，我们需要以某种方式对所有这些单词` IDs `进行编码。\n",
    "\n",
    "一种方法是创建词袋：对于每个评论，以及词汇表中的每个词，我们计算该词在评论中出现的次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])\n",
    "tf.one_hot(simple_example, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "array([[2., 2., 0., 1.],\n",
       "       [3., 0., 2., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个review有2次0，2次1，0次2，1次3，所以它的bag-of-words表示是[2, 2, 0, 1]。同理，第二个review有3次0，0次1，以此类推...\n",
    "\n",
    "让我们将这个逻辑包装在一个小的自定义层中，让我们对其进行测试。我们将删除单词 0 的计数，因为它对应于我们不关心的 `<pad>` 标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(keras.layers.Layer):\n",
    "    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        self.n_tokens = n_tokens\n",
    "    def call(self, inputs):\n",
    "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
    "        return tf.reduce_sum(one_hot, axis=1)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[2., 0., 1.],\n",
       "       [0., 2., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = BagOfWords(n_tokens=4)\n",
    "bag_of_words(simple_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在让我们为我们的训练集创建另一个具有合适词汇量的`BagOfWord`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocabulary_size = 1000\n",
    "n_oov_buckets = 100\n",
    "\n",
    "n_tokens = max_vocabulary_size + n_oov_buckets + 1   # add 1 for <pad>\n",
    "bag_of_words = BagOfWords(n_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来准备训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.5437 - binary_accuracy: 0.7162 - val_loss: 0.5167 - val_binary_accuracy: 0.7342\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.4672 - binary_accuracy: 0.7738 - val_loss: 0.5061 - val_binary_accuracy: 0.7422\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.4149 - binary_accuracy: 0.8078 - val_loss: 0.5189 - val_binary_accuracy: 0.7381\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.3428 - binary_accuracy: 0.8548 - val_loss: 0.5428 - val_binary_accuracy: 0.7306\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 0.2606 - binary_accuracy: 0.9056 - val_loss: 0.5920 - val_binary_accuracy: 0.7262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3384b1cc0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialization()\n",
    "model = keras.models.Sequential([\n",
    "    text_vectorization,  # 自定义TextVectorization预处理层\n",
    "    bag_of_words,        # 词袋编码层\n",
    "    keras.layers.Dense(100, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(1, activation=keras.activations.sigmoid),\n",
    "])\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer='nadam',\n",
    "              metrics=[keras.metrics.binary_accuracy])\n",
    "model.fit(train_set, epochs=5, validation_data=val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第一个 epoch 之后，我们在验证集上获得了大约 74.7% 的准确率，但之后模型没有取得显着进展。我们会在第 16 章做得更好。现在的重点只是使用 `tf.data` 和 `Keras` 预处理层执行有效的预处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  5. Add an `Embedding` layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对此,我们需要一个函数:计算每条评论的平均嵌入值，乘以单词数的平方根。\n",
    "\n",
    "对于每个句子。此函数需要计算 $M \\times \\sqrt N$\n",
    "- $M$ 是句子中所有词嵌入的平均值（不包括填充标记)\n",
    "- $N$ 是句子中的单词数（也不包括填充标记）.\n",
    "\n",
    "我们可以将  $M$ 重写为$\\dfrac{S}{N}$，\n",
    "- $S $是所有词嵌入的总和（我们是否在这个总和中包含填充标记并不重要，因为它们的表示是零向量）。 \n",
    "\n",
    "函数重写为$M \\times \\sqrt N = \\dfrac{S}{N} \\times \\sqrt N = \\dfrac{S}{\\sqrt N \\times \\sqrt N} \\times \\sqrt N= \\dfrac{S}{\\sqrt N}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_embedding(inputs):\n",
    "    not_pad = tf.math.count_nonzero(inputs, axis=-1)  # 用于统计数组中非零元素(不包括填充标记)的个数\n",
    "    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)    \n",
    "    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n",
    "    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[3.535534 , 4.9497476, 2.1213205],\n",
       "       [6.       , 0.       , 0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_example = tf.constant([[[1., 2., 3.], [4., 5., 0.], [0., 0., 0.]],\n",
    "                               [[6., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\n",
    "compute_mean_embedding(another_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n",
       "array([[3, 2, 0],\n",
       "       [1, 0, 0]])>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_pad = tf.math.count_nonzero(another_example, axis=-1)\n",
    "not_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=int64, numpy=\n",
       "array([[2],\n",
       "       [1]])>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)   \n",
    "n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们检查一下这是否正确。第一个评论包含 2 个词（最后一个标记是零向量，代表 `<pad> `标记）。让我们计算这两个词的平均嵌入，并将结果乘以 2 的平方根："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[3.535534 , 4.9497476, 2.1213202]], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(another_example[0:1, :2], axis=1) * tf.sqrt(2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在让我们检查第二条评论，它只包含一个词（我们忽略了两个填充标记）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[6., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(another_example[1:2, :1], axis=1) * tf.sqrt(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们准备好训练我们的最终模型。它和以前一样，除了我们用一个`Embedding`层替换了`BagOfWords`层，然后是一个`compute_mean_embedding`层的 `Lambda `层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 20\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    text_vectorization,\n",
    "    keras.layers.Embedding(input_dim=n_tokens,\n",
    "                           output_dim=embedding_size,\n",
    "                           mask_zero=True), # <pad> tokens => zero vectors\n",
    "    keras.layers.Lambda(compute_mean_embedding),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### f."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 6. Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 0.5553 - accuracy: 0.7093 - val_loss: 0.5164 - val_accuracy: 0.7398\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 0.4960 - accuracy: 0.7546 - val_loss: 0.5114 - val_accuracy: 0.7405\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 0.4872 - accuracy: 0.7601 - val_loss: 0.5098 - val_accuracy: 0.7397\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 0.4798 - accuracy: 0.7637 - val_loss: 0.5102 - val_accuracy: 0.7425\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 0.4726 - accuracy: 0.7656 - val_loss: 0.5102 - val_accuracy: 0.7396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa39e710fd0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=5, validation_data=val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该模型并没有使用嵌入更好（但我们会在第 16 章中做得更好）。流水线看起来足够快（我们之前对其进行了优化）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 7. Use TFDS to load the same dataset more easily: `tfds.load(\"imdb_reviews\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f5b088369e4fc0a221a27d9044d582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482f0d82ae9a47afb8d60f49d0ba219f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteGA5EGZ/imdb_reviews-train.tfrecord*...…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteGA5EGZ/imdb_reviews-test.tfrecord*...:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteGA5EGZ/imdb_reviews-unsupervised.tfrec…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(name=\"imdb_reviews\")\n",
    "train_set, test_set = datasets[\"train\"], datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 10:37:09.370611: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example in train_set.take(1):\n",
    "    print(example[\"text\"])\n",
    "    print(example[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> create:Apotosome 12/13/21\n",
    "\n",
    "> update:Apotosome 10/26/22"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "187px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
