{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 几个用来绘制灰度和 RGB 图像的函数\n",
    "def plot_image(image):\n",
    "    # \"nearest\" 分辨率不匹配时,只显示图像而不尝试在像素之间进行插值\n",
    "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_color_image(image):\n",
    "    plt.imshow(image, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def initialization():\n",
    "    keras.backend.clear_session()\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager\n",
    "my_font = font_manager.FontProperties(fname='../Fonts/SourceHanSerifSC-Medium.otf', size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类和定位 Classification and Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`定位任务`可以表示为`分类任务`:预测物体周围的边界框,一种常用的方法是预测**物体中心的水平坐标和垂直坐标以及其高度和宽度**.\n",
    "\n",
    "因此,可以添加具有四个单位的第二个`Dense层`,使用`MSE`损失对其进行训练."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 加载,拆分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
    "class_names = info.features[\"label\"].names\n",
    "n_classes = info.features[\"label\"].num_classes\n",
    "dataset_size = info.splits[\"train\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 图像预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中央裁切\n",
    "def central_crop(image):\n",
    "    shape = tf.shape(image)   # height, width\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]])\n",
    "    \n",
    "    # 从图像左上到右下\n",
    "    top_crop = (shape[0] - min_dim) // 4\n",
    "    bottom_crop = shape[0] - top_crop\n",
    "    \n",
    "    left_crop = (shape[1] - min_dim) // 4\n",
    "    right_crop = shape[1] - left_crop    \n",
    "    \n",
    "    return image[top_crop:bottom_crop, left_crop:right_crop]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机裁切\n",
    "def random_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 // 100\n",
    "\n",
    "    # 如果一个维度不应该被裁剪，则传递该维度的完整大小\n",
    "    cropped_image = tf.image.random_crop(image, size=[min_dim, min_dim, 3])\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label, randomize=False):\n",
    "    if randomize:\n",
    "        cropped_image = random_crop(image)\n",
    "        # random_flip_left_right:随机水平翻转图像（从左到右）。\n",
    "        cropped_image = tf.image.random_flip_left_right(cropped_image)\n",
    "    else:\n",
    "        cropped_image = central_crop(image)\n",
    "    \n",
    "    resized_image = tf.image.resize(cropped_image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = train_set_raw.shuffle(1000).repeat()\n",
    "train_set = train_set.map(partial(preprocess, randomize=True)).batch(batch_size).prefetch(1)\n",
    "\n",
    "valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n",
    "\n",
    "test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 加载预训练模型\n",
    "    \n",
    "    加载在 `ImageNet` 上预训练的` Xception` 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPool2D, Softmax, BatchNormalization, ReLU, GlobalAvgPool2D, Add, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = keras.applications.Xception(include_top=False, weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = GlobalAvgPool2D()(feature.output)\n",
    "class_output = Dense(n_classes, activation=\"softmax\")(avg)\n",
    "loc_output = Dense(4)(avg) # 物体中心的水平坐标和垂直坐标以及其高度和宽度.\n",
    "model = Model(inputs=feature.input, outputs=[class_output, loc_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，`Keras` 将计算`loss`所有这些损失并简单地将它们相加以获得用于训练的最终损失。由于主输出比辅助输出（因为它只是用于正则化），所以我们想让主输出的损失更大,可以通过`loss_weights`设置."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, decay=0.01)\n",
    "\n",
    "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
    "              loss_weights=[0.8, 0.2],\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 随机生成边界框\n",
    "\n",
    "    假设已经获得了`tf_flower`数据集中每个图像的边界框.接下来,需要创建一个数据集,其数据项是经过预处理的图像的批量处理以及其类标签和边界框.\n",
    "\n",
    "    每个数据项形式如下:`(images, (class_lables,bounding_boxes))`\n",
    "    \n",
    "    对边界框应该进行归一化，以便水平和垂直坐标以及高度和宽度都在 0 到 1 的范围内。\n",
    "\n",
    "    此外，通常要预测高度和宽度的平方根：对于,大边界框的 10 像素错误不会像对小边界框的 10 像素错误一样收到惩罚。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加随机生成边界框\n",
    "def add_random_bounding_boxes(images, lables):\n",
    "    shape = tf.shape(images)   # [224, 224]\n",
    "    fake_boxes = tf.random.uniform([shape[0], 4])   # [224, 4]\n",
    "    return images, (lables, fake_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_train_set = train_set.take(5).repeat(2).map(add_random_bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5/5 [==============================] - 35s 6s/step - loss: 1.1612 - dense_loss: 1.3696 - dense_1_loss: 0.3274 - dense_accuracy: 0.4375 - dense_1_accuracy: 0.2000\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 29s 6s/step - loss: 0.5893 - dense_loss: 0.6893 - dense_1_loss: 0.1894 - dense_accuracy: 0.7437 - dense_1_accuracy: 0.2562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa0003f9390>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(fake_train_set, steps_per_epoch=5, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MSE`通常作为成本函数训练效果很好，但它并不是评估模型预测边界框的好坏的一个很好的指标。最常见的度量指标是`交并比IoU`：预测边界框和目标边界框之间的重叠面积除以它们的联合面积。\n",
    "\n",
    "在 `keras`中,由 `tf.keras.metrics.MeanIoU`类实现。\n",
    "\n",
    "<img src=\"../images/other/14-53.png\" width=\"400\">\n",
    "<img src=\"../images/other/14-54.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目标检测 Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对图像中的**多个对象进行分类和定位的任务**称为`目标检测`。\n",
    "- 一种常见的方法是采用经过训练对单个对象进行分类和定位的 `CNN`，然后将其**在图像上滑动**.\n",
    "    \n",
    "    在此示例中，图像被切成 6 × 8 的网格，我们展示了一个 `CNN` 在所有 3 × 3 区域上滑动。当 `CNN` 查看图像的左上角时，它检测到最左边玫瑰的一部分，然后当它第一次向右移动一步时，它再次检测到同一朵玫瑰。在下一步，它开始检测最上面的玫瑰的一部分，然后一旦它向右移动了一步，它就会再次检测到它。然后，您将继续在整个图像中滑动`CNN`，查看所有 3 × 3 区域。此外，由于对象可以有不同的大小，您还可以在不同大小的区域之间滑动 CNN。\n",
    "<img src=\"../images/other/14-55.png\" width=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "- 滑动操作,会在不同的位置多次检测到同一个对象。接下来需要进行一些**后期处理来消除所有不必要的边界框**。\n",
    "\n",
    "    一种常见的方法称为`非极大抑制non-max suppression`。\n",
    "    \n",
    "    1. 首先，您需要在 CNN 中添加额外的 `置信度分数` 输出，以估计图像中存在花的概率。必须使用 `sigmoid` 激活函数，使用`二元交叉熵损失`来训练它。然后**删除所有 `置信度分数` 低于某个阈值的所有边界框**：这将删除所有实际上不包含花的边界框.\n",
    "    2. **找到 `置信度分数` 最高的边界框，并删除所有其他与它重叠很多的边界框**（例如，`交并比IoU` ＞ 60%）。例如，上图中具有最大 `置信度分数` 分数的边界框是最顶部玫瑰上方的粗边界框 。同一朵玫瑰上的另一个边界框与最大边界框重叠很多，因此将其删除.\n",
    "    3. 重复第二步,直到没有更多的边界框可以删除."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`目标检测`任务基本可以分成两类:\n",
    "\n",
    "- Two-Stage:    (`Faster-RCNN`)\n",
    "    1. 通过专门的模块去生成候选框(RPN),寻找前景以及调整边界框(`基于anchors`)\n",
    "    2. 基于之前生成的候选框进一步分类和调整候选框(`基于proposals`)\n",
    "    3. 优点:检测更准确\n",
    "    \n",
    "    \n",
    "- One-Stage:    (`SSD`,`YOLO`)\n",
    "    1. `基于 anchors`直接进行分类以及调整边界框\n",
    "    2. 优点:检测速度快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种简单的目标检测方法效果很好，但它需要多次运行CNN，因此速度很慢。幸运的是，有一种更快的方法可以在图像上滑动 CNN：`完全卷积网络FCN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全卷积网络 Fully Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> create:Apotosome \n",
    "\n",
    "> update:Apotosome 10/26/22"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
