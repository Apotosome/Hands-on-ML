{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import time\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def initialization(seed=42):\n",
    "    keras.backend.clear_session()\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager\n",
    "my_font = font_manager.FontProperties(fname='../Fonts/SourceHanSerifSC-Medium.otf', size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习题 Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 使用`有状态RNN`与`无状态RNN`各有什么优缺点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - `Stateless RNNs`(at each training iteration **the model starts with a hidden state full of zeros**, then it updates this state at each time step, and after the last time step, it throws it away, as it is not needed anymore.) **can only capture patterns whose length is less than, or equal to, the size of the windows the RNN is trained on.** \n",
    "> - Conversely, `stateful RNNs` can **capture longer-term patterns.** However, implementing a stateful RNN is much **harder—especially preparing the dataset properly**. Moreover, stateful RNNs do not always work better, in part because consecutive batches are **not independent and identically distributed (IID)**. Gradient Descent is not fond of non-IID datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 人们为什么使用编码解码RNN而不是简单序列对序列RNN进行自动翻译？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In general, if you translate a sentence one word at a time, the result will be terrible. For example, the French sentence “Je vous en prie” means “You are welcome,” but if you translate it one word at a time, you get “I you in pray.” Huh? It is much better to read the whole sentence first and then translate it. A plain `sequence-tosequence RNN` would **start translating a sentence immediately after reading the first word**, while an `Encoder–Decoder RNN` will **first read the whole sentence and then translate it.** That said, one could imagine a plain sequence-to-sequence RNN that would output silence whenever it is unsure about what to say next (just like human translators do when they must translate a live broadcast)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 如何处理可变长度的输入序列？可变长度的输出序列呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - `Variable-length input sequences` can be handled by **padding 填充 the shorter sequences so that all sequences in a batch have the same length**, and **using masking 掩码 to ensure the RNN ignores the padding token.** \n",
    ">\n",
    ">    For better performance, you may also want to **create batches containing sequences of similar sizes**. `Ragged tensors` can hold sequences of variable lengths, and `tf.keras` will likely support them eventually, which will greatly simplify handling variable-length input sequences (at the time of this writing, it is not the case yet). `tensorflow`中使用`ragged.constant()`将非矩形列表转为`tensor`类型\n",
    ">\n",
    "> \n",
    "> - `Regarding variable-length output sequences`, \n",
    "> \n",
    ">  if **the length** of the output sequence **is known** in advance (e.g., if you know that it is the same as the input sequence), then you just need to configure the loss function so that it **ignores tokens** that come after the end of the sequence. Similarly, the code that will use the model should ignore tokens beyond the end of the sequence.\n",
    "> \n",
    ">  But generally the length of the output sequence **is not known** ahead of time, so the solution is to train the model so that **it outputs an end of sequence token at the end of each sequence.**\n",
    "\n",
    "> 不规则张量参考: https://www.tensorflow.org/guide/ragged_tensor?hl=zh-cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 什么是集束搜素，为什么要使用它？你可以使用哪种工具来实现它？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - `Beam search` is a technique used to **improve the performance** of a trained Encoder–Decoder model, for example in a neural machine translation system.\n",
    "> <img src=\"../images/other/16-10.png\" width=\"300\">\n",
    "> \n",
    ">   The algorithm **keeps track追踪 of a short list of the $k$ most promising output sentences** (say, the top three), and at each decoder step it tries to extend them by one word; then it **keeps only the $k$ most likely sentences.** The parameter $k$ is called the `beam width`: the larger it is, the more CPU and RAM will be used, but also the more accurate the system will be. \n",
    "> - Instead of *greedily* choosing the most likely next word at each step to extend a single sentence, **this technique allows the system to explore several promising sentences simultaneously同时地.** Moreover, this technique lends itself well to **parallelization并行化**. \n",
    "> - You can implement beam search fairly easily using `TensorFlow Addons`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 什么是注意力机制？它有什么帮助？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An `attention mechanism` is a technique initially used in `Encoder–Decoder models` to give the `decoder` more direct access to the input sequence, **allowing it to deal with longer input sequences.**\n",
    "<img src=\"../images/other/16-11.png\" width=\"400\">\n",
    ">\n",
    "> **At each `decoder` time step, the current decoder’s state and the full output of the `encoder` are processed by an alignment对齐 model that outputs an alignment score for each input time step.** This score indicates which part of the input is **most relevant** to the current decoder time step. **The weighted sum of the encoder output** (weighted by their alignment score) **is then fed to the decoder**, which produces the next decoder state and the output for this time step. \n",
    ">\n",
    "> > $\\begin{array}{rlr}\\alpha_{t s} & =\\frac{\\exp \\left(\\operatorname{score}\\left(\\boldsymbol{h}_{t}, \\overline{\\boldsymbol{h}}_{s}\\right)\\right)}{\\sum_{s^{\\prime}=1}^{S} \\exp \\left(\\operatorname{score}\\left(\\boldsymbol{h}_{t}, \\overline{\\boldsymbol{h}}_{s^{\\prime}}\\right)\\right)} & \\text { [Attention weights] (1)} \n",
    "\\\\ \\boldsymbol{c}_{t} & =\\sum_{s} \\alpha_{t s} \\overline{\\boldsymbol{h}}_{s} & \\text { [Context vector] (2)} \n",
    "\\\\ \\boldsymbol{a}_{t} & =f\\left(\\boldsymbol{c}_{t}, \\boldsymbol{h}_{t}\\right)=\\tanh \\left(\\boldsymbol{W}_{c}\\left[\\boldsymbol{c}_{t} ; \\boldsymbol{h}_{t}\\right]\\right) & \\text { [Attention vector] (3)}\\end{array}$\n",
    "> \n",
    "> The main benefit of using an attention mechanism is the fact that the `Encoder–Decoder` model **can successfully process longer input sequences.** Another benefit is that the alignment scores makes the model **easier to debug调试 and interpret解释**: for example, if the model makes a mistake, you can look at which part of the input it was paying attention to, and this can help diagnose the issue. An attention mechanism is also at the core of the Transformer architecture, in the Multi-Head Attention layers. See the next answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: `Transformer`架构中最重要的层是什么？目的是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The most important layer in the Transformer architecture is the `Multi-Head Attention layer` (the original Transformer architecture contains 18 of them, including 6 Masked Multi-Head Attention layers). \n",
    "> <img src=\"../images/other/16-23.svg\" width=\"400px\">\n",
    "> It is at the core of language models such as BERT and GPT-2. **Its purpose is to allow the model to identify which words are most aligned with each other, and then improve each word’s representation using these contextual clues上下文线索.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 你何时需要使用`采样softmax`？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `Sampled softmax` is used when **training a classification model when there are many classes** (e.g., thousands). It computes an approximation 近似 of the crossentropy loss based on the **logit predicted by the model for the correct class**, and the predicted logits for a sample of incorrect words. This **speeds up training** considerably compared to computing the softmax over all logits and then estimating the cross-entropy loss. After training, the model can be used normally, using the regular softmax function to compute all the class probabilities based on all the logits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: *Hochreiter*和*Schmidhuber*在有关`LSTM`的论文中使用了`嵌入式Reber`语法。它是人工语法，可产生诸如*BPBTSXXVPSEPEPE*之类的字符串。请查阅*Je Orr*对这个主题的精彩介绍。\n",
    "\n",
    "选择一种特定的`嵌人式Reber`语法（例如，Jenny 主页上表示的语法)，然后训练`RNN`以识别字符串是否符合该语法。首先，你编写一个能够生成训练批量处理的函数，其中包含大约50%符合语法的字符, 50%不符合语法的字符串。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reber和嵌入式Reber 字符串生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.   First we need to **build a function that generates strings based on a Reber grammar**. The grammar will be represented as a list of possible transitions for each state. A transition specifies 指定 the **string to output** (or a grammar to generate it) and the **next state**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/other/16-63.gif\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图所示，它基本是个`有环的有向图`。我们从 B 开始，从一个节点移动到下一个节点，边走边添加我们传递给字符串的符号。当我们到达最后的 E 时结束。如果我们可以采取两条路径，例如在 T 之后，我们可以去 S 或 X，我们随机选择一个（概率相等）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_reber_grammar = [\n",
    "    [(\"B\", 1)],           # (state 0) =B=>(state 1)\n",
    "    [(\"T\", 2), (\"P\", 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n",
    "    [(\"S\", 2), (\"X\", 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n",
    "    [(\"T\", 3), (\"V\", 5)], # and so on...\n",
    "    [(\"X\", 3), (\"S\", 6)],\n",
    "    [(\"P\", 4), (\"V\", 6)],\n",
    "    [(\"E\", None)]        # (state 6) =E=>(terminal state)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过这种方式，我们可以生成无限数量的字符串，这些字符串属于相当奇特的 Reber 语言。自己验证下面左边的字符串是可能的 Reber 字符串，而右边的不是。\n",
    "$$\n",
    "\\begin{array}{|l||l|}\n",
    "\\hline {\\text { \"Reber\" }} & \\text { \"Non-Reber\" } \\\\\n",
    "\\hline \\text { BTSSXXTVVE } & \\text { BTSSPXSE } \\\\\n",
    "\\hline \\text { BPVVE } & \\text { BPTVVB } \\\\\n",
    "\\hline \\text { BTXXVPSE } & \\text { BTXXVVSE } \\\\\n",
    "\\hline \\text { BPVPXVPXVPXVVE } & \\text { BPVSPSE } \\\\\n",
    "\\hline \\hline \\text { BTSXXVPSE } & \\text { BTSSSE } \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. Let's generate a few strings based on the default Reber grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_string(grammar):\n",
    "    state = 0\n",
    "    output = []\n",
    "    while state is not None:\n",
    "        index = np.random.randint(len(grammar[state]))   # len:1-> [(\"B\", 1)]\n",
    "        production, state = grammar[state][index]  # production=\"B\", state=1\n",
    "#         if isinstance(production, list):  # for embedded_reber_grammar\n",
    "#             production = generate_string(grammar=production)\n",
    "        output.append(production)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTXXTTVPXTVPXTTVPSE BPVPSE BTXSE BPVVE BPVVE BTSXSE BPTVPXTTTVVE BPVVE BTXSE BTXXVPSE BPTTTTTTTTVVE BTXSE BPVPSE BTXSE BPTVPSE BTXXTVPSE BPVVE BPVVE BPVVE BPTTVVE BPVVE BPVVE BTXXVVE BTXXVVE BTXXVPXVVE "
     ]
    }
   ],
   "source": [
    "initialization(seed=42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_string(default_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Looks good. Now let's generate a few strings based on the embedded Reber grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/other/16-64.gif\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_reber_grammar = [\n",
    "    [(\"B\", 1)],\n",
    "    [(\"T\", 2), (\"P\", 3)],\n",
    "    [(default_reber_grammar, 4)],\n",
    "    [(default_reber_grammar, 5)],\n",
    "    [(\"T\", 6)],\n",
    "    [(\"P\", 6)],\n",
    "    [(\"E\", None)]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把满足`Reber Grammar`的字符串`Embed`到一定的格式中得到的字符串满足`Embedded Reber Grammar`，该语法生成两种类型的字符串:\n",
    "- 使用通过图的顶部路径生成：`BT<reber string>TE`\n",
    "- 使用底部路径生成：`BP<reber string>PE`。\n",
    "\n",
    "如果要判断字符串满足`Embedded Reber Grammar`，需要确定第二个字母和倒数第二个字母相同。对于一个学习模型，需要有某种记忆（第2个字母和倒数第2个字母相同）才能正确判断一个字符串是否满足`Embedded Reber Grammar`。如下格式则不满足:`BP<reber string>TE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_string(grammar):\n",
    "    state = 0\n",
    "    output = []\n",
    "    while state is not None:\n",
    "        index = np.random.randint(len(grammar[state]))   # len:1-> [(\"B\", 1)]\n",
    "        production, state = grammar[state][index]  # production=\"B\", state=1\n",
    "        if isinstance(production, list):  # for embedded_reber_grammar\n",
    "            production = generate_string(grammar=production)\n",
    "        output.append(production)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBPTTTVPXTVPXTTVPSETE BPBPTVPSEPE BPBPVVEPE BPBPVPXVVEPE BPBTXXTTTTVVEPE BPBPVPSEPE BPBTXXVPSEPE BPBTSSSSSSSXSEPE BTBPVVETE BPBTXXVVEPE BPBTXXVPSEPE BTBTXXVVETE BPBPVVEPE BPBPVVEPE BPBTSXSEPE BPBPVVEPE BPBPTVPSEPE BPBTXXVVEPE BTBPTVPXVVETE BTBPVVETE BTBTSSSSSSSXXVVETE BPBTSSSXXTTTTVPSEPE BTBPTTVVETE BPBTXXTVVEPE BTBTXSETE "
     ]
    }
   ],
   "source": [
    "initialization(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_string(embedded_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. Okay, now we need a function to generate strings that **do not respect the grammar**. We could generate a random string, but the task would be a bit too easy, so instead we will generate a string that respects the grammar, and we will corrupt it by changing just one character:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成不正确语法规则的字符串:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSSIBLE_CHARS = \"BEPSTVX\"\n",
    "\n",
    "\n",
    "def generate_corrupted_string(grammer, chars=POSSIBLE_CHARS):\n",
    "    good_string = generate_string(grammer)\n",
    "    index = np.random.randint(len(good_string))\n",
    "    good_char = good_string[index]\n",
    "    # set(POSSIBLE_CHARS) -> {'B', 'E', 'P', 'S', 'T', 'V', 'X'}\n",
    "    # bad_char: 从good_char中随机去除一种字符后 再随机取出一个字符\n",
    "    bad_char = np.random.choice(sorted(set(POSSIBLE_CHARS) - set(good_char)))\n",
    "    out_char = good_string[:index] + bad_char + good_string[index + 1:] # 拼接字符串\n",
    "    return out_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBPTTTPPXTVPXTTVPSETE BPBTXEEPE BPBPTVVVEPE BPBTSSSSXSETE BPTTXSEPE BTBPVPXTTTTTTEVETE BPBTXXSVEPE BSBPTTVPSETE BPBXVVEPE BEBTXSETE BPBPVPSXPE BTBPVVVETE BPBTSXSETE BPBPTTTPTTTTTVPSEPE BTBTXXTTSTVPSETE BBBTXSETE BPBTPXSEPE BPBPVPXTTTTVPXTVPXVPXTTTVVEVE BTBXXXTVPSETE BEBTSSSSSXXVPXTVVETE BTBXTTVVETE BPBTXSTPE BTBTXXTTTVPSBTE BTBTXSETX BTBTSXSSTE "
     ]
    }
   ],
   "source": [
    "initialization(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_corrupted_string(embedded_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5. We cannot feed strings directly to an RNN, so we need to encode them somehow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 一种选择是对每个字符进行`one-hot编码`。\n",
    "2. 另一种选择是使用`embeddings嵌入`。\n",
    "\n",
    "\n",
    "让我们选择第二个选项。为了使嵌入工作，我们需要将每个字符串转换为字符 ID 序列。让我们为此编写一个函数，使用可能字符串`BEPSTVX`中每个字符的索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSSIBLE_CHARS = \"BEPSTVX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_ids(s, chars=POSSIBLE_CHARS):\n",
    "    return [chars.index(c) for c in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 4, 4, 6, 6, 5, 5, 1, 4, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_ids(\"BTTTXXVVETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 6. We can now generate the dataset, with 50% good strings, and 50% bad strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(size):\n",
    "    good_strings = [\n",
    "        string_to_ids(generate_string(embedded_reber_grammar))\n",
    "        for _ in range(size // 2)\n",
    "    ]\n",
    "    bad_strings = [\n",
    "        string_to_ids(generate_corrupted_string(embedded_reber_grammar))\n",
    "        for _ in range(size - size // 2)\n",
    "    ]\n",
    "    all_strings = good_strings + bad_strings\n",
    "    X = tf.ragged.constant(all_strings, ragged_rank=1)\n",
    "    y = np.array([[1.] for _ in range(len(good_strings))] +\n",
    "                 [[0.] for _ in range(len(bad_strings))])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialization(42)\n",
    " \n",
    "X_train, y_train = generate_dataset( 10000 )\n",
    "X_valid, y_valid = generate_dataset( 2000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(22,), dtype=int32, numpy=\n",
       " array([0, 4, 0, 2, 4, 4, 4, 5, 2, 6, 4, 5, 2, 6, 4, 4, 5, 2, 3, 1, 4, 1],\n",
       "       dtype=int32)>, array([1.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 7. Perfect! We are ready to create the RNN to identify good strings. We build a simple sequence binary classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialization(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 5\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    # `Ragged` : 布尔值，创建的占位符是否意味着不规则。\n",
    "    keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n",
    "    keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS),\n",
    "                           output_dim=embedding_size),\n",
    "    keras.layers.GRU(30),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jin/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/gru/RaggedToTensor/boolean_mask_1/GatherV2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/gru/RaggedToTensor/boolean_mask/GatherV2:0\", shape=(None, 5), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/gru/RaggedToTensor/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 8ms/step - loss: 0.6910 - accuracy: 0.5095 - val_loss: 0.6825 - val_accuracy: 0.5645\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6678 - accuracy: 0.5659 - val_loss: 0.6635 - val_accuracy: 0.6105\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6504 - accuracy: 0.5766 - val_loss: 0.6521 - val_accuracy: 0.6110\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6347 - accuracy: 0.5980 - val_loss: 0.6224 - val_accuracy: 0.6445\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.6054 - accuracy: 0.6361 - val_loss: 0.5779 - val_accuracy: 0.6980\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5414 - accuracy: 0.7093 - val_loss: 0.4695 - val_accuracy: 0.7795\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3697 - accuracy: 0.8455 - val_loss: 0.3096 - val_accuracy: 0.8755\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.3424 - accuracy: 0.8583 - val_loss: 0.3092 - val_accuracy: 0.9110\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2946 - accuracy: 0.8830 - val_loss: 0.3003 - val_accuracy: 0.8950\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.1879 - accuracy: 0.9377 - val_loss: 0.1839 - val_accuracy: 0.9655\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1459 - accuracy: 0.9562 - val_loss: 0.4388 - val_accuracy: 0.8085\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1463 - accuracy: 0.9530 - val_loss: 0.0753 - val_accuracy: 0.9820\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0664 - accuracy: 0.9859 - val_loss: 0.0586 - val_accuracy: 0.9845\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0435 - accuracy: 0.9894 - val_loss: 0.0192 - val_accuracy: 0.9975\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0048 - accuracy: 0.9994 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 8.6842e-04 - accuracy: 1.0000 - val_loss: 7.2870e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 6.2245e-04 - accuracy: 1.0000 - val_loss: 5.4719e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 4.7959e-04 - accuracy: 1.0000 - val_loss: 4.4321e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 3.9126e-04 - accuracy: 1.0000 - val_loss: 3.6481e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.02,\n",
    "                                 momentum=0.95,\n",
    "                                 nesterov=True)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 8. Now let's test our RNN on two tricky strings: the first one is bad while the second one is good. They only differ by the second to last character. If the RNN gets this right, it shows that it managed to notice the pattern that the second letter should always be equal to the second to last letter. That requires a fairly long short-term memory (which is the reason why we used a GRU cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = [\"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\",   # bad_char\n",
    "                \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"]   # good_char  仅在倒数第二个字符上有所不同\n",
    "X_test = tf.ragged.constant([string_to_ids(s) for s in test_strings], ragged_rank=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "这些是 Reber 字符串的估计概率：\n",
      "BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE: 0.07%\n",
      "BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE: 99.89%\n"
     ]
    }
   ],
   "source": [
    "y_proba = model.predict(X_test)\n",
    "print()\n",
    "print(\"这些是 Reber 字符串的估计概率：\")\n",
    "for index, string in enumerate(test_strings):\n",
    "    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta-da! It worked fine. The RNN found the correct answers with very high confidence. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q:训练可以将日期字符串从一种格式转换为另一种格式的编码器-解码器模型\n",
    ">\n",
    "> 例如:从`April 22, 2019` 转换为 `2019-04-22`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "详见 \n",
    "- [`第16章 使用RNN和注意力机制进行自然语言处理(2) 1.2  编码器-解码器示例`](./第16章%20使用RNN和注意力机制进行自然语言处理(2).ipynb#编码器-解码器示例)\n",
    "- [`第16章 使用RNN和注意力机制进行自然语言处理(2) 2.2  编码器-解码器示例续`](./第16章%20使用RNN和注意力机制进行自然语言处理(2).ipynb#编码器-解码器示例续)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q:阅读`TensorFlow`的带注意力机制的神经机器翻译教程.\n",
    ">\n",
    "> _Exercise: Go through TensorFlow's [Neural Machine Translation with Attention tutorial](https://homl.info/nmttuto)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "详见 \n",
    "- [基于注意力的神经机器翻译](./nmt_with_attention.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q: 使用最新的语言模型之一（例如BERT)来生成更具说服力的莎士比亚文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用最新语言模型的最简单方法是使用由 `Hugging Face` 开源的优秀`Transformer`库。它为自然语言处理提供了许多现代神经网络架构（包括 `BERT`、`GPT-2`、`RoBERTa`、`XLM`、`DistilBert`、`XLNet` 等），包括许多预训练模型。它依赖于 `TensorFlow` 或 `PyTorch`。最重要的是：使用起来非常简单。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，让我们加载一个预训练模型。在这个例子中，我们将使用 `Open AI` 的 `GPT` 模型，在顶部有一个额外的语言模型（只是一个权重与输入嵌入相关的线性层）。\n",
    "\n",
    "1. 让我们导入它并加载预训练的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFOpenAIGPTLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFOpenAIGPTLMHeadModel.\n",
      "\n",
      "All the layers of TFOpenAIGPTLMHeadModel were initialized from the model checkpoint at openai-gpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFOpenAIGPTLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 接下来,我们需要专门用于此模型的分词器, 如果安装了`spaCy`和`ftfy`库, 你将使用它们. 否则将会回退到`BERT`的`BasicTokenizer`分词器, 然后是`Byte-Pair Encoding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "from transformers import OpenAIGPTTokenizer\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 使用分词器对提示文本进行分词和编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187]], dtype=int32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text = \"This royal throne of kings, this sceptred isle\"\n",
    "\n",
    "encoded_prompt = tokenizer.encode(text=prompt_text,\n",
    "                                  add_special_tokens=False,\n",
    "                                  return_tensors=\"tf\")\n",
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this</w>',\n",
       " 'royal</w>',\n",
       " 'throne</w>',\n",
       " 'of</w>',\n",
       " 'kings</w>',\n",
       " ',</w>',\n",
       " 'this</w>',\n",
       " 'scep',\n",
       " 'tred</w>',\n",
       " 'isle</w>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoded_prompt.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 接下来，让我们使用模型在prompt提示后生成文本。我们将生成 5 个不同的句子，每个句子都以提示文本开头，然后是 40 个附加tokens。要了解所有超参数的作用，请务必查看 Patrick von Platen 的这篇精彩博文-[如何生成文本：使用不同的解码方法通过 Transformers 生成语言](https://huggingface.co/blog/how-to-generate)。您可以使用超参数来尝试获得更好的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 49), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   240,   488,   616,   271,   256,   481,  2650,   498,\n",
       "         1294,  1978,   240,  1119,   535,   488,  1294,   500,   481,\n",
       "         7250,  1092,   240,   488,   984,  2821,  4495,   500,   616,\n",
       "          828,   240,   488,   984,  2821,   580,  2400,  1964,   498,\n",
       "          481, 37494,   498,   481],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   544,   987,   963,  1074,   618,   239, 40477,  1550,\n",
       "         1739,   547,  2500,   485,   246,  1343,   240,   249,  8335,\n",
       "          491,  1098,   239,   547,  1129,   793,   509,  1256,   240,\n",
       "          488,   249,  1443,  6905,   551,   481,  3635,   260,  1114,\n",
       "          488,  6523,   822,   987],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   544,   481,  1700,   239,   481,  1667,   498,   512,\n",
       "          604,  1739,   481,  5363,   498,   622,  6244,   556,   618,\n",
       "        18208,   481,   929,   240,   568,   606,   604,   595,  3431,\n",
       "          481,   749,   244,   487,   899,   488,  1603,   491, 29573,\n",
       "          239, 40477,   244, 33828],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   240,   616,   909,  3573,   500,   481,  2233,   498,\n",
       "          481,  2761,   240,   488,   616,   762,   260,  1276,   260,\n",
       "        11093,  4187,   239,   599,   249,   939,   485,   788,   509,\n",
       "          664,   725,   815,   246,  1594,   498, 35320,   488,  1175,\n",
       "          239, 40477,   566,  1774],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   239,  1099,  3652,   498,   616,  2264,   240,  1099,\n",
       "         1248,  3398,   498,   481,  2510,   240,  1099,  3266,  2056,\n",
       "         2455,   240,   509,   498,   524, 12103,   239,   244,   249,\n",
       "          509,  1593,   239,   249,   868,  1812,   512,   775,  4837,\n",
       "          239,   512,   640,   668]], dtype=int32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sequences = 5\n",
    "length = 40\n",
    "\n",
    "generated_sequences = model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    do_sample=True,\n",
    "    max_length=length + len(encoded_prompt[0]),    # 40+10=50\n",
    "    temperature=1.0,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.0,\n",
    "    num_return_sequences=num_sequences,\n",
    ")\n",
    "\n",
    "generated_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 使用解码生成的序列并打印"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this royal throne of kings, this sceptred isle, and this :'the choice of dark rose, love's and dark in the southern light, and which shall enter in this hand, and which shall be broken free of the woes of the\n",
      "==========\n",
      "this royal throne of kings, this sceptred isle is its very own king. \n",
      " having brought my meeting to a close, i withdrew at once. my work here was done, and i soon sought out the tea - house and stole through its\n",
      "==========\n",
      "this royal throne of kings, this sceptred isle is the answer. the four of you have brought the agreement of our trade with king urban the first, but we have not signed the... \" he turned and stared at melisande. \n",
      " \" treati\n",
      "==========\n",
      "this royal throne of kings, this sceptred isle, this little island in the middle of the sea, and this man - world - ruled gods. what i came to see was no more than a land of shepherds and men. \n",
      " one reason\n",
      "==========\n",
      "this royal throne of kings, this sceptred isle. every inch of this hall, every cranny of the wood, every twisted green tree, was of his ancestors. \" i was wrong. i never meant you any harm. you are just\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "for sequence in generated_sequences:\n",
    "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(text)\n",
    "    print(\"=\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以尝试更新（和更大）的模型，例如 `GPT - 2`、`CTRL`、`Transformer-XL` 或 `XLNet`，它们都可以作为`Transformer`库中的预训练模型使用，包括顶部带有语言模型的变体。模型之间的预处理步骤略有不同，因此请务必查看转换器文档中的这个生成示例（此示例使用`PyTorch`，但只需进行很少的调整即可工作，例如在模型类名称的开头添加 `TF`，删除 `.to()` 方法调用，并使用 `return_tensors=\"tf\"` 而不是`pt`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 使用中文 `GPT2`模型实现中文文本生成."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextGenerationPipeline\n",
    "\n",
    "text_generator = TextGenerationPipeline(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '有一个聋哑的中国男孩 ， 当 他 看 到 新 闻 说 北 京 有 名 叫 孙 海 平 的 女 孩 患 上 了 脑 萎 缩 症 ， 这 一 片 都 是 他 的 梦 想 ， 他 决 定 把 他 接 到 这 个 城 市 ， 也 把 他 送 到 那 里 ， 看 看 可 以 为 他 提 供 什 么 帮 助 。 第 一 次 见 面 ， 孙 海 平 和 新 华 社 社 长 赵 平 刚 一 起 见 了 一 会 儿 ， 双 方 就 打 成 一 片 了 ， 你 跟 她 谈 谈 ， 这 孩 子 是 为 了 治 病 ， 他 不 认 同 中 国 现 在 治 病 ， 不 用 看 ， 他 都 听 得 懂 ， 他 不 愿 意 做 病 人 。 我 们 都 还 在 想 着 怎 么 办 ？ 他 认 为 怎 么 能 让 他 不 听 话 呢 ？ 我 跟 赵 平 说 ， 他 认 为 听 了 你 的 话 就 是 治 病 ， 你 不 要 管 。 他 也 不 太 跟 我 说 他 也 不 理 解 ， 还 说 可 能 那 边 医 疗 条 件 不 好 。 不 知 从 哪 天 起 ， 他 就 一 点 也 不 懂 中 国 的 医 疗 制 度 ， 我 看 到 他 的 报 道 ， 就 说 我 这 个 人 也 不 懂 什 么 叫 医 疗 制 度 ， 真 是 可 怜 我 们 这 些 外 来 人 ， 这 么 多 年 一 直 很 讨 厌 中 国 ， 甚 至 可 以 说 是 侮 辱 。 我 想 说 的 是 ， 中 国 这 么 多 年 已 经 是 个 外 来 物 种 ， 我 们 国 家 是 不 是 犯 错 了 ？ 当 年 的 我 们 这 些 人 对 此 嗤 之 以 鼻 ， 中 国 医 生 怎 么 说 的 他 们 也 不 知 道 ， 在 中 国 这 么 多 年 ， 医 生 只 有 中 国 人 才 是 中 国 人 ， 他 们 的 医 疗 制 度 也 只 是 一 把 杀 猪 刀 ， 杀 生 的 话 一 次 就 够 了 ， 要 么 是 杀 手 病 ， 要 么 就 是 为 他 一 点 办 法 都 没 有 的 病 。 这 样 的 话 ， 我 们 国 家 为 了 不 让 中 国 的 病 人 ， 特 别 是 大 众 在 家 门 口 的 一 家 医 院 来 看 急 诊 ， 大 部 分 医 生 都 是 不 懂 的 。 【'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\"有一个聋哑的中国男孩\", max_length=500, do_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> create:Apotosome 05/19/22\n",
    "\n",
    "> update:Apotosome 10/26/22"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "目录",
   "title_sidebar": "目录",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "                                                                         