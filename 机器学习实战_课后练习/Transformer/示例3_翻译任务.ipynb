{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48eb242c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T16:06:58.887578Z",
     "iopub.status.busy": "2022-05-16T16:06:58.886980Z",
     "iopub.status.idle": "2022-05-16T16:07:21.195944Z",
     "shell.execute_reply": "2022-05-16T16:07:21.194582Z"
    },
    "papermill": {
     "duration": 22.323092,
     "end_time": "2022-05-16T16:07:21.198570",
     "exception": false,
     "start_time": "2022-05-16T16:06:58.875478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\r\n",
      "tensorflow 2.6.3 requires absl-py~=0.10, but you have absl-py 1.0.0 which is incompatible.\r\n",
      "tensorflow 2.6.3 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\r\n",
      "tensorflow 2.6.3 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\r\n",
      "tensorflow 2.6.3 requires wrapt~=1.12.1, but you have wrapt 1.14.0 which is incompatible.\r\n",
      "tensorflow-transform 1.7.0 requires pyarrow<6,>=1, but you have pyarrow 7.0.0 which is incompatible.\r\n",
      "tensorflow-transform 1.7.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<2.9,>=1.15.5, but you have tensorflow 2.6.3 which is incompatible.\r\n",
      "tensorflow-serving-api 2.8.0 requires tensorflow<3,>=2.8.0, but you have tensorflow 2.6.3 which is incompatible.\r\n",
      "rich 12.4.1 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "pytorch-lightning 1.6.3 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "pytools 2022.1.7 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "flax 0.4.2 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "flake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.11.3 which is incompatible.\r\n",
      "apache-beam 2.37.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\r\n",
      "apache-beam 2.37.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.4 which is incompatible.\r\n",
      "apache-beam 2.37.0 requires pyarrow<7.0.0,>=0.15.1, but you have pyarrow 7.0.0 which is incompatible.\r\n",
      "aioitertools 0.10.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "aiobotocore 2.2.0 requires botocore<1.24.22,>=1.24.21, but you have botocore 1.25.12 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 torch==1.11.0 GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065b474f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T16:07:21.221328Z",
     "iopub.status.busy": "2022-05-16T16:07:21.221108Z",
     "iopub.status.idle": "2022-05-16T16:07:22.859896Z",
     "shell.execute_reply": "2022-05-16T16:07:22.859165Z"
    },
    "papermill": {
     "duration": 1.652506,
     "end_time": "2022-05-16T16:07:22.861919",
     "exception": false,
     "start_time": "2022-05-16T16:07:21.209413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1 å¯¼å…¥å¿…å¤‡çš„åº“\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 3 Model Architecture\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å‡½æ•°\n",
    "        :param encoder: ç¼–ç å™¨å¯¹è±¡\n",
    "        :param decoder: è§£ç å™¨å¯¹è±¡\n",
    "        :param src_embed: æºæ•°æ®åµŒå…¥å‡½æ•°\n",
    "        :param tgt_embed: ç›®æ ‡æ•°æ®åµŒå…¥å‡½æ•°\n",
    "        :param generator: ç±»åˆ«ç”Ÿæˆå™¨å¯¹è±¡\n",
    "        \"\"\"\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        å°†src, src_maskä¼ å…¥ç¼–ç å‡½æ•°ï¼Œå¾—åˆ°ç»“æœåä¸src_mask, tgtå’Œtgt_maskä¸€åŒä¼ ç»™è§£ç å‡½æ•°\n",
    "        :param src: æºæ•°æ®\n",
    "        :param tgt: ç›®æ ‡æ•°æ®\n",
    "        :param src_mask: æºæ•°æ®æ©ç å¼ é‡\n",
    "        :param tgt_mask: ç›®æ ‡æ•°æ®æ©ç å¼ é‡\n",
    "        \"\"\"\n",
    "        memory = self.encode(src, src_mask)\n",
    "        res = self.decode(memory, src_mask, tgt, tgt_mask)\n",
    "        return res\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        ç¼–ç å‡½æ•°ï¼Œä½¿ç”¨src_embedå¯¹sourceåšå¤„ç†ï¼Œç„¶åå’Œsrc_maskä¸€èµ·ä¼ ç»™self.encoder\n",
    "        \"\"\"\n",
    "        source_embeddings = self.src_embed(src)\n",
    "        return self.encoder(source_embeddings, src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        \"\"\"\n",
    "        è§£ç å‡½æ•°ï¼Œä½¿ç”¨tgt_embedå¯¹targetåšå¤„ç†ï¼Œç„¶åå’Œsrc_mask,tgt_mask,memoryä¸€èµ·ä¼ ç»™self.decoder\n",
    "        \"\"\"\n",
    "        target_embeddings = self.tgt_embed(tgt)\n",
    "        return self.decoder(target_embeddings, memory, src_mask, tgt_mask)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    å°†çº¿æ€§å±‚å’Œsoftmaxè®¡ç®—å±‚ä¸€èµ·å®ç°ï¼Œ æŠŠç±»çš„åå­—å«åšGeneratorï¼Œç”Ÿæˆå™¨ç±»\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å‡½æ•°\n",
    "        :param d_model: åµŒå…¥çš„ç»´åº¦\n",
    "        :param vocab: vocab.size -> è¯è¡¨çš„å¤§å°\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(in_features=d_model, out_features=vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        è¾“å…¥æ˜¯ä¸Šä¸€å±‚çš„è¾“å‡ºå¼ é‡x\n",
    "        ä½¿ç”¨ä¸Šä¸€æ­¥å¾—åˆ°çš„self.projå¯¹xè¿›è¡Œçº¿æ€§å˜åŒ–, ç„¶åä½¿ç”¨Fä¸­å·²ç»å®ç°çš„log_softmaxè¿›è¡Œsoftmaxå¤„ç†ã€‚\n",
    "        \"\"\"\n",
    "        softmax = F.log_softmax(self.proj(x), dim=-1)\n",
    "        return softmax\n",
    "\n",
    "\n",
    "# 3.1 Encoder and Decoder Stacks\n",
    "\n",
    "# 3.1.1 Encoder\n",
    "def clone(module, N):\n",
    "    \"\"\"\n",
    "    ç”¨äºå…‹éš†å¤šä»½ç»“æ„\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clone(layer, N)  # å®ç°ç®€å•çš„å…‹éš†ï¼Œåœ¨å åŠ åœ¨ä¸€èµ·\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        å°†è¾“å…¥ï¼ˆå’Œæ©ç ï¼‰ä¾æ¬¡é€šè¿‡æ¯ä¸€å±‚ã€‚\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "# 3.1.2 Layer Normalization and Residual Connections\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å‡½æ•°\n",
    "        :param feature_size: è¯åµŒå…¥çš„ç»´åº¦\n",
    "        :param eps: é˜²æ­¢åˆ†æ¯ä¸º0ï¼Œé»˜è®¤æ˜¯1e-6\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # ä½¿ç”¨nn.parameterå°è£…ï¼Œä»£è¡¨ä»–ä»¬æ˜¯æ¨¡å‹çš„å‚æ•°\n",
    "        self.gamma = nn.Parameter(torch.ones(feature_size))  # ç¼©æ”¾å‚æ•°å‘é‡ åˆå§‹åŒ–ä¸º1å¼ é‡\n",
    "        self.beta = nn.Parameter(torch.zeros(feature_size))  # å¹³ç§»å‚æ•°å‘é‡ åˆå§‹åŒ–ä¸º0å¼ é‡\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1. å¯¹è¾“å…¥å˜é‡xæ±‚å…¶æœ€åä¸€ä¸ªç»´åº¦,å³è¯åµŒå…¥ç»´åº¦çš„å‡å€¼ï¼Œå¹¶ä¿æŒè¾“å‡ºç»´åº¦ä¸è¾“å…¥ç»´åº¦ä¸€è‡´\n",
    "        2. æ±‚æœ€åä¸€ä¸ªç»´åº¦çš„æ ‡å‡†å·®ï¼Œè¿›è¡Œè§„èŒƒåŒ–ï¼šç”¨xå‡å»å‡å€¼é™¤ä»¥æ ‡å‡†å·®\n",
    "        3. å¯¹ç»“æœä¹˜ä»¥æˆ‘ä»¬çš„ç¼©æ”¾å‚æ•°gamma, *è¡¨ç¤ºç‚¹ä¹˜ï¼ŒåŠ ä¸Šä½ç§»å‚beta\n",
    "        :param x: æ¥è‡ªä¸Šä¸€å±‚çš„è¾“å‡º\n",
    "        \"\"\"\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return (x - mean) / (std + self.eps) * self.gamma + self.beta\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    SublayerConnectionç±»:å®ç°å­å±‚è¿æ¥ç»“æ„.ğ‘¥è¡¨ç¤ºä¸Šä¸€å±‚æ·»åŠ äº†æ®‹å·®è¿æ¥çš„è¾“å‡ºï¼Œè¿™ä¸€å±‚æ·»åŠ äº†æ®‹å·®è¿æ¥çš„è¾“å‡ºéœ€è¦å°†  ğ‘¥  æ‰§è¡Œå±‚çº§å½’ä¸€åŒ–ï¼Œ\n",
    "    ç„¶åé¦ˆé€åˆ° Multi-Head Attention å±‚æˆ–å…¨è¿æ¥å±‚ï¼Œæ·»åŠ  Dropout æ“ä½œåå¯ä½œä¸ºè¿™ä¸€å­å±‚çº§çš„è¾“å‡ºã€‚æœ€åå°†è¯¥å­å±‚çš„è¾“å‡ºå‘é‡ä¸è¾“å…¥å‘é‡ç›¸åŠ å¾—åˆ°ä¸‹ä¸€å±‚çš„è¾“å…¥ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        \"\"\"\n",
    "        :param size: ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™=512\n",
    "        :param dropout: ä¸¢å¼ƒå‚æ•°\n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # åŸæ–¹æ¡ˆï¼šå…ˆå°†xæ‰§è¡Œå±‚çº§å½’ä¸€åŒ–\n",
    "        # sublayer_out = sublayer(self.norm(x))\n",
    "        # return x + self.dropout(sublayer_out)\n",
    "        # æ”¹è¿›ç‰ˆæœ¬ï¼šå–å‡ºnorm åŠ å¿«æ”¶æ•›é€Ÿåº¦\n",
    "        sublayer_out = sublayer(x)\n",
    "        sublayer_out = self.dropout(sublayer_out)\n",
    "        return x + self.norm(sublayer_out)\n",
    "\n",
    "\n",
    "# 3.1.3 Encoder Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attention, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clone(module=SublayerConnection(size, dropout), N=2)  # ä¸¤æ¬¡çš„è·³è¿‡è¿æ¥\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        ç¬¬ä¸€ä¸ªå­å±‚åŒ…æ‹¬ä¸€ä¸ªå¤šå¤´è‡ªæ³¨æ„åŠ›å±‚å’Œè§„èŒƒåŒ–å±‚ä»¥åŠä¸€ä¸ªæ®‹å·®è¿æ¥\n",
    "        ç¬¬äºŒä¸ªå­å±‚åŒ…æ‹¬ä¸€ä¸ªå‰é¦ˆå…¨è¿æ¥å±‚å’Œè§„èŒƒåŒ–å±‚ä»¥åŠä¸€ä¸ªæ®‹å·®è¿æ¥\n",
    "        \"\"\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))  # è¾“å…¥ Queryã€Key å’Œ Value éƒ½ä¸º x å°±è¡¨ç¤ºè‡ªæ³¨æ„åŠ›ã€‚\n",
    "        z = self.sublayer[1](x, self.feed_forward)\n",
    "        return z\n",
    "\n",
    "\n",
    "# 3.1.4 Decoder\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        \"\"\"\n",
    "        :param layer: è§£ç å™¨å±‚layer\n",
    "        :param N: è§£ç å™¨å±‚çš„ä¸ªæ•°N\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clone(layer, N)  # å®ç°ç®€å•çš„å…‹éš†ï¼Œåœ¨å åŠ åœ¨ä¸€èµ·\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "# 3.1.5 Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attention, src_attn, feed_forward, dropout):\n",
    "        \"\"\"\n",
    "        :param self_attention: å¤šå¤´è‡ªæ³¨æ„åŠ›å¯¹è±¡ï¼Œè¯¥æ³¨æ„åŠ›æœºåˆ¶éœ€è¦Q=K=V\n",
    "        :param src_attn: å¤šå¤´æ³¨æ„åŠ›å¯¹è±¡ï¼Œè¿™é‡ŒQ!=K=V\n",
    "        :param dropout: dropoutç½®0æ¯”ç‡\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = self_attention\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clone(module=SublayerConnection(size, dropout), N=3)  # ä¸‰æ¬¡çš„è·³è¿‡è¿æ¥\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        :param x: ä¸Šä¸€å±‚çš„è¾“å…¥\n",
    "        :param memory: æ¥è‡ªç¼–ç å™¨å±‚çš„è¯­ä¹‰å­˜å‚¨å˜é‡\n",
    "        :param src_mask: æºæ•°æ®æ©ç å¼ é‡\n",
    "        :param tgt_mask: ç›®æ ‡æ•°æ®æ©ç å¼ é‡\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        å°†xä¼ å…¥ç¬¬ä¸€ä¸ªå­å±‚ç»“æ„ï¼Œç¬¬ä¸€ä¸ªå­å±‚ç»“æ„çš„è¾“å…¥åˆ†åˆ«æ˜¯xå’Œself-attnå‡½æ•°ï¼Œå› ä¸ºæ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‰€ä»¥Q,K,Véƒ½æ˜¯xï¼Œ\n",
    "        æœ€åä¸€ä¸ªå‚æ•°æ—¶ç›®æ ‡æ•°æ®æ©ç å¼ é‡ï¼Œè¿™æ—¶è¦å¯¹ç›®æ ‡æ•°æ®è¿›è¡Œé®æ©ï¼Œå› ä¸ºæ­¤æ—¶æ¨¡å‹å¯èƒ½è¿˜æ²¡æœ‰ç”Ÿæˆä»»ä½•ç›®æ ‡æ•°æ®ã€‚\n",
    "        æ¯”å¦‚åœ¨è§£ç å™¨å‡†å¤‡ç”Ÿæˆç¬¬ä¸€ä¸ªå­—ç¬¦æˆ–è¯æ±‡æ—¶ï¼Œæˆ‘ä»¬å…¶å®å·²ç»ä¼ å…¥äº†ç¬¬ä¸€ä¸ªå­—ç¬¦ä»¥ä¾¿è®¡ç®—æŸå¤±ï¼Œä½†æ˜¯æˆ‘ä»¬ä¸å¸Œæœ›åœ¨ç”Ÿæˆç¬¬ä¸€ä¸ªå­—ç¬¦æ—¶æ¨¡å‹èƒ½åˆ©ç”¨è¿™ä¸ªä¿¡æ¯ï¼Œ\n",
    "        å› æ­¤æˆ‘ä»¬ä¼šå°†å…¶é®æ©ï¼ŒåŒæ ·ç”Ÿæˆç¬¬äºŒä¸ªå­—ç¬¦æˆ–è¯æ±‡æ—¶ï¼Œæ¨¡å‹åªèƒ½ä½¿ç”¨ç¬¬ä¸€ä¸ªå­—ç¬¦æˆ–è¯æ±‡ä¿¡æ¯ï¼Œç¬¬äºŒä¸ªå­—ç¬¦ä»¥åŠä¹‹åçš„ä¿¡æ¯éƒ½ä¸å…è®¸è¢«æ¨¡å‹ä½¿ç”¨ã€‚\n",
    "        \"\"\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))  # è¾“å…¥ Queryã€Key å’Œ Value éƒ½ä¸º x å°±è¡¨ç¤ºè‡ªæ³¨æ„åŠ›ã€‚\n",
    "        \"\"\"\n",
    "        æ¥ç€è¿›å…¥ç¬¬äºŒä¸ªå­å±‚ï¼Œè¿™ä¸ªå­å±‚ä¸­å¸¸è§„çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œqæ˜¯è¾“å…¥x;\n",
    "        k,væ˜¯ç¼–ç å±‚è¾“å‡ºmemoryï¼ŒåŒæ ·ä¹Ÿä¼ å…¥source_maskï¼Œä½†æ˜¯è¿›è¡Œæºæ•°æ®é®æ©çš„åŸå› å¹¶éæ˜¯æŠ‘åˆ¶ä¿¡æ¯æ³„éœ²ï¼Œè€Œæ˜¯é®è”½æ‰å¯¹ç»“æœæ²¡æœ‰æ„ä¹‰çš„paddingã€‚\n",
    "        \"\"\"\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        \"\"\"\n",
    "        æœ€åä¸€ä¸ªå­å±‚å°±æ˜¯å‰é¦ˆå…¨è¿æ¥å­å±‚ï¼Œç»è¿‡å®ƒçš„å¤„ç†åå°±å¯ä»¥è¿”å›ç»“æœï¼Œè¿™å°±æ˜¯æˆ‘ä»¬çš„è§£ç å™¨ç»“æ„\n",
    "        \"\"\"\n",
    "        z = self.sublayer[2](x, self.feed_forward)\n",
    "        return z\n",
    "\n",
    "\n",
    "# 3.1.6 Mask\n",
    "def subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆå‘åé®æ©çš„æ©ç å¼ é‡->å½¢æˆä¸€ä¸ªä¸‰è§’çŸ©é˜µ\n",
    "    :param size: æ©ç å¼ é‡æœ€åä¸¤ä¸ªç»´åº¦çš„å¤§å°, æœ€åä¸¤ç»´å½¢æˆä¸€ä¸ªæ–¹é˜µ\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "\n",
    "    # ç„¶åä½¿ç”¨np.ones()å‘è¿™ä¸ªå½¢çŠ¶ä¸­æ·»åŠ 1å…ƒç´ ï¼Œnp.triu()å½¢æˆä¸Šä¸‰è§’é˜µ\n",
    "    mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "\n",
    "    # æœ€åå°†numpyç±»å‹è½¬åŒ–ä¸ºtorchä¸­çš„tensorï¼Œå†…éƒ¨åšä¸€ä¸ª1- çš„æ“ä½œã€‚è¿™ä¸ªå…¶å®æ˜¯åšäº†ä¸€ä¸ªä¸‰è§’é˜µçš„åè½¬ï¼Œsubsequent_maskä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½ä¼šè¢«1å‡ã€‚\n",
    "    # å¦‚æœæ˜¯0ï¼Œsubsequent_maskä¸­çš„è¯¥ä½ç½®ç”±0å˜æˆ1\n",
    "    # å¦‚æœæ˜¯1ï¼Œsubsequent_maskä¸­çš„è¯¥ä½ç½®ç”±1å˜æˆ0\n",
    "    return torch.from_numpy(mask) == 0\n",
    "\n",
    "\n",
    "# 3.2 Attention\n",
    "\n",
    "# 3.2.1 Scaled Dot-Product Attention\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    å®ç°äº†ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›\n",
    "    1. é¦–å…ˆå–queryçš„æœ€åä¸€ç»´çš„å¤§å°ï¼Œå¯¹åº”è¯åµŒå…¥ç»´åº¦\n",
    "    2. åˆ©ç”¨å…¬å¼è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°scores, è¿™é‡Œé¢keyæ˜¯å°†æœ€åä¸¤ä¸ªç»´åº¦è¿›è¡Œè½¬ç½® -> (å¥å­é•¿åº¦ç»´åº¦,è¯(å¤šå¤´)å‘é‡ç»´åº¦)\n",
    "    3. åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ©ç å¼ é‡\n",
    "    4. å¯¹scoresçš„æœ€åä¸€ç»´è¿›è¡Œsoftmaxæ“ä½œï¼Œè·å¾—æœ€ç»ˆçš„æ³¨æ„åŠ›å¼ é‡\n",
    "    5. åˆ¤æ–­æ˜¯å¦ä½¿ç”¨dropoutè¿›è¡Œéšæœºç½®0\n",
    "    6. æœ€åï¼Œå°†p_attnä¸valueå¼ é‡ç›¸ä¹˜è·å¾—æœ€ç»ˆçš„queryæ³¨æ„åŠ›è¡¨ç¤ºï¼ŒåŒæ—¶è¿”å›æ³¨æ„åŠ›å¼ é‡\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        # å°†æ©ç å¼ é‡å’Œscoreså¼ é‡æ¯ä¸ªä½ç½®ä¸€ä¸€æ¯”è¾ƒ\n",
    "        # å¦‚æœæ©ç å¼ é‡åˆ™å¯¹åº”çš„scoreså¼ é‡ç›¸åŒï¼Œåˆ™ç”¨-1e9æ¥æ›¿æ¢\n",
    "        scores = scores.masked_fill(mask == 0, value=-1e9)\n",
    "\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    attn = torch.matmul(p_attn, value)\n",
    "    return attn, p_attn\n",
    "\n",
    "\n",
    "# 3.2.2 Multi-Head Attention\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param n_heads: æ³¨æ„åŠ›å¤´æ•°\n",
    "        :param d_model: è¯åµŒå…¥ç»´åº¦\n",
    "        :param dropout: æ¯”ç‡é»˜è®¤ä¸º0.1\n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        # åˆ¤æ–­n_headsæ˜¯å¦èƒ½è¢«d_modelæ•´é™¤ -> embedding_dim / n_heads\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_k = d_model // n_heads  # 512//8=64\n",
    "        self.h = n_heads  # 8\n",
    "\n",
    "        # åˆ›å»ºlinearå±‚ï¼Œå¹¶ä¸”å…‹éš†4ä¸ª -> Q,K,Vå„ä¸€ä¸ªï¼Œæœ€åæ‹¼æ¥çš„çŸ©é˜µè¿˜éœ€è¦ä¸€ä¸ª\n",
    "        self.linear = clone(module=nn.Linear(d_model, d_model), N=4)  # 512*512\n",
    "        self.p_attn = None  # ä»£è¡¨æœ€åå¾—åˆ°çš„æ³¨æ„åŠ›å¼ é‡\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        1. ä» d_model(512) --> h*d_k(8*64) æ‰¹é‡æ‰§è¡Œæ‰€æœ‰çº¿æ€§æŠ•å½±\n",
    "        2. å°†æ³¨æ„åŠ›é›†ä¸­åœ¨æ‰€æœ‰æŠ•å½±å‘é‡ä¸Š\n",
    "        3. Concatå¹¶æœ€ç»ˆåº”ç”¨åˆ°çº¿æ€§å±‚\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # æ‹“å±•ç»´åº¦ï¼Œè¡¨ç¤ºå¤šå¤´ä¸­çš„ç¬¬nå¤´\n",
    "        n_batches = query.size(0)  # batch_sizeä»£è¡¨æœ‰å¤šå°‘æ¡æ ·æœ¬\n",
    "\n",
    "        \"\"\"\n",
    "        1. é¦–å…ˆåˆ©ç”¨zipå°†è¾“å…¥QKVä¸ä¸‰ä¸ªçº¿æ€§å±‚ç»„åˆ°ä¸€èµ·ï¼Œç„¶ååˆ©ç”¨forå¾ªç¯ï¼Œå°†è¾“å…¥QKVåˆ†åˆ«ä¼ åˆ°çº¿æ€§å±‚ä¸­,\n",
    "           ä½¿ç”¨view()å¯¹çº¿æ€§å˜æ¢çš„ç»“æ„è¿›è¡Œç»´åº¦é‡å¡‘ï¼Œä¸ºæ¯ä¸ªå¤´åˆ†å‰²è¾“å…¥\n",
    "               å¤šåŠ äº†ä¸€ä¸ªç»´åº¦hä»£è¡¨å¤´ï¼Œè¿™æ ·å°±æ„å‘³ç€æ¯ä¸ªå¤´å¯ä»¥è·å¾—ä¸€éƒ¨åˆ†è¯ç‰¹å¾ç»„æˆçš„å¥å­\n",
    "               å…¶ä¸­çš„-1ä»£è¡¨è‡ªé€‚åº”ç»´åº¦ï¼Œå³må¥å­é•¿åº¦ç»´åº¦ï¼Œå°†è‡ªåŠ¨è®¡ç®—è¿™é‡Œçš„å€¼\n",
    "           ç„¶åå¯¹ç¬¬äºŒç»´å’Œç¬¬ä¸‰ç»´è¿›è¡Œè½¬ç½®æ“ä½œï¼š\n",
    "               åŸå› ï¼šä¸ºäº†è®©ä»£è¡¨å¥å­é•¿åº¦ç»´åº¦å’Œè¯å‘é‡ç»´åº¦èƒ½å¤Ÿç›¸é‚»ï¼Œè¿™æ ·æ³¨æ„åŠ›æœºåˆ¶æ‰èƒ½æ‰¾åˆ°è¯ä¹‰ä¸å¥å­ä½ç½®çš„å…³ç³»ï¼Œ\n",
    "               ä»attentionå‡½æ•°ä¸­å¯ä»¥çœ‹åˆ°ï¼Œåˆ©ç”¨çš„æ˜¯åŸå§‹è¾“å…¥çš„å€’æ•°ç¬¬ä¸€å’Œç¬¬äºŒç»´ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†æ¯ä¸ªå¤´çš„è¾“å…¥\n",
    "        \"\"\"\n",
    "        query, key, value = [\n",
    "            lin(x).view(n_batches, -1, self.h, self.d_k).transpose(1, 2)  # -1 <-> self.h\n",
    "            for lin, x in zip(self.linear, (query, key, value))\n",
    "        ]\n",
    "        \"\"\"\n",
    "        2. å¾—åˆ°æ¯ä¸ªå¤´çš„è¾“å…¥åï¼Œæ¥ä¸‹æ¥å°±æ˜¯å°†ä»–ä»¬ä¼ å…¥åˆ°attentionä¸­ï¼Œ\n",
    "           è¿™é‡Œç›´æ¥è°ƒç”¨æˆ‘ä»¬ä¹‹å‰å®ç°çš„attentionå‡½æ•°ï¼ŒåŒæ—¶ä¹Ÿå°†maskå’Œdropoutä¼ å…¥å…¶ä¸­\n",
    "        \"\"\"\n",
    "        attn, self.p_attn = attention(query, key, value, mask, self.dropout)\n",
    "        \"\"\"\n",
    "        3. é€šè¿‡å¤šå¤´æ³¨æ„åŠ›è®¡ç®—åï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†æ¯ä¸ªå¤´è®¡ç®—ç»“æœç»„æˆçš„4ç»´å¼ é‡ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸ºè¾“å…¥çš„å½¢çŠ¶ä»¥æ–¹ä¾¿åç»­çš„è®¡ç®—ï¼Œ\n",
    "           å› æ­¤è¿™é‡Œå¼€å§‹è¿›è¡Œç¬¬ä¸€æ­¥å¤„ç†ç¯èŠ‚çš„é€†æ“ä½œï¼Œå…ˆå¯¹ç¬¬äºŒå’Œç¬¬ä¸‰ç»´è¿›è¡Œè½¬ç½®ï¼Œ\n",
    "           ç„¶åä½¿ç”¨contiguous(): èƒ½å¤Ÿè®©è½¬ç½®åçš„å¼ é‡åº”ç”¨view()ï¼Œå¦åˆ™å°†æ— æ³•ç›´æ¥ä½¿ç”¨.\n",
    "           ä¸‹ä¸€æ­¥å°±æ˜¯ä½¿ç”¨viewé‡å¡‘å½¢çŠ¶ï¼Œå˜æˆå’Œè¾“å…¥å½¢çŠ¶ç›¸åŒã€‚  \n",
    "           æœ€åä½¿ç”¨çº¿æ€§å±‚åˆ—è¡¨ä¸­çš„æœ€åä¸€ä¸ªçº¿æ€§å˜æ¢å¾—åˆ°æœ€ç»ˆçš„å¤šå¤´æ³¨æ„åŠ›ç»“æ„çš„è¾“å‡º\n",
    "        \"\"\"\n",
    "        concat = attn.transpose(1, 2).contiguous().view(n_batches, -1, self.h * self.d_k)\n",
    "        x = self.linear[-1](concat)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# 3.3 Position-wise Feed-Forward Networks\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: é€šè¿‡å‰é¦ˆå…¨è¿æ¥å±‚åè¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦ä¸å˜\n",
    "        :param d_ff: å†…éƒ¨ç»´åº¦ï¼šç¬¬äºŒä¸ªçº¿æ€§å±‚çš„è¾“å…¥ç»´åº¦å’Œç¬¬ä¸€ä¸ªçº¿æ€§å±‚çš„è¾“å‡º\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        é¦–å…ˆç»è¿‡ç¬¬ä¸€ä¸ªçº¿æ€§å±‚ï¼Œç„¶åä½¿ç”¨Fä¸­çš„reluå‡½æ•°è¿›è¡Œæ¿€æ´»ï¼Œ\n",
    "        ä¹‹åå†ä½¿ç”¨dropoutè¿›è¡Œéšæœºç½®0ï¼Œæœ€åé€šè¿‡ç¬¬äºŒä¸ªçº¿æ€§å±‚w2ï¼Œè¿”å›æœ€ç»ˆç»“æœ\n",
    "        \"\"\"\n",
    "        x = self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "        return x\n",
    "\n",
    "\n",
    "# 3.4 Embeddings and Softmax\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: è¿™é‡Œä»£è¡¨è¾“å…¥ç»™æ¨¡å‹çš„å•è¯æ–‡æœ¬é€šè¿‡è¯è¡¨æ˜ å°„åçš„one-hotå‘é‡\n",
    "        :return: å°†xä¼ ç»™self.lutå¹¶ä¸æ ¹å·ä¸‹self.d_modelç›¸ä¹˜ä½œä¸ºç»“æœè¿”å›\n",
    "        \"\"\"\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "# 3.5 Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        \"\"\"\n",
    "        :param d_model: è¯åµŒå…¥ç»´åº¦ è¿™é‡Œæ˜¯512ç»´\n",
    "        :param dropout: è¯åµŒå…¥ç»´åº¦\n",
    "        :param max_len: æ¯ä¸ªå¥å­çš„æœ€å¤§é•¿åº¦\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # ä½¿ç”¨ä¸åŸå…¬å¼ç­‰ä»·çš„è¡¨ç¤º\n",
    "        # ç›®çš„æ˜¯é¿å…ä¸­é—´çš„æ•°å€¼è®¡ç®—ç»“æœè¶…å‡ºfloatçš„èŒƒå›´\n",
    "        pos_embed = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # 0->4999 å†æ’å…¥ä¸€ä¸ªç»´åº¦(5000,1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )  # shape=[256]\n",
    "        # div_term å®ç°çš„æ˜¯åˆ†æ¯\n",
    "        # pe[:, 0::2] è¡¨ç¤ºç¬¬äºŒä¸ªç»´åº¦ä» 0 å¼€å§‹ä»¥é—´éš”ä¸º 2 å–å€¼ï¼Œå³å¶æ•°ã€‚\n",
    "        pos_embed[:, ::2] = torch.sin(position * div_term)  # shape=[max_len, 256]\n",
    "        pos_embed[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pos_embed = pos_embed.unsqueeze(0)  # shape=[1, 500, 512]\n",
    "        self.register_buffer('pe', pos_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# 3.6 Full Model\n",
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"\"\"\n",
    "    æ„å»ºæ¨¡å‹\n",
    "    :param src_vocab: è¾“å…¥è¯è¡¨å¤§å°\n",
    "    :param tgt_vocab: ç›®æ ‡è¯è¡¨å¤§å°\n",
    "    :param N: ç¼–ç å™¨å’Œè§£ç å™¨å †å çš„åŸºç¡€æ¨¡å—ä¸ªæ•°\n",
    "    :param d_model: è¯åµŒå…¥çš„ç»´åº¦\n",
    "    :param d_ff: é€ä½ç½®çš„å‰é¦ˆç½‘ç»œä¸­çš„å†…éƒ¨ç»´åº¦\n",
    "    :param h: æ³¨æ„åŠ›å¤´çš„ä¸ªæ•°\n",
    "    :param dropout:\n",
    "    \"\"\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(n_heads=h, d_model=d_model, dropout=dropout)\n",
    "    ff = PositionwiseFeedForward(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
    "    position = PositionalEncoding(d_model=d_model, dropout=dropout)\n",
    "    # -------\n",
    "    encoderLayer = EncoderLayer(size=d_model, self_attention=c(attn), feed_forward=c(ff), dropout=dropout)\n",
    "    decoderLayer = DecoderLayer(size=d_model, self_attention=c(attn), src_attn=c(attn), feed_forward=c(ff),\n",
    "                                dropout=dropout)\n",
    "    srcEmbed = Embeddings(d_model=d_model, vocab=src_vocab)\n",
    "    tgtEmbed = Embeddings(d_model=d_model, vocab=tgt_vocab)\n",
    "    generator = Generator(d_model=d_model, vocab=tgt_vocab)\n",
    "    # -------\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        encoder=Encoder(layer=encoderLayer, N=N),\n",
    "        decoder=Decoder(layer=decoderLayer, N=N),\n",
    "        src_embed=nn.Sequential(srcEmbed, c(position)),\n",
    "        tgt_embed=nn.Sequential(tgtEmbed, c(position)),\n",
    "        generator=generator\n",
    "    )\n",
    "    # åˆå§‹åŒ–å‚æ•°: ä½¿ç”¨Glorotåˆå§‹åŒ–: 1/ğ‘“ğ‘ğ‘›_ğ‘ğ‘£ğ‘”, ğ‘“ğ‘ğ‘›_ğ‘ğ‘£ğ‘”=(ğ‘“ğ‘ğ‘›_ğ‘–ğ‘› +ğ‘“ğ‘ğ‘›_ğ‘œğ‘¢ğ‘¡)/2\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa95f8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T16:07:22.883687Z",
     "iopub.status.busy": "2022-05-16T16:07:22.883167Z",
     "iopub.status.idle": "2022-05-16T16:07:23.076385Z",
     "shell.execute_reply": "2022-05-16T16:07:23.075718Z"
    },
    "papermill": {
     "duration": 0.206293,
     "end_time": "2022-05-16T16:07:23.078469",
     "exception": false,
     "start_time": "2022-05-16T16:07:22.872176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "\n",
    "# 5 Training\n",
    "# 5.1 Batched and Masking\n",
    "class Batch:\n",
    "    def __init__(self, src, tgt, pad=2):\n",
    "        \"\"\"\n",
    "        :param pad: é»˜è®¤2 è¡¨ç¤º<blank>\n",
    "        \"\"\"\n",
    "        self.src = src\n",
    "        # å°†ä¸ä»¤ç‰ŒåŒ¹é…çš„ä½ç½®è¡¨ç¤ºä¸ºFalse, å¦åˆ™ä¸ºTrue\n",
    "        # å¹¶åœ¨å€’æ•°ç¬¬äºŒä¸ªç»´åº¦åé¢æ·»åŠ ä¸€ç»´åº¦\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1]  # Decoderçš„è¾“å…¥ï¼Œå³é™¤å»æœ€åä¸€ä¸ªç»“æŸtokençš„éƒ¨åˆ†\n",
    "            self.tgt_y = tgt[:, 1:]  # Decoderçš„æœŸæœ›è¾“å…¥ï¼Œå³é™¤å»é¦–ä¸ªä¸€ä¸ªèµ·å§‹tokençš„éƒ¨åˆ†\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()  # æ‰€æœ‰Trueçš„è¯å…ƒæ•°é‡\n",
    "\n",
    "    @staticmethod\n",
    "    # staticmethod è¿”å›å‡½æ•°çš„é™æ€æ–¹æ³• å¯ä»¥ä¸å®ä¾‹åŒ–å³å¯è°ƒç”¨æ–¹æ³•\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"\"\"\n",
    "        pad å’Œ future words å‡åœ¨maskä¸­ç”¨padè¡¨ç¤º\n",
    "        \"\"\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        sequence_len = tgt.size(-1)  # æˆ–æ˜¯batchä¸­æœ€é•¿æ—¶é—´æ­¥æ•°\n",
    "        tgt_mask = tgt_mask & subsequent_mask(size=sequence_len).type_as(\n",
    "            tgt_mask.data\n",
    "            # &:è¿›è¡Œä½è¿ç®—\n",
    "            # subsequent_mask()è¿”å›ç»´åº¦ä¸º(1, size, size)\n",
    "            # type_as():å°†æ•°æ®ç±»å‹è½¬æ¢ä¸ºtgt_maskçš„æ•°æ®ç±»å‹\n",
    "        )\n",
    "        return tgt_mask\n",
    "\n",
    "\n",
    "# 5.2 Training Loop\n",
    "class TrainState:\n",
    "    \"\"\"\n",
    "    è·Ÿè¸ªå¤„ç†çš„æ­¥éª¤ã€ç¤ºä¾‹å’Œæ ‡è®°çš„æ•°é‡\n",
    "    \"\"\"\n",
    "    step: int = 0  # å½“å‰epochçš„æ­¥\n",
    "    accum_step: int = 0  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
    "    samples: int = 0  # ä½¿ç”¨çš„ç¤ºä¾‹æ€»æ•°\n",
    "    tokens: int = 0  # å¤„ç†çš„tokensæ€»æ•°\n",
    "\n",
    "\n",
    "def run_epoch(data_iter, model, loss_compute,\n",
    "              optimizer, scheduler,\n",
    "              mode=\"train\", accum_iter=1,\n",
    "              train_state=TrainState(),\n",
    "              device=None):\n",
    "    \"\"\"\n",
    "    å®Œæˆäº†ä¸€ä¸ªepochè®­ç»ƒçš„æ‰€æœ‰å·¥ä½œ\n",
    "    åŒ…æ‹¬æ•°æ®åŠ è½½ã€æ¨¡å‹æ¨ç†ã€æŸå¤±è®¡ç®—ä¸æ–¹å‘ä¼ æ’­ï¼ŒåŒæ—¶å°†è®­ç»ƒè¿‡ç¨‹ä¿¡æ¯è¿›è¡Œæ‰“å°\n",
    "    \"\"\"\n",
    "    # è®­ç»ƒå•ä¸ªepoch\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # modelæ˜¯ä¸€ä¸ªEncoderDecoderå¯¹è±¡\n",
    "        # å‰å‘ä¼ æ’­å°†src, src_maskä¼ å…¥ç¼–ç å‡½æ•°ï¼Œå¾—åˆ°ç»“æœåä¸src_mask, tgtå’Œtgt_maskä¸€åŒä¼ ç»™è§£ç å‡½æ•°\n",
    "        out = model.forward(src=batch.src, tgt=batch.tgt, src_mask=batch.src_mask, tgt_mask=batch.tgt_mask)\n",
    "        # æ¢¯åº¦ç´¯åŠ æŠ€æœ¯ loss_node = loss_node / accum_iter\n",
    "        # accum_iter:å°æ‰¹æ¬¡æ•° é»˜è®¤æ˜¯1 ä¸ä½¿ç”¨æ¢¯åº¦ç´¯åŠ æŠ€æœ¯\n",
    "        loss, loss_node = loss_compute(x=out, y=batch.tgt_y, norm=batch.ntokens)  # è®¡ç®—æŸå¤±->SimpleLossCompute\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward()  # åå‘ä¼ æ’­->ä¸è¿›è¡Œæ¢¯åº¦æ¸…é›¶, æ‰§è¡Œæ¢¯åº¦ç´¯åŠ çš„æ“ä½œ\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch.src.shape[0]\n",
    "            train_state.tokens += batch.ntokens\n",
    "            if i % accum_iter == 0:  # æ¢¯åº¦ç´¯åŠ è¾¾åˆ°å›ºå®šæ¬¡æ•°ä¹‹å\n",
    "                optimizer.step()  # æ›´æ–°å‚æ•°\n",
    "                optimizer.zero_grad(set_to_none=True)  # æ¢¯åº¦æ¸…é›¶\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]  # è·å–å­¦ä¹ ç‡\n",
    "            elapsed = time.time() - start  # è®¡ç®—40ä¸ªè¿­ä»£æ‰€éœ€æ—¶é—´\n",
    "            print((\"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \" +\n",
    "                   \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\") %\n",
    "                  (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "\n",
    "        del loss\n",
    "        del loss_node\n",
    "    return total_loss / total_tokens, train_state\n",
    "\n",
    "\n",
    "# 5.2 Optimizer\n",
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    å¯¹äº Lambda LR å‡½æ•°ï¼Œæˆ‘ä»¬å¿…é¡»å°†æ­¥éª¤é»˜è®¤ä¸º 1 é¿å…é›¶æå‡ä¸ºè´Ÿå¹‚ã€‚\n",
    "    :param step: æ—¶é—´æ­¥é•¿\n",
    "    :param model_size: æ¨¡å‹ç»´åº¦\n",
    "    :param factor: ç¤ºä¾‹ä¸­ä¸º1\n",
    "    :param warmup: é¢„çƒ­è¿­ä»£æ•°\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "            model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )\n",
    "\n",
    "\n",
    "def example_learning_schedule():\n",
    "    \"\"\"\n",
    "    å­¦ä¹ ç‡è°ƒåº¦ç¤ºä¾‹: åœ¨ opts åˆ—è¡¨ä¸­æœ‰ 3 ä¸ªç¤ºä¾‹ã€‚\n",
    "    ä¸ºæ¯ä¸ªç¤ºä¾‹è¿è¡Œ 20000 ä¸ª epoch\n",
    "    å­¦ä¹ ç‡è°ƒåº¦ä½¿ç”¨ è‡ªå®šä¹‰è°ƒæ•´å­¦ä¹ ç‡LambdaLR\n",
    "    æ•°æ®å¯è§†åŒ–å·¥å…·: Altair\n",
    "    \"\"\"\n",
    "    opts = [\n",
    "        [512, 1, 4000],  # example 1\n",
    "        [512, 1, 8000],  # example 2\n",
    "        [256, 1, 4000],  # example 3\n",
    "    ]\n",
    "    dummy_model = torch.nn.Linear(1, 1)\n",
    "    learning_rates = []\n",
    "\n",
    "    for idx, example in enumerate(opts):\n",
    "        optimizer = torch.optim.Adam(dummy_model.parameters(),\n",
    "                                     lr=1,\n",
    "                                     betas=(0.9, 0.98),\n",
    "                                     eps=1e-9)\n",
    "        lr_scheduler = LambdaLR(\n",
    "            optimizer=optimizer,\n",
    "            lr_lambda=lambda step: rate(step, *example))\n",
    "        tmp = []\n",
    "        #  é‡‡å–20000æ¬¡çš„è™šæ‹Ÿè®­ç»ƒæ­¥éª¤ï¼Œå¹¶ä¿å­˜æ¯ä¸€æ­¥çš„å­¦ä¹ ç‡\n",
    "        for step in range(20000):\n",
    "            # optimizer.param_groups[0]ï¼šé•¿åº¦ä¸º6çš„å­—å…¸ï¼Œ\n",
    "            # åŒ…æ‹¬[â€˜amsgradâ€™, â€˜paramsâ€™, â€˜lrâ€™, â€˜betasâ€™, â€˜weight_decayâ€™, â€˜epsâ€™]\n",
    "            tmp.append(optimizer.param_groups[0][\"lr\"])\n",
    "            optimizer.step()  # æ›´æ–°å‚æ•°\n",
    "            lr_scheduler.step()  # æ›´æ–°å‚æ•°\n",
    "        learning_rates.append(tmp)\n",
    "\n",
    "    learning_rates = torch.tensor(learning_rates)\n",
    "    # ----æ•°æ®å¯è§†åŒ–----\n",
    "    # ä½¿ altair èƒ½å¤Ÿå¤„ç†è¶…è¿‡ 5000 è¡Œ\n",
    "    alt.data_transformers.disable_max_rows()\n",
    "\n",
    "    opts_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Learning Rate\": learning_rates[warmup_idx, :],\n",
    "                    \"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][\n",
    "                        warmup_idx\n",
    "                    ],\n",
    "                    \"step\": range(20000),\n",
    "                }\n",
    "            )\n",
    "            for warmup_idx in [0, 1, 2]\n",
    "        ]\n",
    "    )\n",
    "    return (\n",
    "        alt.Chart(opts_data)\n",
    "            .mark_line()\n",
    "            .properties(width=600)\n",
    "            .encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\")\n",
    "            .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "# 5.3 Regularization\n",
    "\n",
    "# 5.3.2 Label Smoothing\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())\n",
    "\n",
    "\n",
    "RUN_EXAMPLES = True\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def example_label_smoothing():\n",
    "    crit = LabelSmoothing(5, 0, 0.4)\n",
    "    predict = torch.FloatTensor(\n",
    "        [\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "        ]\n",
    "    )\n",
    "    crit(x=predict.log(), target=torch.LongTensor([2, 1, 0, 3, 3]))\n",
    "    LS_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"target distribution\": crit.true_dist[x, y].flatten(),\n",
    "                    \"columns\": y,\n",
    "                    \"rows\": x,\n",
    "                }\n",
    "            )\n",
    "            for y in range(5)\n",
    "            for x in range(5)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(LS_data)\n",
    "            .mark_rect(color=\"Blue\", opacity=1)\n",
    "            .properties(height=200, width=200)\n",
    "            .encode(\n",
    "            alt.X(\"columns:O\", title=None),\n",
    "            alt.Y(\"rows:O\", title=None),\n",
    "            alt.Color(\n",
    "                \"target distribution:Q\", scale=alt.Scale(scheme=\"viridis\")\n",
    "            ),\n",
    "        )\n",
    "            .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "def loss(x, crit):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])\n",
    "    return crit(predict.log(), torch.LongTensor([1])).data\n",
    "\n",
    "\n",
    "def penalization_visualization():\n",
    "    crit = LabelSmoothing(5, 0, 0.1)\n",
    "    loss_data = pd.DataFrame({\n",
    "        \"Loss\": [loss(x, crit) for x in range(1, 100)],\n",
    "        \"Steps\": list(range(99)),\n",
    "    }).astype(\"float\")\n",
    "\n",
    "    return (alt.Chart(loss_data).mark_line().properties(width=350).encode(\n",
    "        x=\"Steps\",\n",
    "        y=\"Loss\",\n",
    "    ).interactive())\n",
    "\n",
    "\n",
    "# 6 A First Example\n",
    "\n",
    "# 6.1 Synthetic Data\n",
    "def data_gen(V, n_batches, batch_size, s_len=10, device=None):\n",
    "    \"\"\"\n",
    "    <ç¼–ç å™¨-è§£ç å™¨æ•°æ®å¤åˆ¶ä»»åŠ¡> éšæœºæ•°æ®ç”Ÿæˆå™¨\n",
    "    :param device: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ\n",
    "    :param V: è¯å…¸æ•°é‡ï¼Œå–å€¼èŒƒå›´[0, V-1]ï¼Œçº¦å®š0ä½œä¸ºç‰¹æ®Šç¬¦å·ä½¿ç”¨ä»£è¡¨padding\n",
    "    :param batch_size: æ‰¹æ¬¡å¤§å°\n",
    "    :param n_batches: éœ€è¦ç”Ÿæˆçš„æ‰¹æ¬¡æ•°é‡\n",
    "    :param s_len: ç”Ÿæˆçš„åºåˆ—æ•°æ®çš„é•¿åº¦\n",
    "    \"\"\"\n",
    "    for i in range(n_batches):\n",
    "        src_data = torch.randint(2, V, size=(batch_size, s_len))\n",
    "        # çº¦å®šè¾“å‡ºä¸ºè¾“å…¥é™¤å»åºåˆ—ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œå³å‘åå¹³ç§»ä¸€ä½è¿›è¡Œè¾“å‡ºï¼ŒåŒæ—¶è¾“å‡ºæ•°æ®è¦åœ¨ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥æ·»åŠ ä¸€ä¸ªèµ·å§‹ç¬¦\n",
    "        tgt_data = src_data.clone()\n",
    "        tgt_data[:, 0] = 1  # å°†åºåˆ—çš„ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ç½®ä¸º1(å³çº¦å®šçš„èµ·å§‹ç¬¦)\n",
    "        # .batch()\n",
    "        # è¿”å›ä¸€ä¸ªæ–°çš„tensorï¼Œä»å½“å‰è®¡ç®—å›¾ä¸­åˆ†ç¦»ä¸‹æ¥çš„ï¼Œä½†æ˜¯ä»æŒ‡å‘åŸå˜é‡çš„å­˜æ”¾ä½ç½®\n",
    "        # ä¸åŒä¹‹å¤„åªæ˜¯requires_gradä¸ºfalseï¼Œå¾—åˆ°çš„è¿™ä¸ªtensoræ°¸è¿œä¸éœ€è¦è®¡ç®—å…¶æ¢¯åº¦ï¼Œä¸å…·æœ‰gradã€‚\n",
    "        # requires_grad é»˜è®¤ä¸ºFalse\n",
    "        src = src_data.requires_grad_(False).clone().detach()\n",
    "        tgt = tgt_data.requires_grad_(False).clone().detach()\n",
    "        if device == \"cuda\":\n",
    "            src = src.cuda()\n",
    "            tgt = tgt.cuda()\n",
    "        yield Batch(src=src, tgt=tgt, pad=0)\n",
    "\n",
    "\n",
    "# 6.2 Loss Computation\n",
    "class SimpleLossCompute:\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion  # ä½¿ç”¨æ ‡ç­¾å¹³æ»‘\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        \"\"\"\n",
    "        :param x: decoderè¾“å‡ºçš„ç»“æœ\n",
    "        :param y: æ ‡ç­¾æ•°æ®\n",
    "        :param norm: lossçš„å½’ä¸€åŒ–ç³»æ•°ï¼Œç”¨batchä¸­æ‰€æœ‰æœ‰æ•ˆtokenæ•°å³å¯\n",
    "        \"\"\"\n",
    "        x = self.generator(x)\n",
    "        # contiguous():\n",
    "        # 1. ç”±äºtorch.viewç­‰æ–¹æ³•æ“ä½œéœ€è¦è¿ç»­çš„Tensor\n",
    "        # 2. å‡ºäºæ€§èƒ½è€ƒè™‘ ä½¿ç”¨è¯¥æ–¹æ³•åä¼šé‡æ–°æ®å¼€è¾Ÿä¸€å—å†…å­˜ç©ºé—´ä¿è¯æ•°æ˜¯åœ¨é€»è¾‘é¡ºåºå’Œå†…å­˜ä¸­æ˜¯ä¸€è‡´çš„\n",
    "        x_ = x.contiguous().view(-1, x.size(-1))\n",
    "        y_ = y.contiguous().view(-1)\n",
    "        loss = self.criterion(x_, y_)\n",
    "        sloss = (loss / norm)\n",
    "\n",
    "        return sloss.data * norm, loss\n",
    "\n",
    "\n",
    "# 6.3 Greedy Decoding\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    # encoder()ç¼–ç å‡½æ•°: ä½¿ç”¨src_embedå¯¹srcåšå¤„ç†ï¼Œç„¶åå’Œsrc_maskä¸€èµ·ä¼ ç»™self.encoder\n",
    "    memory = model.encode(src=src, src_mask=src_mask)\n",
    "    # ysä»£è¡¨ç›®å‰å·²ç”Ÿæˆçš„åºåˆ—ï¼Œæœ€åˆä¸ºä»…åŒ…å«ä¸€ä¸ªèµ·å§‹ç¬¦çš„åºåˆ—ï¼Œä¸æ–­å°†é¢„æµ‹ç»“æœè¿½åŠ åˆ°åºåˆ—æœ€å\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len - 1):\n",
    "        # decoder()è§£ç å‡½æ•°: ä½¿ç”¨tgt_embedå¯¹tgtåšå¤„ç†ï¼Œç„¶åå’Œsrc_mask, tgt_mask, memoryä¸€èµ·ä¼ ç»™self.decoder\n",
    "        out = model.decode(memory=memory,\n",
    "                           src_mask=src_mask,\n",
    "                           tgt=ys,\n",
    "                           tgt_mask=subsequent_mask(size=ys.size(1)).type_as(src.data))\n",
    "        # generator: ç±»åˆ«ç”Ÿæˆå™¨å¯¹è±¡ -> linear+softmax\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        # cat(): å®ç°æ‹¼æ¥æ“ä½œ\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)],\n",
    "            dim=1)\n",
    "    return ys\n",
    "\n",
    "\n",
    "# 6.4 Training Example\n",
    "# def execute_example(fn, args=[]):\n",
    "#     if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "#         fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "\n",
    "def example_simple_model(device=None):\n",
    "    V = 11  # å­—å…¸çš„å¤§å°\n",
    "    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "    model = make_model(src_vocab=V, tgt_vocab=V, N=2)\n",
    "    if device == \"cuda\":\n",
    "        model.cuda()\n",
    "    model_size = model.src_embed[0].d_model  # 512\n",
    "\n",
    "    n_epochs = 20\n",
    "    n_batch_train_epoch = 20  # è®­ç»ƒæ—¶æ¯ä¸ªepochæ‰€éœ€æ‰¹æ¬¡å¤§å°\n",
    "    n_batch_val_epoch = 5  # éªŒè¯æ—¶æ¯ä¸ªepochæ‰€éœ€æ‰¹æ¬¡å¤§å°\n",
    "    batch_size = 80\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=0.5,\n",
    "                                 betas=(0.9, 0.98),\n",
    "                                 eps=1e-9)\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(step=step, model_size=model_size, factor=0.1, warmup=400)\n",
    "    )\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_compute = SimpleLossCompute(generator=model.generator,\n",
    "                                         criterion=criterion)\n",
    "\n",
    "        print(f\"\\n|   æ‰¹æ¬¡: {epoch}   |\")\n",
    "        print(\"*\" * 5 + \"è®­ç»ƒ\" + \"*\" * 5)\n",
    "        model.train()  # self.training=True\n",
    "\n",
    "        train_data_iter = data_gen(V=V, n_batches=n_batch_train_epoch,\n",
    "                                   batch_size=batch_size, device=device)\n",
    "        run_epoch(data_iter=train_data_iter,\n",
    "                  model=model,\n",
    "                  loss_compute=loss_compute,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=lr_scheduler,\n",
    "                  mode=\"train\")\n",
    "\n",
    "        # -----------\n",
    "        print(\"*\" * 5 + \"éªŒè¯\" + \"*\" * 5)\n",
    "        model.eval()  # self.training=False\n",
    "\n",
    "        val_data_iter = data_gen(V=V, n_batches=n_batch_val_epoch,\n",
    "                                 batch_size=batch_size, device=device)\n",
    "        valid_mean_loss = run_epoch(data_iter=val_data_iter,\n",
    "                                    model=model,\n",
    "                                    loss_compute=loss_compute,\n",
    "                                    optimizer=DummyOptimizer(),  # None\n",
    "                                    scheduler=DummyScheduler(),  # None\n",
    "                                    mode=\"eval\")[0]  # è¿”å›: total_loss / total_tokens\n",
    "        print(f\"|éªŒè¯æŸå¤±: {valid_mean_loss} |\")\n",
    "\n",
    "    model.eval()\n",
    "    torch.save(model, './models/Pytorch/example_1_copy.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd77fe03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T16:07:23.098866Z",
     "iopub.status.busy": "2022-05-16T16:07:23.098643Z",
     "iopub.status.idle": "2022-05-16T16:07:32.449644Z",
     "shell.execute_reply": "2022-05-16T16:07:32.448746Z"
    },
    "papermill": {
     "duration": 9.363989,
     "end_time": "2022-05-16T16:07:32.452203",
     "exception": false,
     "start_time": "2022-05-16T16:07:23.088214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GPUtil in /opt/conda/lib/python3.7/site-packages (1.4.0)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9243f963",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T16:07:32.476192Z",
     "iopub.status.busy": "2022-05-16T16:07:32.475956Z",
     "iopub.status.idle": "2022-05-16T16:07:41.302987Z",
     "shell.execute_reply": "2022-05-16T16:07:41.302132Z"
    },
    "papermill": {
     "duration": 8.84156,
     "end_time": "2022-05-16T16:07:41.305165",
     "exception": false,
     "start_time": "2022-05-16T16:07:32.463605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "import GPUtil\n",
    "import spacy\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. åŠ è½½å¾·æ–‡ï¼Œè‹±æ–‡åˆ†è¯å™¨\n",
    "def load_tokenizers():\n",
    "    try:\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download de_core_news_sm\")\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    return spacy_de, spacy_en\n",
    "\n",
    "\n",
    "# 2. åˆ†è¯\n",
    "def tokenize(text, tokenizer):\n",
    "    \"\"\"\n",
    "    åˆ†è¯å¤„ç†\n",
    "    Spacy ä¼šå…ˆå°†æ–‡æ¡£åˆ†è§£æˆå¥å­ï¼Œç„¶åå† tokenize ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿­ä»£æ¥éå†æ•´ä¸ªæ–‡æ¡£\n",
    "    :param text: æ–‡æœ¬\n",
    "    :param tokenizer: åˆ†è¯å™¨ spacy_zh æˆ– spacy_en\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return [token.text for token in tokenizer.tokenizer(text)]\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter, tokenizer, index):\n",
    "    # index[0]:å¾·æ–‡ index[1]:è‹±æ–‡\n",
    "    for from_to_tuple in data_iter:\n",
    "        yield tokenizer(from_to_tuple[index])\n",
    "\n",
    "\n",
    "# 3. æ„å»ºè¯æ±‡è¡¨\n",
    "def build_vocabulary(spacy_de, spacy_en):\n",
    "    \"\"\"\n",
    "    æ„å»ºæ•°æ®é›†\n",
    "    - ä½¿ç”¨torchtextè‡ªå¸¦çš„æœºå™¨ç¿»è¯‘æ•°æ®é›† Multi30k\n",
    "        -- language_pair:æŒ‡å®šä½¿ç”¨çš„ç¿»è¯‘å¥å­å¯¹çš„è¯­è¨€ï¼Œé»˜è®¤æ˜¯ä»å¾·è¯­åˆ°è‹±è¯­ã€‚æ•°æ®é›†ä¸­çš„æ¯ä¸€è¡Œæ˜¯ä¸€å¯¹æŒ‡å®šè¯­è¨€çš„å¥å­å¯¹\n",
    "    - build_vocab_from_iterator() :ä»è¿­ä»£å™¨æ„å»ºè¯æ±‡å‡½æ•°\n",
    "        -- iterator: æ„å»º Vocab çš„è¿­ä»£å™¨\n",
    "        -- min_freq: åœ¨è¯æ±‡è¡¨ä¸­åŒ…å«æ ‡è®°æ‰€éœ€çš„æœ€å°é¢‘ç‡\n",
    "        -- specials: è¦æ·»åŠ çš„ç‰¹æ®Šç¬¦å·\n",
    "    \"\"\"\n",
    "    BOS_WORD = '<s>'  # beginning of sequence åºåˆ—å¼€å§‹æ ‡è¯†\n",
    "    EOS_WORD = '</s>'  # end of sequence åºåˆ—ç»“æŸæ ‡è¯†\n",
    "    BLANK_WORD = '<blank>'  # ç©ºç™½æ ‡è¯†\n",
    "    UNK_WORD = '<unk>'  # æœªçŸ¥å­—ç¬¦æ ‡è¯†\n",
    "\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text=text, tokenizer=spacy_de)\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text=text, tokenizer=spacy_en)\n",
    "\n",
    "    print(\"***æ„å»ºå¾·æ–‡æ•°æ®é›†***\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_src = build_vocab_from_iterator(\n",
    "        iterator=yield_tokens(\n",
    "            data_iter=train + val + test,\n",
    "            tokenizer=tokenize_de,\n",
    "            index=0\n",
    "        ),\n",
    "        min_freq=2,\n",
    "        specials=[BOS_WORD, EOS_WORD, BLANK_WORD, UNK_WORD]\n",
    "    )\n",
    "\n",
    "    print(\"***æ„å»ºè‹±æ–‡æ•°æ®é›†***\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_tgt = build_vocab_from_iterator(\n",
    "        iterator=yield_tokens(\n",
    "            data_iter=train + val + test,\n",
    "            tokenizer=tokenize_en,\n",
    "            index=1\n",
    "        ),\n",
    "        min_freq=2,\n",
    "        specials=[BOS_WORD, EOS_WORD, BLANK_WORD, UNK_WORD]\n",
    "    )\n",
    "\n",
    "    vocab_src.set_default_index(vocab_src[UNK_WORD])\n",
    "    vocab_tgt.set_default_index(vocab_src[UNK_WORD])\n",
    "\n",
    "    return vocab_src, vocab_tgt\n",
    "\n",
    "\n",
    "vocab_path = \"./vocab.pt\"\n",
    "\n",
    "\n",
    "def load_vocab(spacy_de, spacy_en, vocab_path):\n",
    "    if not exists(vocab_path):\n",
    "        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)\n",
    "        torch.save((vocab_src, vocab_tgt), vocab_path)\n",
    "        print(\"å®Œæˆæ„å»º!\")\n",
    "    else:\n",
    "        vocab_src, vocab_tgt = torch.load(vocab_path)\n",
    "        print(\"å®ŒæˆåŠ è½½!\")\n",
    "    print(\"å¾·æ–‡è¯æ±‡é‡ï¼š\" + str(len(vocab_src)))\n",
    "    print(\"è‹±æ–‡è¯æ±‡é‡ï¼š\" + str(len(vocab_tgt)))\n",
    "    return vocab_src, vocab_tgt\n",
    "\n",
    "\n",
    "# 4. è¿­ä»£å™¨\n",
    "def collate_batch(batch, src_pipeline, tgt_pipeline, src_vocab, tgt_vocab, device, max_padding=128, PAD_id=2):\n",
    "    \"\"\"\n",
    "    æ‰¹æ¬¡æ•°æ®æ•´ç†:æ ‡è¯†å¼€å§‹ç»“æŸtoken å¹¶è¿›è¡Œå¡«å……è‡³ç»Ÿä¸€é•¿åº¦\n",
    "    :param batch:\n",
    "    :param src_pipeline: è¾“å…¥åˆ†è¯å™¨-tokenize_de\n",
    "    :param tgt_pipeline: ç›®æ ‡åˆ†è¯å™¨-tokenize_en\n",
    "    :param src_vocab: è¾“å…¥è¯æ±‡è¡¨-vocab_src\n",
    "    :param tgt_vocab: è¾“å‡ºè¯æ±‡è¡¨-vocab_tgt\n",
    "    :param device: ä½¿ç”¨GPUåŠ é€Ÿ\n",
    "    :param max_padding: æœ€å¤§å¡«å……é»˜è®¤128\n",
    "    :param PAD_id: å¡«å……id -> <black> ç©ºç™½æ ‡è¯†\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    BOS_id = torch.tensor([0], device=device)  # <s>  åºåˆ—å¼€å§‹æ ‡è¯†ID\n",
    "    EOS_id = torch.tensor([1], device=device)  # </s> åºåˆ—ç»“æŸæ ‡è¯†ID\n",
    "    src_list, tgt_list = [], []\n",
    "    for (_src, _tgt) in batch:\n",
    "        # å¯¹è¾“å…¥æ‰¹æ¬¡è¿›è¡Œé¢„å¤„ç† æ·»åŠ åºåˆ—å¼€å§‹å’Œç»“æŸæ ‡è¯†\n",
    "        processed_src = torch.cat(\n",
    "            [\n",
    "                BOS_id,  # <s>\n",
    "                torch.tensor(\n",
    "                    src_vocab(src_pipeline(_src)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                EOS_id,  # </s>\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        # å¯¹ç›®æ ‡æ‰¹æ¬¡è¿›è¡Œé¢„å¤„ç†\n",
    "        processed_tgt = torch.cat(\n",
    "            [\n",
    "                BOS_id,  # <s>\n",
    "                torch.tensor(\n",
    "                    tgt_vocab(tgt_pipeline(_tgt)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                EOS_id,  # </s>\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # F.pad é€šè¿‡å¡«å……è¾ƒçŸ­çš„åºåˆ—æ¥å¤„ç†ï¼Œä»¥ä¾¿æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰åºåˆ—å…·æœ‰ç›¸åŒçš„é•¿åº¦\n",
    "        src_list.append(\n",
    "            F.pad(\n",
    "                input=processed_src,\n",
    "                pad=(0, max_padding - len(processed_src)),  # (0, 128-len(processed_src))\n",
    "                mode=\"constant\",\n",
    "                value=PAD_id,\n",
    "            ))\n",
    "        tgt_list.append(\n",
    "            F.pad(\n",
    "                input=processed_tgt,\n",
    "                pad=(0, max_padding - len(processed_tgt)),\n",
    "                mode=\"constant\",\n",
    "                value=PAD_id,\n",
    "            ))\n",
    "\n",
    "    # stack(): å¯¹å¼ é‡åºåˆ—è¿›è¡Œè¿æ¥\n",
    "    src = torch.stack(src_list)\n",
    "    tgt = torch.stack(tgt_list)\n",
    "\n",
    "    return (src, tgt)\n",
    "\n",
    "\n",
    "def create_dataloaders(device, vocab_src, vocab_tgt, spacy_de, spacy_en, batch_size=12000, max_padding=128,\n",
    "                       is_distributed=True):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "    :param spacy_de: å¾·æ–‡åˆ†è¯å™¨\n",
    "    :param spacy_en: è‹±æ–‡åˆ†è¯å™¨\n",
    "    :param batch_size: æ‰¹æ¬¡å¤§å°ä¸º12000\n",
    "    :param is_distributed: æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text=text, tokenizer=spacy_de)\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text=text, tokenizer=spacy_en)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return collate_batch(\n",
    "            batch=batch,\n",
    "            src_pipeline=tokenize_de,\n",
    "            tgt_pipeline=tokenize_en,\n",
    "            src_vocab=vocab_src,\n",
    "            tgt_vocab=vocab_tgt,\n",
    "            device=device,\n",
    "            max_padding=max_padding,\n",
    "            # get_stoi(): å­—å…¸å°†æ ‡è®°æ˜ å°„åˆ°ç´¢å¼•\n",
    "            PAD_id=vocab_src.get_stoi()[\"<blank>\"],  # 2\n",
    "        )\n",
    "\n",
    "    train_iter, valid_iter, test_iter = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "\n",
    "    # 1. è½¬æ¢æ•°æ®é›†ç±»å‹ä¸ºmap-style\n",
    "    #   - to_map_style_dataset(): å°†`iterable-style`æ•°æ®é›†è½¬æ¢ä¸º`map-style`æ•°æ®é›†ã€‚\n",
    "    #       - `map-style`æ˜¯ä½¿ç”¨ç´¢å¼•/é”®å‘æ•°æ®æ ·æœ¬è¿›è¡Œæ˜ å°„\n",
    "    #       - `iterable-style`çš„è¿­ä»£å‹çš„æ•°æ®é›†å°±æ˜¯çœŸæ­£è½½å…¥æ•°æ®\n",
    "    train_iter_map = to_map_style_dataset(train_iter)\n",
    "    valid_iter_map = to_map_style_dataset(valid_iter)\n",
    "\n",
    "    # 2. ä½¿ç”¨åˆ†å¸ƒå¼é‡‡æ ·å™¨\n",
    "    #   - DistributedSampler(): åˆ†å¸ƒå¼é‡‡æ ·å™¨  ç”±äºä½¿ç”¨å¤šGPUè®­ç»ƒ åŠ è½½ç­–ç•¥æ˜¯è´Ÿè´£åªæä¾›åŠ è½½æ•°æ®é›†ä¸­çš„ä¸€ä¸ªå­é›†\n",
    "    #       - ä½¿ç”¨åˆ†å¸ƒå¼é‡‡æ ·å™¨éœ€è¦æ•°æ®é›†çš„len()\n",
    "    train_sampler = (\n",
    "        DistributedSampler(dataset=train_iter_map) if is_distributed else None\n",
    "    )\n",
    "    valid_sampler = (\n",
    "        DistributedSampler(dataset=valid_iter_map) if is_distributed else None\n",
    "    )\n",
    "\n",
    "    # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼š ä½¿ç”¨DataLoader()ç»“åˆæ•°æ®é›†å’Œé‡‡æ ·å™¨ï¼Œå¹¶æä¾›å¯è¿­ä»£çš„ç»™å®šçš„æ•°æ®é›†ã€‚\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(train_sampler is None),  # å¦‚æœæœªæŒ‡å®šé‡‡æ ·å™¨åˆ™è¿›è¡Œæ··æ´—\n",
    "        sampler=train_sampler,\n",
    "        collate_fn=collate_fn,  # åœ¨ä½¿ç”¨æ‰¹é‡åŠ è½½`map-style`æ•°æ®é›†æ—¶ä½¿ç”¨ æ‰¹æ¬¡æ•°æ®æ•´ç†\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        dataset=valid_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(valid_sampler is None),\n",
    "        sampler=valid_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "\n",
    "# 5. å¤š GPU è®­ç»ƒ\n",
    "\n",
    "def train_worker(gpu, ngpus_per_node, vocab_src, vocab_tgt, spacy_de, spacy_en, config, is_distributed=False):\n",
    "    \"\"\"\n",
    "    é…ç½®è®­ç»ƒä»»åŠ¡\n",
    "    :param gpu: ä¸»æœºç¼–å·\n",
    "    :param ngpus_per_node: ä¸»æœºæ•°é‡\n",
    "    :param config: å‚æ•°é…ç½®\n",
    "    :param is_distributed: æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # ----å¤šå¡é…ç½®----\n",
    "    print(f\"ä½¿ç”¨ GPU è®­ç»ƒå·¥ä½œè¿›ç¨‹ï¼š {gpu} è®­ç»ƒ\", flush=True)\n",
    "    torch.cuda.set_device(gpu)\n",
    "\n",
    "    pad_idx = vocab_tgt[\"<blank>\"]  # ç©ºç™½å¡«å……tokençš„id\n",
    "    d_model = 512\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n",
    "    model.cuda(gpu)\n",
    "    module = model\n",
    "    is_main_process = True\n",
    "\n",
    "    if is_distributed:\n",
    "        \"\"\"\n",
    "        torch.distributedåº“: å®ç°å•æœºå¤šå¡è®­ç»ƒåº“\n",
    "        - init_process_group(): åˆå§‹åŒ–é»˜è®¤çš„åˆ†å¸ƒå¼è¿›ç¨‹ç»„\n",
    "           - backend: ä¸€èˆ¬æ¥è¯´ä½¿ç”¨NCCLå¯¹äºGPUåˆ†å¸ƒå¼è®­ç»ƒï¼Œä½¿ç”¨glooå¯¹CPUè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ\n",
    "           - init_method: URLæŒ‡å®šäº†å¦‚ä½•åˆå§‹åŒ–äº’ç›¸é€šä¿¡çš„è¿›ç¨‹    \n",
    "           - rank: ä¼˜å…ˆåº¦æˆ–gpuçš„ç¼–å·/è¿›ç¨‹çš„ç¼–å· rank=0çš„ä¸»æœºå°±æ˜¯ä¸»è¦èŠ‚ç‚¹\n",
    "           - world_size: æ‰§è¡Œè®­ç»ƒçš„æ‰€æœ‰çš„è¿›ç¨‹æ•°/GPUæ•°\n",
    "        - DistributedDataParallel(): DDPæ¨¡å¼: ä½¿ç”¨å¤šè¿›ç¨‹ï¼›æ€§èƒ½æ›´ä¼˜ï¼›æ¨¡å‹å¹¿æ’­åªåœ¨åˆå§‹åŒ–çš„æ—¶å€™, æ•…è®­ç»ƒåŠ é€Ÿ\n",
    "        \"\"\"\n",
    "        dist.init_process_group(\n",
    "            backend=\"nccl\",\n",
    "            init_method=\"env://\",\n",
    "            rank=gpu,\n",
    "            world_size=ngpus_per_node\n",
    "        )\n",
    "        model = DDP(module=model, device_ids=[gpu])\n",
    "        module = model.module\n",
    "        is_main_process = gpu == 0\n",
    "\n",
    "    # ----è®­ç»ƒé…ç½®----\n",
    "    criterion = LabelSmoothing(\n",
    "        size=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1\n",
    "    )\n",
    "    criterion.cuda(gpu)\n",
    "    # åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "    train_dataloader, valid_dataloader = create_dataloaders(\n",
    "        gpu,\n",
    "        vocab_src,\n",
    "        vocab_tgt,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        batch_size=config[\"batch_size\"] // ngpus_per_node,  # æ‰¹æ¬¡å¤§å° // æ€»GPUæ•°\n",
    "        max_padding=config[\"max_padding\"],\n",
    "        is_distributed=is_distributed,\n",
    "    )\n",
    "    # ä¼˜åŒ–å™¨\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=config[\"base_lr\"],\n",
    "                                 betas=(0.9, 0.98),\n",
    "                                 eps=1e-9)\n",
    "    # å­¦ä¹ ç‡è°ƒåº¦\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(step, d_model, factor=1, warmup=config[\"warmup\"]),\n",
    "    )\n",
    "    train_state = TrainState()  # è·Ÿè¸ªå¤„ç†çš„æ­¥éª¤ã€ç¤ºä¾‹å’Œæ ‡è®°çš„æ•°é‡\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        if is_distributed:  # ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒå°±éœ€è¦è¿›è¡Œåˆ†æ‰¹æ“ä½œ\n",
    "            train_dataloader.sampler.set_epoch(epoch)\n",
    "            valid_dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        print(f\"\\n|   æ‰¹æ¬¡: {epoch}   |\")\n",
    "        print(\"*\" * 5 + \"è®­ç»ƒ\" + \"*\" * 5)\n",
    "        model.train()\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True)\n",
    "        train_data_iter = (Batch(src=b[0], tgt=b[1], pad=pad_idx) for b in train_dataloader)  #######\n",
    "        _, train_state = run_epoch(\n",
    "            data_iter=train_data_iter,\n",
    "            model=model,\n",
    "            loss_compute=SimpleLossCompute(module.generator, criterion),\n",
    "            optimizer=optimizer,\n",
    "            scheduler=lr_scheduler,\n",
    "            mode=\"train+log\",\n",
    "            accum_iter=config[\"accum_iter\"],\n",
    "            train_state=train_state,\n",
    "        )\n",
    "\n",
    "        GPUtil.showUtilization()  # å®æ—¶æŸ¥çœ‹GPUçŠ¶å†µ\n",
    "        # ä¿å­˜æ£€æŸ¥ç‚¹æ¨¡å‹\n",
    "        if is_main_process:\n",
    "            file_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\n",
    "            torch.save(module.state_dict(), file_path)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # -----------\n",
    "        print(\"*\" * 5 + \"éªŒè¯\" + \"*\" * 5)\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True)\n",
    "        model.eval()\n",
    "        valid_data_iter = (Batch(src=b[0], tgt=b[1], pad=pad_idx) for b in valid_dataloader)  ######\n",
    "        valid_mean_loss = run_epoch(\n",
    "            data_iter=valid_data_iter,\n",
    "            model=model,\n",
    "            loss_compute=SimpleLossCompute(module.generator, criterion),\n",
    "            optimizer=DummyOptimizer(),\n",
    "            scheduler=DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "        )\n",
    "        print(valid_mean_loss)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "    if is_main_process:\n",
    "        file_path = \"%sfinal.pt\" % config[\"file_prefix\"]\n",
    "        torch.save(module.state_dict(), file_path)\n",
    "\n",
    "\n",
    "def train_distributed_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\n",
    "    \"\"\"\n",
    "    é…ç½®åˆ†å¸ƒå¼è®­ç»ƒä»»åŠ¡\n",
    "    \"\"\"\n",
    "    ngpus = torch.cuda.device_count()\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12356\"\n",
    "    print(f\"æ£€æµ‹åˆ°çš„ GPUs æ•°é‡ï¼š {ngpus}\")\n",
    "    print(\"äº§ç”Ÿè®­ç»ƒè¿‡ç¨‹ä¸­ ...\")\n",
    "\n",
    "    # torch.multiprocessing(): å®ç°pytorchå¤šè¿›ç¨‹\n",
    "    mp.spawn(\n",
    "        fn=train_worker,\n",
    "        nprocs=ngpus,\n",
    "        args=(ngpus, vocab_src, vocab_tgt, spacy_de, spacy_en, config, True),\n",
    "    )\n",
    "\n",
    "\n",
    "def train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\n",
    "    \"\"\"\n",
    "    é€‰æ‹©è®­ç»ƒä»»åŠ¡çš„è®­ç»ƒç±»å‹\n",
    "    \"\"\"\n",
    "    if config[\"distributed\"]:  # æ‰§è¡Œåˆ†å¸ƒå¼è®­ç»ƒ\n",
    "        train_distributed_model(\n",
    "            vocab_src, vocab_tgt, spacy_de, spacy_en, config\n",
    "        )\n",
    "    else:   # æ‰§è¡Œå•GPUè®­ç»ƒ\n",
    "        train_worker(\n",
    "            gpu=0,\n",
    "            ngpus_per_node=1,\n",
    "            vocab_src=vocab_src,\n",
    "            vocab_tgt=vocab_tgt,\n",
    "            spacy_de=spacy_de,\n",
    "            spacy_en=spacy_en,\n",
    "            config=config,\n",
    "            is_distributed=False\n",
    "        )\n",
    "\n",
    "\n",
    "def load_trained_model():\n",
    "    config = {\n",
    "        \"batch_size\": 32,\n",
    "        \"distributed\": False,\n",
    "        \"num_epochs\": 8,\n",
    "        \"accum_iter\": 10,\n",
    "        \"base_lr\": 1.0,\n",
    "        \"max_padding\": 72,\n",
    "        \"warmup\": 3000,\n",
    "        \"file_prefix\": \"multi30k_model_\",\n",
    "    }\n",
    "    model_path = \"multi30k_model_final.pt\"\n",
    "    if not exists(model_path):\n",
    "        train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)\n",
    "\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n",
    "    model.load_state_dict(torch.load(\"multi30k_model_final.pt\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5628bba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T16:07:41.330824Z",
     "iopub.status.busy": "2022-05-16T16:07:41.329435Z",
     "iopub.status.idle": "2022-05-16T16:28:09.367778Z",
     "shell.execute_reply": "2022-05-16T16:28:09.366914Z"
    },
    "papermill": {
     "duration": 1228.053151,
     "end_time": "2022-05-16T16:28:09.370115",
     "exception": false,
     "start_time": "2022-05-16T16:07:41.316964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.2.0/de_core_news_sm-3.2.0-py3-none-any.whl (19.1 MB)\n",
      "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 19.1/19.1 MB 20.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from de-core-news-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.63.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.21.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.3)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (59.8.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.12)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.11.3)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.2.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***æ„å»ºå¾·æ–‡æ•°æ®é›†***\n",
      "***æ„å»ºè‹±æ–‡æ•°æ®é›†***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/datapipes/iter/combining.py:181: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å®Œæˆæ„å»º!\n",
      "å¾·æ–‡è¯æ±‡é‡ï¼š8315\n",
      "è‹±æ–‡è¯æ±‡é‡ï¼š6384\n",
      "ä½¿ç”¨ GPU è®­ç»ƒå·¥ä½œè¿›ç¨‹ï¼š 0 è®­ç»ƒ\n",
      "\n",
      "|   æ‰¹æ¬¡: 0   |\n",
      "*****è®­ç»ƒ*****\n",
      "[GPU0] Epoch 0 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   7.58 | Tokens / Sec:   825.4 | Learning Rate: 5.4e-07\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   7.36 | Tokens / Sec:  2880.7 | Learning Rate: 1.1e-05\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   6.98 | Tokens / Sec:  2856.0 | Learning Rate: 2.2e-05\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   6.71 | Tokens / Sec:  2825.3 | Learning Rate: 3.3e-05\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   6.45 | Tokens / Sec:  2841.8 | Learning Rate: 4.4e-05\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   6.38 | Tokens / Sec:  2801.3 | Learning Rate: 5.4e-05\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   6.23 | Tokens / Sec:  2863.9 | Learning Rate: 6.5e-05\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   5.95 | Tokens / Sec:  2808.8 | Learning Rate: 7.6e-05\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   5.81 | Tokens / Sec:  2841.6 | Learning Rate: 8.7e-05\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   5.47 | Tokens / Sec:  2853.7 | Learning Rate: 9.7e-05\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   5.35 | Tokens / Sec:  2832.0 | Learning Rate: 1.1e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   5.07 | Tokens / Sec:  2776.3 | Learning Rate: 1.2e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   4.94 | Tokens / Sec:  2829.5 | Learning Rate: 1.3e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   4.84 | Tokens / Sec:  2819.0 | Learning Rate: 1.4e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   4.54 | Tokens / Sec:  2813.4 | Learning Rate: 1.5e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   4.47 | Tokens / Sec:  2829.4 | Learning Rate: 1.6e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   4.56 | Tokens / Sec:  2811.0 | Learning Rate: 1.7e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   4.28 | Tokens / Sec:  2828.3 | Learning Rate: 1.8e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   4.12 | Tokens / Sec:  2836.7 | Learning Rate: 1.9e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   4.28 | Tokens / Sec:  2845.6 | Learning Rate: 2.0e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   4.15 | Tokens / Sec:  2821.5 | Learning Rate: 2.2e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   4.12 | Tokens / Sec:  2843.6 | Learning Rate: 2.3e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   4.08 | Tokens / Sec:  2850.2 | Learning Rate: 2.4e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 60% | 26% |\n",
      "*****éªŒè¯*****\n",
      "[GPU0] Epoch 0 Validation ====\n",
      "(tensor(3.9669, device='cuda:0'), <__main__.TrainState object at 0x7f5210239c10>)\n",
      "\n",
      "|   æ‰¹æ¬¡: 1   |\n",
      "*****è®­ç»ƒ*****\n",
      "[GPU0] Epoch 1 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   4.16 | Tokens / Sec:  3167.9 | Learning Rate: 2.4e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   3.97 | Tokens / Sec:  2784.8 | Learning Rate: 2.6e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   4.04 | Tokens / Sec:  2796.5 | Learning Rate: 2.7e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   3.76 | Tokens / Sec:  2840.4 | Learning Rate: 2.8e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   3.72 | Tokens / Sec:  2829.4 | Learning Rate: 2.9e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   3.71 | Tokens / Sec:  2834.6 | Learning Rate: 3.0e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   3.54 | Tokens / Sec:  2832.1 | Learning Rate: 3.1e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   3.69 | Tokens / Sec:  2863.1 | Learning Rate: 3.2e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   3.60 | Tokens / Sec:  2858.6 | Learning Rate: 3.3e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   3.49 | Tokens / Sec:  2809.6 | Learning Rate: 3.4e-04\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   3.27 | Tokens / Sec:  2835.1 | Learning Rate: 3.5e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   3.51 | Tokens / Sec:  2826.6 | Learning Rate: 3.6e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   3.66 | Tokens / Sec:  2857.5 | Learning Rate: 3.7e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   3.59 | Tokens / Sec:  2843.2 | Learning Rate: 3.8e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   3.58 | Tokens / Sec:  2862.0 | Learning Rate: 4.0e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   3.33 | Tokens / Sec:  2813.8 | Learning Rate: 4.1e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   3.24 | Tokens / Sec:  2820.7 | Learning Rate: 4.2e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   3.26 | Tokens / Sec:  2833.1 | Learning Rate: 4.3e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   3.27 | Tokens / Sec:  2848.2 | Learning Rate: 4.4e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   3.35 | Tokens / Sec:  2919.9 | Learning Rate: 4.5e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   3.30 | Tokens / Sec:  2824.6 | Learning Rate: 4.6e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   3.33 | Tokens / Sec:  2835.3 | Learning Rate: 4.7e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   3.00 | Tokens / Sec:  2868.4 | Learning Rate: 4.8e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 96% | 31% |\n",
      "*****éªŒè¯*****\n",
      "[GPU0] Epoch 1 Validation ====\n",
      "(tensor(3.0281, device='cuda:0'), <__main__.TrainState object at 0x7f5210239c10>)\n",
      "\n",
      "|   æ‰¹æ¬¡: 2   |\n",
      "*****è®­ç»ƒ*****\n",
      "[GPU0] Epoch 2 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   3.12 | Tokens / Sec:  3347.4 | Learning Rate: 4.9e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   3.17 | Tokens / Sec:  2842.0 | Learning Rate: 5.0e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   2.83 | Tokens / Sec:  2841.5 | Learning Rate: 5.1e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   2.97 | Tokens / Sec:  2813.7 | Learning Rate: 5.2e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   3.09 | Tokens / Sec:  2836.0 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   2.87 | Tokens / Sec:  2836.9 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   3.00 | Tokens / Sec:  2856.0 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   2.80 | Tokens / Sec:  2797.3 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   2.83 | Tokens / Sec:  2764.5 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   2.52 | Tokens / Sec:  2831.7 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   2.65 | Tokens / Sec:  2867.2 | Learning Rate: 6.0e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   3.10 | Tokens / Sec:  2860.3 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   2.59 | Tokens / Sec:  2856.5 | Learning Rate: 6.2e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   2.42 | Tokens / Sec:  2844.0 | Learning Rate: 6.3e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   2.64 | Tokens / Sec:  2860.4 | Learning Rate: 6.4e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   2.55 | Tokens / Sec:  2839.6 | Learning Rate: 6.5e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   2.31 | Tokens / Sec:  2820.6 | Learning Rate: 6.6e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   2.85 | Tokens / Sec:  2846.2 | Learning Rate: 6.7e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   2.51 | Tokens / Sec:  2790.9 | Learning Rate: 6.8e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   2.63 | Tokens / Sec:  2865.6 | Learning Rate: 6.9e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   2.46 | Tokens / Sec:  2855.9 | Learning Rate: 7.0e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   2.36 | Tokens / Sec:  2858.5 | Learning Rate: 7.1e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   2.34 | Tokens / Sec:  2878.1 | Learning Rate: 7.3e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 94% | 31% |\n",
      "*****éªŒè¯*****\n",
      "[GPU0] Epoch 2 Validation ====\n",
      "(tensor(2.3495, device='cuda:0'), <__main__.TrainState object at 0x7f5210239c10>)\n",
      "\n",
      "|   æ‰¹æ¬¡: 3   |\n",
      "*****è®­ç»ƒ*****\n",
      "[GPU0] Epoch 3 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   2.30 | Tokens / Sec:  3084.9 | Learning Rate: 7.3e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   2.23 | Tokens / Sec:  2816.2 | Learning Rate: 7.4e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   2.17 | Tokens / Sec:  2853.1 | Learning Rate: 7.5e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   2.51 | Tokens / Sec:  2814.0 | Learning Rate: 7.6e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   2.05 | Tokens / Sec:  2822.2 | Learning Rate: 7.8e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   2.39 | Tokens / Sec:  2845.4 | Learning Rate: 7.9e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   2.02 | Tokens / Sec:  2748.0 | Learning Rate: 8.0e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   2.15 | Tokens / Sec:  2846.1 | Learning Rate: 8.1e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   2.39 | Tokens / Sec:  2839.0 | Learning Rate: 8.0e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   2.44 | Tokens / Sec:  2849.7 | Learning Rate: 8.0e-04\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   2.14 | Tokens / Sec:  2886.9 | Learning Rate: 7.9e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   2.21 | Tokens / Sec:  2809.5 | Learning Rate: 7.9e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   2.02 | Tokens / Sec:  2825.9 | Learning Rate: 7.8e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   2.13 | Tokens / Sec:  2807.1 | Learning Rate: 7.8e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   2.15 | Tokens / Sec:  2858.1 | Learning Rate: 7.7e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   2.26 | Tokens / Sec:  2897.7 | Learning Rate: 7.7e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   2.09 | Tokens / Sec:  2811.1 | Learning Rate: 7.6e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   2.11 | Tokens / Sec:  2850.1 | Learning Rate: 7.6e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   1.74 | Tokens / Sec:  2820.9 | Learning Rate: 7.5e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   1.67 | Tokens / Sec:  2812.8 | Learning Rate: 7.5e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   2.11 | Tokens / Sec:  2807.2 | Learning Rate: 7.4e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   1.83 | Tokens / Sec:  2790.6 | Learning Rate: 7.4e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   1.94 | Tokens / Sec:  2851.3 | Learning Rate: 7.4e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 77% | 31% |\n",
      "*****éªŒè¯*****\n",
      "[GPU0] Epoch 3 Validation ====\n",
      "(tensor(1.9626, device='cuda:0'), <__main__.TrainState object at 0x7f5210239c10>)\n",
      "\n",
      "|   æ‰¹æ¬¡: 4   |\n",
      "*****è®­ç»ƒ*****\n",
      "[GPU0] Epoch 4 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   1.68 | Tokens / Sec:  3411.7 | Learning Rate: 7.3e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   1.95 | Tokens / Sec:  2865.8 | Learning Rate: 7.3e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   2.31 | Tokens / Sec:  2848.4 | Learning Rate: 7.3e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   1.71 | Tokens / Sec:  2763.0 | Learning Rate: 7.2e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   1.83 | Tokens / Sec:  2876.2 | Learning Rate: 7.2e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   1.88 | Tokens / Sec:  2803.9 | Learning Rate: 7.1e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   2.13 | Tokens / Sec:  2840.7 | Learning Rate: 7.1e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   2.06 | Tokens / Sec:  2822.8 | Learning Rate: 7.1e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   1.81 | Tokens / Sec:  2836.3 | Learning Rate: 7.0e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   1.87 | Tokens / Sec:  2812.7 | Learning Rate: 7.0e-04\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   1.73 | Tokens / Sec:  2837.5 | Learning Rate: 7.0e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   1.89 | Tokens / Sec:  2845.3 | Learning Rate: 6.9e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   1.70 | Tokens / Sec:  2877.9 | Learning Rate: 6.9e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   1.72 | Tokens / Sec:  2895.8 | Learning Rate: 6.9e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   1.77 | Tokens / Sec:  2857.9 | Learning Rate: 6.8e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   1.54 | Tokens / Sec:  2865.8 | Learning Rate: 6.8e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   1.83 | Tokens / Sec:  2871.7 | Learning Rate: 6.8e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   1.81 | Tokens / Sec:  2807.2 | Learning Rate: 6.7e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   1.63 | Tokens / Sec:  2810.2 | Learning Rate: 6.7e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   1.73 | Tokens / Sec:  2835.1 | Learning Rate: 6.7e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   1.71 | Tokens / Sec:  2815.2 | Learning Rate: 6.6e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   1.66 | Tokens / Sec:  2856.6 | Learning Rate: 6.6e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   1.82 | Tokens / Sec:  2804.8 | Learning Rate: 6.6e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 56% | 31% |\n",
      "*****éªŒè¯*****\n",
      "[GPU0] Epoch 4 Validation ====\n",
      "(tensor(1.7757, device='cuda:0'), <__main__.TrainState object at 0x7f5210239c10>)\n",
      "\n",
      "|   æ‰¹æ¬¡: 5   |\n",
      "*****è®­ç»ƒ*****\n",
      "[GPU0] Epoch 5 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   1.60 | Tokens / Sec:  3143.2 | Learning Rate: 6.6e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   1.53 | Tokens / Sec:  2836.1 | Learning Rate: 6.5e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   1.54 | Tokens / Sec:  2812.3 | Learning Rate: 6.5e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   1.51 | Tokens / Sec:  2814.1 | Learning Rate: 6.5e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   1.45 | Tokens / Sec:  2879.2 | Learning Rate: 6.4e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   1.32 | Tokens / Sec:  2810.0 | Learning Rate: 6.4e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   1.75 | Tokens / Sec:  2841.6 | Learning Rate: 6.4e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   1.73 | Tokens / Sec:  2814.1 | Learning Rate: 6.4e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   1.58 | Tokens / Sec:  2830.9 | Learning Rate: 6.3e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   1.61 | Tokens / Sec:  2851.9 | Learning Rate: 6.3e-04\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   1.60 | Tokens / Sec:  2881.4 | Learning Rate: 6.3e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   1.56 | Tokens / Sec:  2823.2 | Learning Rate: 6.3e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   1.53 | Tokens / Sec:  2832.9 | Learning Rate: 6.2e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   1.66 | Tokens / Sec:  2760.7 | Learning Rate: 6.2e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   1.49 | Tokens / Sec:  2892.9 | Learning Rate: 6.2e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   1.66 | Tokens / Sec:  2824.4 | Learning Rate: 6.2e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   1.48 | Tokens / Sec:  2848.2 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   1.36 | Tokens / Sec:  2923.6 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   1.78 | Tokens / Sec:  2841.6 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   1.80 | Tokens / Sec:  2870.4 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   1.27 | Tokens / Sec:  2821.5 | Learning Rate: 6.0e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   1.64 | Tokens / Sec:  2842.7 | Learning Rate: 6.0e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   1.91 | Tokens / Sec:  2810.9 | Learning Rate: 6.0e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 80% | 31% |\n",
      "*****éªŒè¯*****\n",
      "[GPU0] Epoch 5 Validation ====\n",
      "(tensor(1.6801, device='cuda:0'), <__main__.TrainState object at 0x7f5210239c10>)\n",
      "\n",
      "|   æ‰¹æ¬¡: 6   |\n",
      "*****è®­ç»ƒ*****\n",
      "[GPU0] Epoch 6 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   1.40 | Tokens / Sec:  3227.7 | Learning Rate: 6.0e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   1.46 | Tokens / Sec:  2857.9 | Learning Rate: 6.0e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   1.23 | Tokens / Sec:  2870.3 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   1.59 | Tokens / Sec:  2834.6 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   1.15 | Tokens / Sec:  2845.6 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   1.41 | Tokens / Sec:  2828.3 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   1.31 | Tokens / Sec:  2811.7 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   1.39 | Tokens / Sec:  2831.7 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   1.43 | Tokens / Sec:  2791.7 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   1.63 | Tokens / Sec:  2846.4 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   1.47 | Tokens / Sec:  2814.6 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   1.43 | Tokens / Sec:  2863.8 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   1.41 | Tokens / Sec:  2812.8 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   1.54 | Tokens / Sec:  2895.1 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   1.41 | Tokens / Sec:  2882.4 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   1.42 | Tokens / Sec:  2862.1 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   1.24 | Tokens / Sec:  2855.6 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   1.42 | Tokens / Sec:  2819.9 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   1.54 | Tokens / Sec:  2858.1 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   1.36 | Tokens / Sec:  2826.9 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   1.53 | Tokens / Sec:  2887.3 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   1.42 | Tokens / Sec:  2822.2 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   1.58 | Tokens / Sec:  2849.3 | Learning Rate: 5.6e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 86% | 31% |\n",
      "*****éªŒè¯*****\n",
      "[GPU0] Epoch 6 Validation ====\n",
      "(tensor(1.6165, device='cuda:0'), <__main__.TrainState object at 0x7f5210239c10>)\n",
      "\n",
      "|   æ‰¹æ¬¡: 7   |\n",
      "*****è®­ç»ƒ*****\n",
      "[GPU0] Epoch 7 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   1.32 | Tokens / Sec:  3409.2 | Learning Rate: 5.5e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   1.23 | Tokens / Sec:  2816.7 | Learning Rate: 5.5e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   1.16 | Tokens / Sec:  2844.1 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   1.24 | Tokens / Sec:  2860.7 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   1.37 | Tokens / Sec:  2799.3 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   1.00 | Tokens / Sec:  2854.4 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   1.07 | Tokens / Sec:  2834.8 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   1.30 | Tokens / Sec:  2894.7 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   1.26 | Tokens / Sec:  2872.2 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   1.42 | Tokens / Sec:  2817.0 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   1.66 | Tokens / Sec:  2831.4 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   1.24 | Tokens / Sec:  2844.0 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   1.42 | Tokens / Sec:  2882.4 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   1.26 | Tokens / Sec:  2819.4 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   1.45 | Tokens / Sec:  2894.7 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   1.46 | Tokens / Sec:  2809.3 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   1.28 | Tokens / Sec:  2855.1 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   1.20 | Tokens / Sec:  2846.2 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   1.20 | Tokens / Sec:  2832.8 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   1.38 | Tokens / Sec:  2888.6 | Learning Rate: 5.2e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   1.43 | Tokens / Sec:  2803.5 | Learning Rate: 5.2e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   1.27 | Tokens / Sec:  2835.9 | Learning Rate: 5.2e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   1.37 | Tokens / Sec:  2793.6 | Learning Rate: 5.2e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 94% | 31% |\n",
      "*****éªŒè¯*****\n",
      "[GPU0] Epoch 7 Validation ====\n",
      "(tensor(1.5847, device='cuda:0'), <__main__.TrainState object at 0x7f5210239c10>)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spacy_de, spacy_en = load_tokenizers()\n",
    "    vocab_path = \"./vocab.pt\"\n",
    "    vocab_src, vocab_tgt = load_vocab(spacy_de, spacy_en, vocab_path)\n",
    "    model = load_trained_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ad1f3",
   "metadata": {
    "papermill": {
     "duration": 0.076626,
     "end_time": "2022-05-16T16:28:09.524845",
     "exception": false,
     "start_time": "2022-05-16T16:28:09.448219",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1281.863644,
   "end_time": "2022-05-16T16:28:12.496663",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-16T16:06:50.633019",
   "version": "2.3.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "ç›®å½•",
   "title_sidebar": "ç›®å½•",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
