{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e02517c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:22.472925Z",
     "iopub.status.busy": "2022-05-18T15:04:22.472367Z",
     "iopub.status.idle": "2022-05-18T15:04:32.974461Z",
     "shell.execute_reply": "2022-05-18T15:04:32.973252Z"
    },
    "papermill": {
     "duration": 10.523276,
     "end_time": "2022-05-18T15:04:32.977092",
     "exception": false,
     "start_time": "2022-05-18T15:04:22.453816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /home/sora/anaconda3/envs/DL/lib/python3.10/site-packages (1.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4050046",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:33.014783Z",
     "iopub.status.busy": "2022-05-18T15:04:33.014554Z",
     "iopub.status.idle": "2022-05-18T15:04:33.714413Z",
     "shell.execute_reply": "2022-05-18T15:04:33.712861Z"
    },
    "papermill": {
     "duration": 0.720368,
     "end_time": "2022-05-18T15:04:33.716486",
     "exception": false,
     "start_time": "2022-05-18T15:04:32.996118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 25 16:40:16 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.76       Driver Version: 515.76       CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 40%   36C    P8    N/A /  75W |    223MiB /  4096MiB |     25%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      3023      G   /usr/lib/xorg/Xorg                111MiB |\r\n",
      "|    0   N/A  N/A      3149    C+G   ...ome-remote-desktop-daemon       59MiB |\r\n",
      "|    0   N/A  N/A      3186      G   /usr/bin/gnome-shell               37MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da16be2a",
   "metadata": {
    "papermill": {
     "duration": 0.017858,
     "end_time": "2022-05-18T15:04:33.753838",
     "exception": false,
     "start_time": "2022-05-18T15:04:33.735980",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8645bc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:33.791302Z",
     "iopub.status.busy": "2022-05-18T15:04:33.790619Z",
     "iopub.status.idle": "2022-05-18T15:04:35.637634Z",
     "shell.execute_reply": "2022-05-18T15:04:35.636994Z"
    },
    "papermill": {
     "duration": 1.867772,
     "end_time": "2022-05-18T15:04:35.639447",
     "exception": false,
     "start_time": "2022-05-18T15:04:33.771675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cuda'), device(type='cuda', index=1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')\n",
    "# torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687d0891",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:35.677358Z",
     "iopub.status.busy": "2022-05-18T15:04:35.677152Z",
     "iopub.status.idle": "2022-05-18T15:04:40.416757Z",
     "shell.execute_reply": "2022-05-18T15:04:40.415945Z"
    },
    "papermill": {
     "duration": 4.761301,
     "end_time": "2022-05-18T15:04:40.418785",
     "exception": false,
     "start_time": "2022-05-18T15:04:35.657484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1 å¯¼å…¥å¿…å¤‡çš„åº“\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import altair as alt\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "# 3 Model Architecture\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å‡½æ•°\n",
    "        :param encoder: ç¼–ç å™¨å¯¹è±¡\n",
    "        :param decoder: è§£ç å™¨å¯¹è±¡\n",
    "        :param src_embed: æºæ•°æ®åµŒå…¥å‡½æ•°\n",
    "        :param tgt_embed: ç›®æ ‡æ•°æ®åµŒå…¥å‡½æ•°\n",
    "        :param generator: ç±»åˆ«ç”Ÿæˆå™¨å¯¹è±¡\n",
    "        \"\"\"\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        å°†src, src_maskä¼ å…¥ç¼–ç å‡½æ•°ï¼Œå¾—åˆ°ç»“æœåä¸src_mask, tgtå’Œtgt_maskä¸€åŒä¼ ç»™è§£ç å‡½æ•°\n",
    "        :param src: æºæ•°æ®\n",
    "        :param tgt: ç›®æ ‡æ•°æ®\n",
    "        :param src_mask: æºæ•°æ®æ©ç å¼ é‡\n",
    "        :param tgt_mask: ç›®æ ‡æ•°æ®æ©ç å¼ é‡\n",
    "        \"\"\"\n",
    "        memory = self.encode(src, src_mask)\n",
    "        res = self.decode(memory, src_mask, tgt, tgt_mask)\n",
    "        return res\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        ç¼–ç å‡½æ•°ï¼Œä½¿ç”¨src_embedå¯¹sourceåšå¤„ç†ï¼Œç„¶åå’Œsrc_maskä¸€èµ·ä¼ ç»™self.encoder\n",
    "        \"\"\"\n",
    "        source_embeddings = self.src_embed(src)\n",
    "        return self.encoder(source_embeddings, src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        \"\"\"\n",
    "        è§£ç å‡½æ•°ï¼Œä½¿ç”¨tgt_embedå¯¹targetåšå¤„ç†ï¼Œç„¶åå’Œsrc_mask,tgt_mask,memoryä¸€èµ·ä¼ ç»™self.decoder\n",
    "        \"\"\"\n",
    "        target_embeddings = self.tgt_embed(tgt)\n",
    "        return self.decoder(target_embeddings, memory, src_mask, tgt_mask)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    å°†çº¿æ€§å±‚å’Œsoftmaxè®¡ç®—å±‚ä¸€èµ·å®ç°ï¼Œ æŠŠç±»çš„åå­—å«åšGeneratorï¼Œç”Ÿæˆå™¨ç±»\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å‡½æ•°\n",
    "        :param d_model: åµŒå…¥çš„ç»´åº¦\n",
    "        :param vocab: vocab.size -> è¯è¡¨çš„å¤§å°\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(in_features=d_model, out_features=vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        è¾“å…¥æ˜¯ä¸Šä¸€å±‚çš„è¾“å‡ºå¼ é‡x\n",
    "        ä½¿ç”¨ä¸Šä¸€æ­¥å¾—åˆ°çš„self.projå¯¹xè¿›è¡Œçº¿æ€§å˜åŒ–, ç„¶åä½¿ç”¨Fä¸­å·²ç»å®ç°çš„log_softmaxè¿›è¡Œsoftmaxå¤„ç†ã€‚\n",
    "        \"\"\"\n",
    "        softmax = F.log_softmax(self.proj(x), dim=-1)\n",
    "        return softmax\n",
    "\n",
    "\n",
    "# 3.1 Encoder and Decoder Stacks\n",
    "\n",
    "# 3.1.1 Encoder\n",
    "def clone(mudule, N):\n",
    "    \"\"\"\n",
    "    ç”¨äºå…‹éš†å¤šä»½ç»“æ„\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(mudule) for _ in range(N)])\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clone(layer, N)  # å®ç°ç®€å•çš„å…‹éš†ï¼Œåœ¨å åŠ åœ¨ä¸€èµ·\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        å°†è¾“å…¥ï¼ˆå’Œæ©ç ï¼‰ä¾æ¬¡é€šè¿‡æ¯ä¸€å±‚ã€‚\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "# 3.1.2 Layer Normalization and Residual Connections\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å‡½æ•°\n",
    "        :param feature_size: è¯åµŒå…¥çš„ç»´åº¦\n",
    "        :param eps: é˜²æ­¢åˆ†æ¯ä¸º0ï¼Œé»˜è®¤æ˜¯1e-6\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # ä½¿ç”¨nn.parameterå°è£…ï¼Œä»£è¡¨ä»–ä»¬æ˜¯æ¨¡å‹çš„å‚æ•°\n",
    "        self.gamma = nn.Parameter(torch.ones(feature_size))  # ç¼©æ”¾å‚æ•°å‘é‡ åˆå§‹åŒ–ä¸º1å¼ é‡\n",
    "        self.beta = nn.Parameter(torch.zeros(feature_size))  # å¹³ç§»å‚æ•°å‘é‡ åˆå§‹åŒ–ä¸º0å¼ é‡\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1. å¯¹è¾“å…¥å˜é‡xæ±‚å…¶æœ€åä¸€ä¸ªç»´åº¦,å³è¯åµŒå…¥ç»´åº¦çš„å‡å€¼ï¼Œå¹¶ä¿æŒè¾“å‡ºç»´åº¦ä¸è¾“å…¥ç»´åº¦ä¸€è‡´\n",
    "        2. æ±‚æœ€åä¸€ä¸ªç»´åº¦çš„æ ‡å‡†å·®ï¼Œè¿›è¡Œè§„èŒƒåŒ–ï¼šç”¨xå‡å»å‡å€¼é™¤ä»¥æ ‡å‡†å·®\n",
    "        3. å¯¹ç»“æœä¹˜ä»¥æˆ‘ä»¬çš„ç¼©æ”¾å‚æ•°gamma, *è¡¨ç¤ºç‚¹ä¹˜ï¼ŒåŠ ä¸Šä½ç§»å‚beta\n",
    "        :param x: æ¥è‡ªä¸Šä¸€å±‚çš„è¾“å‡º\n",
    "        \"\"\"\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return (x - mean) / (std + self.eps) * self.gamma + self.beta  ##########\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    SublayerConnectionç±»:å®ç°å­å±‚è¿æ¥ç»“æ„.ğ‘¥è¡¨ç¤ºä¸Šä¸€å±‚æ·»åŠ äº†æ®‹å·®è¿æ¥çš„è¾“å‡ºï¼Œè¿™ä¸€å±‚æ·»åŠ äº†æ®‹å·®è¿æ¥çš„è¾“å‡ºéœ€è¦å°†  ğ‘¥  æ‰§è¡Œå±‚çº§å½’ä¸€åŒ–ï¼Œ\n",
    "    ç„¶åé¦ˆé€åˆ° Multi-Head Attention å±‚æˆ–å…¨è¿æ¥å±‚ï¼Œæ·»åŠ  Dropout æ“ä½œåå¯ä½œä¸ºè¿™ä¸€å­å±‚çº§çš„è¾“å‡ºã€‚æœ€åå°†è¯¥å­å±‚çš„è¾“å‡ºå‘é‡ä¸è¾“å…¥å‘é‡ç›¸åŠ å¾—åˆ°ä¸‹ä¸€å±‚çš„è¾“å…¥ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        \"\"\"\n",
    "        :param size: ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™=512\n",
    "        :param dropout: ä¸¢å¼ƒå‚æ•°\n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # åŸæ–¹æ¡ˆï¼šå…ˆå°†xæ‰§è¡Œå±‚çº§å½’ä¸€åŒ–\n",
    "        # sublayer_out = sublayer(self.norm(x))\n",
    "        # return x + self.dropout(sublayer_out)\n",
    "        # æ”¹è¿›ç‰ˆæœ¬ï¼šå–å‡ºnorm åŠ å¿«æ”¶æ•›é€Ÿåº¦\n",
    "        sublayer_out = sublayer(x)\n",
    "        sublayer_out = self.dropout(sublayer_out)\n",
    "        return x + self.norm(sublayer_out)\n",
    "\n",
    "\n",
    "# 3.1.3 Encoder Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attention, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clone(mudule=SublayerConnection(size, dropout), N=2)  # ä¸¤æ¬¡çš„è·³è¿‡è¿æ¥\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        ç¬¬ä¸€ä¸ªå­å±‚åŒ…æ‹¬ä¸€ä¸ªå¤šå¤´è‡ªæ³¨æ„åŠ›å±‚å’Œè§„èŒƒåŒ–å±‚ä»¥åŠä¸€ä¸ªæ®‹å·®è¿æ¥\n",
    "        ç¬¬äºŒä¸ªå­å±‚åŒ…æ‹¬ä¸€ä¸ªå‰é¦ˆå…¨è¿æ¥å±‚å’Œè§„èŒƒåŒ–å±‚ä»¥åŠä¸€ä¸ªæ®‹å·®è¿æ¥\n",
    "        \"\"\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))  # è¾“å…¥ Queryã€Key å’Œ Value éƒ½ä¸º x å°±è¡¨ç¤ºè‡ªæ³¨æ„åŠ›ã€‚\n",
    "        z = self.sublayer[1](x, self.feed_forward)\n",
    "        return z\n",
    "\n",
    "\n",
    "# 3.1.4 Decoder\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        \"\"\"\n",
    "        :param layer: è§£ç å™¨å±‚layer\n",
    "        :param N: è§£ç å™¨å±‚çš„ä¸ªæ•°N\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clone(layer, N)  # å®ç°ç®€å•çš„å…‹éš†ï¼Œåœ¨å åŠ åœ¨ä¸€èµ·\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "# 3.1.5 Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attention, src_attn, feed_forward, dropout):\n",
    "        \"\"\"\n",
    "        :param self_attention: å¤šå¤´è‡ªæ³¨æ„åŠ›å¯¹è±¡ï¼Œè¯¥æ³¨æ„åŠ›æœºåˆ¶éœ€è¦Q=K=V\n",
    "        :param src_attn: å¤šå¤´æ³¨æ„åŠ›å¯¹è±¡ï¼Œè¿™é‡ŒQ!=K=V\n",
    "        :param dropout: dropoutç½®0æ¯”ç‡\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = self_attention\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clone(mudule=SublayerConnection(size, dropout), N=3)  # ä¸‰æ¬¡çš„è·³è¿‡è¿æ¥\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        :param x: ä¸Šä¸€å±‚çš„è¾“å…¥\n",
    "        :param memory: æ¥è‡ªç¼–ç å™¨å±‚çš„è¯­ä¹‰å­˜å‚¨å˜é‡\n",
    "        :param src_mask: æºæ•°æ®æ©ç å¼ é‡\n",
    "        :param tgt_mask: ç›®æ ‡æ•°æ®æ©ç å¼ é‡\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        å°†xä¼ å…¥ç¬¬ä¸€ä¸ªå­å±‚ç»“æ„ï¼Œç¬¬ä¸€ä¸ªå­å±‚ç»“æ„çš„è¾“å…¥åˆ†åˆ«æ˜¯xå’Œself-attnå‡½æ•°ï¼Œå› ä¸ºæ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‰€ä»¥Q,K,Véƒ½æ˜¯xï¼Œ\n",
    "        æœ€åä¸€ä¸ªå‚æ•°æ—¶ç›®æ ‡æ•°æ®æ©ç å¼ é‡ï¼Œè¿™æ—¶è¦å¯¹ç›®æ ‡æ•°æ®è¿›è¡Œé®æ©ï¼Œå› ä¸ºæ­¤æ—¶æ¨¡å‹å¯èƒ½è¿˜æ²¡æœ‰ç”Ÿæˆä»»ä½•ç›®æ ‡æ•°æ®ã€‚\n",
    "        æ¯”å¦‚åœ¨è§£ç å™¨å‡†å¤‡ç”Ÿæˆç¬¬ä¸€ä¸ªå­—ç¬¦æˆ–è¯æ±‡æ—¶ï¼Œæˆ‘ä»¬å…¶å®å·²ç»ä¼ å…¥äº†ç¬¬ä¸€ä¸ªå­—ç¬¦ä»¥ä¾¿è®¡ç®—æŸå¤±ï¼Œä½†æ˜¯æˆ‘ä»¬ä¸å¸Œæœ›åœ¨ç”Ÿæˆç¬¬ä¸€ä¸ªå­—ç¬¦æ—¶æ¨¡å‹èƒ½åˆ©ç”¨è¿™ä¸ªä¿¡æ¯ï¼Œ\n",
    "        å› æ­¤æˆ‘ä»¬ä¼šå°†å…¶é®æ©ï¼ŒåŒæ ·ç”Ÿæˆç¬¬äºŒä¸ªå­—ç¬¦æˆ–è¯æ±‡æ—¶ï¼Œæ¨¡å‹åªèƒ½ä½¿ç”¨ç¬¬ä¸€ä¸ªå­—ç¬¦æˆ–è¯æ±‡ä¿¡æ¯ï¼Œç¬¬äºŒä¸ªå­—ç¬¦ä»¥åŠä¹‹åçš„ä¿¡æ¯éƒ½ä¸å…è®¸è¢«æ¨¡å‹ä½¿ç”¨ã€‚\n",
    "        \"\"\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))  # è¾“å…¥ Queryã€Key å’Œ Value éƒ½ä¸º x å°±è¡¨ç¤ºè‡ªæ³¨æ„åŠ›ã€‚\n",
    "        \"\"\"\n",
    "        æ¥ç€è¿›å…¥ç¬¬äºŒä¸ªå­å±‚ï¼Œè¿™ä¸ªå­å±‚ä¸­å¸¸è§„çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œqæ˜¯è¾“å…¥x;\n",
    "        k,væ˜¯ç¼–ç å±‚è¾“å‡ºmemoryï¼ŒåŒæ ·ä¹Ÿä¼ å…¥source_maskï¼Œä½†æ˜¯è¿›è¡Œæºæ•°æ®é®æ©çš„åŸå› å¹¶éæ˜¯æŠ‘åˆ¶ä¿¡æ¯æ³„éœ²ï¼Œè€Œæ˜¯é®è”½æ‰å¯¹ç»“æœæ²¡æœ‰æ„ä¹‰çš„paddingã€‚\n",
    "        \"\"\"\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        \"\"\"\n",
    "        æœ€åä¸€ä¸ªå­å±‚å°±æ˜¯å‰é¦ˆå…¨è¿æ¥å­å±‚ï¼Œç»è¿‡å®ƒçš„å¤„ç†åå°±å¯ä»¥è¿”å›ç»“æœï¼Œè¿™å°±æ˜¯æˆ‘ä»¬çš„è§£ç å™¨ç»“æ„\n",
    "        \"\"\"\n",
    "        z = self.sublayer[2](x, self.feed_forward)\n",
    "        return z\n",
    "\n",
    "\n",
    "# 3.1.6 Mask\n",
    "def subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆå‘åé®æ©çš„æ©ç å¼ é‡->å½¢æˆä¸€ä¸ªä¸‰è§’çŸ©é˜µ\n",
    "    :param size: æ©ç å¼ é‡æœ€åä¸¤ä¸ªç»´åº¦çš„å¤§å°, æœ€åä¸¤ç»´å½¢æˆä¸€ä¸ªæ–¹é˜µ\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "\n",
    "    # ç„¶åä½¿ç”¨np.ones()å‘è¿™ä¸ªå½¢çŠ¶ä¸­æ·»åŠ 1å…ƒç´ ï¼Œnp.triu()å½¢æˆä¸Šä¸‰è§’é˜µ\n",
    "    mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "\n",
    "    # æœ€åå°†numpyç±»å‹è½¬åŒ–ä¸ºtorchä¸­çš„tensorï¼Œå†…éƒ¨åšä¸€ä¸ª1- çš„æ“ä½œã€‚è¿™ä¸ªå…¶å®æ˜¯åšäº†ä¸€ä¸ªä¸‰è§’é˜µçš„åè½¬ï¼Œsubsequent_maskä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½ä¼šè¢«1å‡ã€‚\n",
    "    # å¦‚æœæ˜¯0ï¼Œsubsequent_maskä¸­çš„è¯¥ä½ç½®ç”±0å˜æˆ1\n",
    "    # å¦‚æœæ˜¯1ï¼Œsubsequent_maskä¸­çš„è¯¥ä½ç½®ç”±1å˜æˆ0\n",
    "    return torch.from_numpy(mask) == 0\n",
    "\n",
    "\n",
    "# 3.2 Attention\n",
    "\n",
    "# 3.2.1 Scaled Dot-Product Attention\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    å®ç°äº†ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›\n",
    "    1. é¦–å…ˆå–queryçš„æœ€åä¸€ç»´çš„å¤§å°ï¼Œå¯¹åº”è¯åµŒå…¥ç»´åº¦\n",
    "    2. åˆ©ç”¨å…¬å¼è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°scores, è¿™é‡Œé¢keyæ˜¯å°†æœ€åä¸¤ä¸ªç»´åº¦è¿›è¡Œè½¬ç½® -> (å¥å­é•¿åº¦ç»´åº¦,è¯(å¤šå¤´)å‘é‡ç»´åº¦)\n",
    "    3. åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ©ç å¼ é‡\n",
    "    4. å¯¹scoresçš„æœ€åä¸€ç»´è¿›è¡Œsoftmaxæ“ä½œï¼Œè·å¾—æœ€ç»ˆçš„æ³¨æ„åŠ›å¼ é‡\n",
    "    5. åˆ¤æ–­æ˜¯å¦ä½¿ç”¨dropoutè¿›è¡Œéšæœºç½®0\n",
    "    6. æœ€åï¼Œå°†p_attnä¸valueå¼ é‡ç›¸ä¹˜è·å¾—æœ€ç»ˆçš„queryæ³¨æ„åŠ›è¡¨ç¤ºï¼ŒåŒæ—¶è¿”å›æ³¨æ„åŠ›å¼ é‡\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        # å°†æ©ç å¼ é‡å’Œscoreså¼ é‡æ¯ä¸ªä½ç½®ä¸€ä¸€æ¯”è¾ƒ\n",
    "        # å¦‚æœæ©ç å¼ é‡åˆ™å¯¹åº”çš„scoreså¼ é‡ç›¸åŒï¼Œåˆ™ç”¨-1e9æ¥æ›¿æ¢\n",
    "        scores = scores.masked_fill(mask == 0, value=-1e9)\n",
    "\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    attn = torch.matmul(p_attn, value)\n",
    "    return attn, p_attn\n",
    "\n",
    "\n",
    "# 3.2.2 Multi-Head Attention\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param n_heads: æ³¨æ„åŠ›å¤´æ•°\n",
    "        :param d_model: è¯åµŒå…¥ç»´åº¦\n",
    "        :param dropout: æ¯”ç‡é»˜è®¤ä¸º0.1\n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        # åˆ¤æ–­n_headsæ˜¯å¦èƒ½è¢«d_modelæ•´é™¤ -> embedding_dim / n_heads\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_k = d_model // n_heads  # 512//8=64\n",
    "        self.h = n_heads  # 8\n",
    "\n",
    "        # åˆ›å»ºlinearå±‚ï¼Œå¹¶ä¸”å…‹éš†4ä¸ª -> Q,K,Vå„ä¸€ä¸ªï¼Œæœ€åæ‹¼æ¥çš„çŸ©é˜µè¿˜éœ€è¦ä¸€ä¸ª\n",
    "        self.linear = clone(mudule=nn.Linear(d_model, d_model), N=4)  # 512*512\n",
    "        self.p_attn = None  # ä»£è¡¨æœ€åå¾—åˆ°çš„æ³¨æ„åŠ›å¼ é‡\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        1. ä» d_model(512) --> h*d_k(8*64) æ‰¹é‡æ‰§è¡Œæ‰€æœ‰çº¿æ€§æŠ•å½±\n",
    "        2. å°†æ³¨æ„åŠ›é›†ä¸­åœ¨æ‰€æœ‰æŠ•å½±å‘é‡ä¸Š\n",
    "        3. Concatå¹¶æœ€ç»ˆåº”ç”¨åˆ°çº¿æ€§å±‚\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # æ‹“å±•ç»´åº¦ï¼Œè¡¨ç¤ºå¤šå¤´ä¸­çš„ç¬¬nå¤´\n",
    "        n_batches = query.size(0)  # batch_sizeä»£è¡¨æœ‰å¤šå°‘æ¡æ ·æœ¬\n",
    "\n",
    "        \"\"\"\n",
    "        1. é¦–å…ˆåˆ©ç”¨zipå°†è¾“å…¥QKVä¸ä¸‰ä¸ªçº¿æ€§å±‚ç»„åˆ°ä¸€èµ·ï¼Œç„¶ååˆ©ç”¨forå¾ªç¯ï¼Œå°†è¾“å…¥QKVåˆ†åˆ«ä¼ åˆ°çº¿æ€§å±‚ä¸­,\n",
    "           ä½¿ç”¨view()å¯¹çº¿æ€§å˜æ¢çš„ç»“æ„è¿›è¡Œç»´åº¦é‡å¡‘ï¼Œä¸ºæ¯ä¸ªå¤´åˆ†å‰²è¾“å…¥\n",
    "               å¤šåŠ äº†ä¸€ä¸ªç»´åº¦hä»£è¡¨å¤´ï¼Œè¿™æ ·å°±æ„å‘³ç€æ¯ä¸ªå¤´å¯ä»¥è·å¾—ä¸€éƒ¨åˆ†è¯ç‰¹å¾ç»„æˆçš„å¥å­\n",
    "               å…¶ä¸­çš„-1ä»£è¡¨è‡ªé€‚åº”ç»´åº¦ï¼Œå³må¥å­é•¿åº¦ç»´åº¦ï¼Œå°†è‡ªåŠ¨è®¡ç®—è¿™é‡Œçš„å€¼\n",
    "           ç„¶åå¯¹ç¬¬äºŒç»´å’Œç¬¬ä¸‰ç»´è¿›è¡Œè½¬ç½®æ“ä½œï¼š\n",
    "               åŸå› ï¼šä¸ºäº†è®©ä»£è¡¨å¥å­é•¿åº¦ç»´åº¦å’Œè¯å‘é‡ç»´åº¦èƒ½å¤Ÿç›¸é‚»ï¼Œè¿™æ ·æ³¨æ„åŠ›æœºåˆ¶æ‰èƒ½æ‰¾åˆ°è¯ä¹‰ä¸å¥å­ä½ç½®çš„å…³ç³»ï¼Œ\n",
    "               ä»attentionå‡½æ•°ä¸­å¯ä»¥çœ‹åˆ°ï¼Œåˆ©ç”¨çš„æ˜¯åŸå§‹è¾“å…¥çš„å€’æ•°ç¬¬ä¸€å’Œç¬¬äºŒç»´ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†æ¯ä¸ªå¤´çš„è¾“å…¥\n",
    "        \"\"\"\n",
    "        query, key, value = [\n",
    "            lin(x).view(n_batches, -1, self.h, self.d_k).transpose(1, 2)  # -1 <-> self.h\n",
    "            for lin, x in zip(self.linear, (query, key, value))\n",
    "        ]\n",
    "        \"\"\"\n",
    "        2. å¾—åˆ°æ¯ä¸ªå¤´çš„è¾“å…¥åï¼Œæ¥ä¸‹æ¥å°±æ˜¯å°†ä»–ä»¬ä¼ å…¥åˆ°attentionä¸­ï¼Œ\n",
    "           è¿™é‡Œç›´æ¥è°ƒç”¨æˆ‘ä»¬ä¹‹å‰å®ç°çš„attentionå‡½æ•°ï¼ŒåŒæ—¶ä¹Ÿå°†maskå’Œdropoutä¼ å…¥å…¶ä¸­\n",
    "        \"\"\"\n",
    "        attn, self.p_attn = attention(query, key, value, mask, self.dropout)\n",
    "        \"\"\"\n",
    "        3. é€šè¿‡å¤šå¤´æ³¨æ„åŠ›è®¡ç®—åï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†æ¯ä¸ªå¤´è®¡ç®—ç»“æœç»„æˆçš„4ç»´å¼ é‡ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸ºè¾“å…¥çš„å½¢çŠ¶ä»¥æ–¹ä¾¿åç»­çš„è®¡ç®—ï¼Œ\n",
    "           å› æ­¤è¿™é‡Œå¼€å§‹è¿›è¡Œç¬¬ä¸€æ­¥å¤„ç†ç¯èŠ‚çš„é€†æ“ä½œï¼Œå…ˆå¯¹ç¬¬äºŒå’Œç¬¬ä¸‰ç»´è¿›è¡Œè½¬ç½®ï¼Œ\n",
    "           ç„¶åä½¿ç”¨contiguous(): èƒ½å¤Ÿè®©è½¬ç½®åçš„å¼ é‡åº”ç”¨view()ï¼Œå¦åˆ™å°†æ— æ³•ç›´æ¥ä½¿ç”¨.\n",
    "           ä¸‹ä¸€æ­¥å°±æ˜¯ä½¿ç”¨viewé‡å¡‘å½¢çŠ¶ï¼Œå˜æˆå’Œè¾“å…¥å½¢çŠ¶ç›¸åŒã€‚  \n",
    "           æœ€åä½¿ç”¨çº¿æ€§å±‚åˆ—è¡¨ä¸­çš„æœ€åä¸€ä¸ªçº¿æ€§å˜æ¢å¾—åˆ°æœ€ç»ˆçš„å¤šå¤´æ³¨æ„åŠ›ç»“æ„çš„è¾“å‡º\n",
    "        \"\"\"\n",
    "        concat = attn.transpose(1, 2).contiguous().view(n_batches, -1, self.h * self.d_k)\n",
    "        x = self.linear[-1](concat)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# 3.3 Position-wise Feed-Forward Networks\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: é€šè¿‡å‰é¦ˆå…¨è¿æ¥å±‚åè¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦ä¸å˜\n",
    "        :param d_ff: å†…éƒ¨ç»´åº¦ï¼šç¬¬äºŒä¸ªçº¿æ€§å±‚çš„è¾“å…¥ç»´åº¦å’Œç¬¬ä¸€ä¸ªçº¿æ€§å±‚çš„è¾“å‡º\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        é¦–å…ˆç»è¿‡ç¬¬ä¸€ä¸ªçº¿æ€§å±‚ï¼Œç„¶åä½¿ç”¨Fä¸­çš„reluå‡½æ•°è¿›è¡Œæ¿€æ´»ï¼Œ\n",
    "        ä¹‹åå†ä½¿ç”¨dropoutè¿›è¡Œéšæœºç½®0ï¼Œæœ€åé€šè¿‡ç¬¬äºŒä¸ªçº¿æ€§å±‚w2ï¼Œè¿”å›æœ€ç»ˆç»“æœ\n",
    "        \"\"\"\n",
    "        x = self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "        return x\n",
    "\n",
    "\n",
    "# 3.4 Embeddings and Softmax\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: è¿™é‡Œä»£è¡¨è¾“å…¥ç»™æ¨¡å‹çš„å•è¯æ–‡æœ¬é€šè¿‡è¯è¡¨æ˜ å°„åçš„one-hotå‘é‡\n",
    "        :return: å°†xä¼ ç»™self.lutå¹¶ä¸æ ¹å·ä¸‹self.d_modelç›¸ä¹˜ä½œä¸ºç»“æœè¿”å›\n",
    "        \"\"\"\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "# 3.5 Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        \"\"\"\n",
    "        :param d_model: è¯åµŒå…¥ç»´åº¦ è¿™é‡Œæ˜¯512ç»´\n",
    "        :param dropout: è¯åµŒå…¥ç»´åº¦\n",
    "        :param max_len: æ¯ä¸ªå¥å­çš„æœ€å¤§é•¿åº¦\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # ä½¿ç”¨ä¸åŸå…¬å¼ç­‰ä»·çš„è¡¨ç¤º\n",
    "        # ç›®çš„æ˜¯é¿å…ä¸­é—´çš„æ•°å€¼è®¡ç®—ç»“æœè¶…å‡ºfloatçš„èŒƒå›´\n",
    "        pos_embed = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # 0->4999 å†æ’å…¥ä¸€ä¸ªç»´åº¦(5000,1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )  # shape=[256]\n",
    "        # div_term å®ç°çš„æ˜¯åˆ†æ¯\n",
    "        # pe[:, 0::2] è¡¨ç¤ºç¬¬äºŒä¸ªç»´åº¦ä» 0 å¼€å§‹ä»¥é—´éš”ä¸º 2 å–å€¼ï¼Œå³å¶æ•°ã€‚\n",
    "        pos_embed[:, ::2] = torch.sin(position * div_term)  # shape=[max_len, 256]\n",
    "        pos_embed[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pos_embed = pos_embed.unsqueeze(0)  # shape=[1, 500, 512]\n",
    "        self.register_buffer('pe', pos_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# 3.6 Full Model\n",
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"\"\"\n",
    "    æ„å»ºæ¨¡å‹\n",
    "    :param src_vocab: è¾“å…¥è¯è¡¨å¤§å°\n",
    "    :param tgt_vocab: ç›®æ ‡è¯è¡¨å¤§å°\n",
    "    :param N: ç¼–ç å™¨å’Œè§£ç å™¨å †å çš„åŸºç¡€æ¨¡å—ä¸ªæ•°\n",
    "    :param d_model: è¯åµŒå…¥çš„ç»´åº¦\n",
    "    :param d_ff: é€ä½ç½®çš„å‰é¦ˆç½‘ç»œä¸­çš„å†…éƒ¨ç»´åº¦\n",
    "    :param h: æ³¨æ„åŠ›å¤´çš„ä¸ªæ•°\n",
    "    \"\"\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(n_heads=h, d_model=d_model, dropout=dropout)\n",
    "    ff = PositionwiseFeedForward(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
    "    position = PositionalEncoding(d_model=d_model, dropout=dropout)\n",
    "    # -------\n",
    "    encoderLayer = EncoderLayer(size=d_model, self_attention=c(attn), feed_forward=c(ff), dropout=dropout)\n",
    "    decoderLayer = DecoderLayer(size=d_model, self_attention=c(attn), src_attn=c(attn), feed_forward=c(ff),\n",
    "                                dropout=dropout)\n",
    "    srcEmbed = Embeddings(d_model=d_model, vocab=src_vocab)\n",
    "    tgtEmbed = Embeddings(d_model=d_model, vocab=tgt_vocab)\n",
    "    generator = Generator(d_model=d_model, vocab=tgt_vocab)\n",
    "    # -------\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        encoder=Encoder(layer=encoderLayer, N=N),\n",
    "        decoder=Decoder(layer=decoderLayer, N=N),\n",
    "        src_embed=nn.Sequential(srcEmbed, c(position)),\n",
    "        tgt_embed=nn.Sequential(tgtEmbed, c(position)),\n",
    "        generator=generator\n",
    "    )\n",
    "    # åˆå§‹åŒ–å‚æ•°: ä½¿ç”¨Glorotåˆå§‹åŒ–: 1/ğ‘“ğ‘ğ‘›_ğ‘ğ‘£ğ‘”, ğ‘“ğ‘ğ‘›_ğ‘ğ‘£ğ‘”=(ğ‘“ğ‘ğ‘›_ğ‘–ğ‘› +ğ‘“ğ‘ğ‘›_ğ‘œğ‘¢ğ‘¡)/2\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553fccd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:40.458861Z",
     "iopub.status.busy": "2022-05-18T15:04:40.458615Z",
     "iopub.status.idle": "2022-05-18T15:04:40.519247Z",
     "shell.execute_reply": "2022-05-18T15:04:40.518543Z"
    },
    "papermill": {
     "duration": 0.082664,
     "end_time": "2022-05-18T15:04:40.521494",
     "exception": false,
     "start_time": "2022-05-18T15:04:40.438830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5 Training\n",
    "# 5.1 Batched and Masking\n",
    "class Batch:\n",
    "    def __init__(self, src, tgt, pad=2):\n",
    "        \"\"\"\n",
    "        :param pad: é»˜è®¤2 è¡¨ç¤º<blank>\n",
    "        \"\"\"\n",
    "        self.src = src\n",
    "        # å°†ä¸ä»¤ç‰ŒåŒ¹é…çš„ä½ç½®è¡¨ç¤ºä¸ºFalse, å¦åˆ™ä¸ºTrue\n",
    "        # å¹¶åœ¨å€’æ•°ç¬¬äºŒä¸ªç»´åº¦åé¢æ·»åŠ ä¸€ç»´åº¦\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1]  # Decoderçš„è¾“å…¥ï¼Œå³é™¤å»æœ€åä¸€ä¸ªç»“æŸtokençš„éƒ¨åˆ†\n",
    "            self.tgt_y = tgt[:, 1:]  # Decoderçš„æœŸæœ›è¾“å…¥ï¼Œå³é™¤å»é¦–ä¸ªä¸€ä¸ªèµ·å§‹tokençš„éƒ¨åˆ†\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()  # æ‰€æœ‰Trueçš„è¯å…ƒæ•°é‡\n",
    "\n",
    "    @staticmethod\n",
    "    # staticmethod è¿”å›å‡½æ•°çš„é™æ€æ–¹æ³• å¯ä»¥ä¸å®ä¾‹åŒ–å³å¯è°ƒç”¨æ–¹æ³•\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"\"\"\n",
    "        pad å’Œ future words å‡åœ¨maskä¸­ç”¨padè¡¨ç¤º\n",
    "        \"\"\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        sequence_len = tgt.size(-1)  # æˆ–æ˜¯batchä¸­æœ€é•¿æ—¶é—´æ­¥æ•°\n",
    "        tgt_mask = tgt_mask & subsequent_mask(size=sequence_len).type_as(\n",
    "            tgt_mask.data\n",
    "            # &:è¿›è¡Œä½è¿ç®—\n",
    "            # subsequent_mask()è¿”å›ç»´åº¦ä¸º(1, size, size)\n",
    "            # type_as():å°†æ•°æ®ç±»å‹è½¬æ¢ä¸ºtgt_maskçš„æ•°æ®ç±»å‹\n",
    "        )\n",
    "        return tgt_mask\n",
    "\n",
    "\n",
    "# 5.2 Training Loop\n",
    "class TrainState:\n",
    "    \"\"\"\n",
    "    è·Ÿè¸ªå¤„ç†çš„æ­¥éª¤ã€ç¤ºä¾‹å’Œæ ‡è®°çš„æ•°é‡\n",
    "    \"\"\"\n",
    "    step: int = 0  # å½“å‰epochçš„æ­¥\n",
    "    accum_step: int = 0  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
    "    samples: int = 0  # ä½¿ç”¨çš„ç¤ºä¾‹æ€»æ•°\n",
    "    tokens: int = 0  # å¤„ç†çš„tokensæ€»æ•°\n",
    "\n",
    "\n",
    "def run_epoch(data_iter, model, loss_compute,\n",
    "              optimizer, scheduler,\n",
    "              mode=\"train\", accum_iter=1,\n",
    "              train_state=TrainState(),\n",
    "              device=None):\n",
    "    \"\"\"\n",
    "    å®Œæˆäº†ä¸€ä¸ªepochè®­ç»ƒçš„æ‰€æœ‰å·¥ä½œ\n",
    "    åŒ…æ‹¬æ•°æ®åŠ è½½ã€æ¨¡å‹æ¨ç†ã€æŸå¤±è®¡ç®—ä¸æ–¹å‘ä¼ æ’­ï¼ŒåŒæ—¶å°†è®­ç»ƒè¿‡ç¨‹ä¿¡æ¯è¿›è¡Œæ‰“å°\n",
    "    \"\"\"\n",
    "    # è®­ç»ƒå•ä¸ªepoch\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # modelæ˜¯ä¸€ä¸ªEncoderDecoderå¯¹è±¡\n",
    "        # å‰å‘ä¼ æ’­å°†src, src_maskä¼ å…¥ç¼–ç å‡½æ•°ï¼Œå¾—åˆ°ç»“æœåä¸src_mask, tgtå’Œtgt_maskä¸€åŒä¼ ç»™è§£ç å‡½æ•°\n",
    "        out = model.forward(src=batch.src, tgt=batch.tgt, src_mask=batch.src_mask, tgt_mask=batch.tgt_mask)\n",
    "        # æ¢¯åº¦ç´¯åŠ æŠ€æœ¯ loss_node = loss_node / accum_iter\n",
    "        # accum_iter:å°æ‰¹æ¬¡æ•° é»˜è®¤æ˜¯1 ä¸ä½¿ç”¨æ¢¯åº¦ç´¯åŠ æŠ€æœ¯\n",
    "        loss, loss_node = loss_compute(x=out, y=batch.tgt_y, norm=batch.ntokens)  # è®¡ç®—æŸå¤±->SimpleLossCompute\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward()  # åå‘ä¼ æ’­->ä¸è¿›è¡Œæ¢¯åº¦æ¸…é›¶, æ‰§è¡Œæ¢¯åº¦ç´¯åŠ çš„æ“ä½œ\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch.src.shape[0]\n",
    "            train_state.tokens += batch.ntokens\n",
    "            if i % accum_iter == 0:  # æ¢¯åº¦ç´¯åŠ è¾¾åˆ°å›ºå®šæ¬¡æ•°ä¹‹å\n",
    "                optimizer.step()  # æ›´æ–°å‚æ•°\n",
    "                optimizer.zero_grad(set_to_none=True)  # æ¢¯åº¦æ¸…é›¶\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]  # è·å–å­¦ä¹ ç‡\n",
    "            elapsed = time.time() - start  # è®¡ç®—40ä¸ªè¿­ä»£æ‰€éœ€æ—¶é—´\n",
    "            print((\"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \" +\n",
    "                   \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\") %\n",
    "                  (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "\n",
    "        del loss\n",
    "        del loss_node\n",
    "    return total_loss / total_tokens, train_state\n",
    "\n",
    "\n",
    "# 5.2 Optimizer\n",
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    å¯¹äº Lambda LR å‡½æ•°ï¼Œæˆ‘ä»¬å¿…é¡»å°†æ­¥éª¤é»˜è®¤ä¸º 1 é¿å…é›¶æå‡ä¸ºè´Ÿå¹‚ã€‚\n",
    "    :param step: æ—¶é—´æ­¥é•¿\n",
    "    :param model_size: æ¨¡å‹ç»´åº¦\n",
    "    :param factor: ç¤ºä¾‹ä¸­ä¸º1\n",
    "    :param warmup: é¢„çƒ­è¿­ä»£æ•°\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "            model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )\n",
    "\n",
    "\n",
    "def example_learning_schedule():\n",
    "    \"\"\"\n",
    "    å­¦ä¹ ç‡è°ƒåº¦ç¤ºä¾‹: åœ¨ opts åˆ—è¡¨ä¸­æœ‰ 3 ä¸ªç¤ºä¾‹ã€‚\n",
    "    ä¸ºæ¯ä¸ªç¤ºä¾‹è¿è¡Œ 20000 ä¸ª epoch\n",
    "    å­¦ä¹ ç‡è°ƒåº¦ä½¿ç”¨ è‡ªå®šä¹‰è°ƒæ•´å­¦ä¹ ç‡LambdaLR\n",
    "    æ•°æ®å¯è§†åŒ–å·¥å…·: Altair\n",
    "    \"\"\"\n",
    "    opts = [\n",
    "        [512, 1, 4000],  # example 1\n",
    "        [512, 1, 8000],  # example 2\n",
    "        [256, 1, 4000],  # example 3\n",
    "    ]\n",
    "    dummy_model = torch.nn.Linear(1, 1)\n",
    "    learning_rates = []\n",
    "\n",
    "    for idx, example in enumerate(opts):\n",
    "        optimizer = torch.optim.Adam(dummy_model.parameters(),\n",
    "                                     lr=1,\n",
    "                                     betas=(0.9, 0.98),\n",
    "                                     eps=1e-9)\n",
    "        lr_scheduler = LambdaLR(\n",
    "            optimizer=optimizer,\n",
    "            lr_lambda=lambda step: rate(step, *example))\n",
    "        tmp = []\n",
    "        #  é‡‡å–20000æ¬¡çš„è™šæ‹Ÿè®­ç»ƒæ­¥éª¤ï¼Œå¹¶ä¿å­˜æ¯ä¸€æ­¥çš„å­¦ä¹ ç‡\n",
    "        for step in range(20000):\n",
    "            # optimizer.param_groups[0]ï¼šé•¿åº¦ä¸º6çš„å­—å…¸ï¼Œ\n",
    "            # åŒ…æ‹¬[â€˜amsgradâ€™, â€˜paramsâ€™, â€˜lrâ€™, â€˜betasâ€™, â€˜weight_decayâ€™, â€˜epsâ€™]\n",
    "            tmp.append(optimizer.param_groups[0][\"lr\"])\n",
    "            optimizer.step()  # æ›´æ–°å‚æ•°\n",
    "            lr_scheduler.step()  # æ›´æ–°å‚æ•°\n",
    "        learning_rates.append(tmp)\n",
    "\n",
    "    learning_rates = torch.tensor(learning_rates)\n",
    "    # ----æ•°æ®å¯è§†åŒ–----\n",
    "    # ä½¿ altair èƒ½å¤Ÿå¤„ç†è¶…è¿‡ 5000 è¡Œ\n",
    "    alt.data_transformers.disable_max_rows()\n",
    "\n",
    "    opts_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Learning Rate\": learning_rates[warmup_idx, :],\n",
    "                    \"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][\n",
    "                        warmup_idx\n",
    "                    ],\n",
    "                    \"step\": range(20000),\n",
    "                }\n",
    "            )\n",
    "            for warmup_idx in [0, 1, 2]\n",
    "        ]\n",
    "    )\n",
    "    return (\n",
    "        alt.Chart(opts_data)\n",
    "            .mark_line()\n",
    "            .properties(width=600)\n",
    "            .encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\")\n",
    "            .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "# 5.3 Regularization\n",
    "\n",
    "# 5.3.2 Label Smoothing\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())\n",
    "\n",
    "\n",
    "RUN_EXAMPLES = True\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def example_label_smoothing():\n",
    "    crit = LabelSmoothing(5, 0, 0.4)\n",
    "    predict = torch.FloatTensor(\n",
    "        [\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "        ]\n",
    "    )\n",
    "    crit(x=predict.log(), target=torch.LongTensor([2, 1, 0, 3, 3]))\n",
    "    LS_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"target distribution\": crit.true_dist[x, y].flatten(),\n",
    "                    \"columns\": y,\n",
    "                    \"rows\": x,\n",
    "                }\n",
    "            )\n",
    "            for y in range(5)\n",
    "            for x in range(5)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(LS_data)\n",
    "            .mark_rect(color=\"Blue\", opacity=1)\n",
    "            .properties(height=200, width=200)\n",
    "            .encode(\n",
    "            alt.X(\"columns:O\", title=None),\n",
    "            alt.Y(\"rows:O\", title=None),\n",
    "            alt.Color(\n",
    "                \"target distribution:Q\", scale=alt.Scale(scheme=\"viridis\")\n",
    "            ),\n",
    "        )\n",
    "            .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "def loss(x, crit):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])\n",
    "    return crit(predict.log(), torch.LongTensor([1])).data\n",
    "\n",
    "\n",
    "def penalization_visualization():\n",
    "    crit = LabelSmoothing(5, 0, 0.1)\n",
    "    loss_data = pd.DataFrame({\n",
    "        \"Loss\": [loss(x, crit) for x in range(1, 100)],\n",
    "        \"Steps\": list(range(99)),\n",
    "    }).astype(\"float\")\n",
    "\n",
    "    return (alt.Chart(loss_data).mark_line().properties(width=350).encode(\n",
    "        x=\"Steps\",\n",
    "        y=\"Loss\",\n",
    "    ).interactive())\n",
    "\n",
    "\n",
    "# 6 A First Example\n",
    "\n",
    "# 6.1 Synthetic Data\n",
    "def data_gen(V, n_batches, batch_size, s_len=10, device=None):\n",
    "    \"\"\"\n",
    "    <ç¼–ç å™¨-è§£ç å™¨æ•°æ®å¤åˆ¶ä»»åŠ¡> éšæœºæ•°æ®ç”Ÿæˆå™¨\n",
    "    :param device: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ\n",
    "    :param V: è¯å…¸æ•°é‡ï¼Œå–å€¼èŒƒå›´[0, V-1]ï¼Œçº¦å®š0ä½œä¸ºç‰¹æ®Šç¬¦å·ä½¿ç”¨ä»£è¡¨padding\n",
    "    :param batch_size: æ‰¹æ¬¡å¤§å°\n",
    "    :param n_batches: éœ€è¦ç”Ÿæˆçš„æ‰¹æ¬¡æ•°é‡\n",
    "    :param s_len: ç”Ÿæˆçš„åºåˆ—æ•°æ®çš„é•¿åº¦\n",
    "    \"\"\"\n",
    "    for i in range(n_batches):\n",
    "        src_data = torch.randint(2, V, size=(batch_size, s_len))\n",
    "        # çº¦å®šè¾“å‡ºä¸ºè¾“å…¥é™¤å»åºåˆ—ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œå³å‘åå¹³ç§»ä¸€ä½è¿›è¡Œè¾“å‡ºï¼ŒåŒæ—¶è¾“å‡ºæ•°æ®è¦åœ¨ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥æ·»åŠ ä¸€ä¸ªèµ·å§‹ç¬¦\n",
    "        tgt_data = src_data.clone()\n",
    "        tgt_data[:, 0] = 1  # å°†åºåˆ—çš„ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ç½®ä¸º1(å³çº¦å®šçš„èµ·å§‹ç¬¦)\n",
    "        # .batch()\n",
    "        # è¿”å›ä¸€ä¸ªæ–°çš„tensorï¼Œä»å½“å‰è®¡ç®—å›¾ä¸­åˆ†ç¦»ä¸‹æ¥çš„ï¼Œä½†æ˜¯ä»æŒ‡å‘åŸå˜é‡çš„å­˜æ”¾ä½ç½®\n",
    "        # ä¸åŒä¹‹å¤„åªæ˜¯requires_gradä¸ºfalseï¼Œå¾—åˆ°çš„è¿™ä¸ªtensoræ°¸è¿œä¸éœ€è¦è®¡ç®—å…¶æ¢¯åº¦ï¼Œä¸å…·æœ‰gradã€‚\n",
    "        # requires_grad é»˜è®¤ä¸ºFalse\n",
    "        src = src_data.requires_grad_(False).clone().detach()\n",
    "        tgt = tgt_data.requires_grad_(False).clone().detach()\n",
    "        if device == \"cuda\":\n",
    "            src = src.cuda()\n",
    "            tgt = tgt.cuda()\n",
    "        yield Batch(src=src, tgt=tgt, pad=0)\n",
    "\n",
    "\n",
    "# 6.2 Loss Computation\n",
    "class SimpleLossCompute:\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion  # ä½¿ç”¨æ ‡ç­¾å¹³æ»‘\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        \"\"\"\n",
    "        :param x: decoderè¾“å‡ºçš„ç»“æœ\n",
    "        :param y: æ ‡ç­¾æ•°æ®\n",
    "        :param norm: lossçš„å½’ä¸€åŒ–ç³»æ•°ï¼Œç”¨batchä¸­æ‰€æœ‰æœ‰æ•ˆtokenæ•°å³å¯\n",
    "        \"\"\"\n",
    "        x = self.generator(x)\n",
    "        # contiguous():\n",
    "        # 1. ç”±äºtorch.viewç­‰æ–¹æ³•æ“ä½œéœ€è¦è¿ç»­çš„Tensor\n",
    "        # 2. å‡ºäºæ€§èƒ½è€ƒè™‘ ä½¿ç”¨è¯¥æ–¹æ³•åä¼šé‡æ–°æ®å¼€è¾Ÿä¸€å—å†…å­˜ç©ºé—´ä¿è¯æ•°æ˜¯åœ¨é€»è¾‘é¡ºåºå’Œå†…å­˜ä¸­æ˜¯ä¸€è‡´çš„\n",
    "        x_ = x.contiguous().view(-1, x.size(-1))\n",
    "        y_ = y.contiguous().view(-1)\n",
    "        loss = self.criterion(x_, y_)\n",
    "        sloss = (loss / norm)\n",
    "\n",
    "        return sloss.data * norm, loss\n",
    "\n",
    "\n",
    "# 6.3 Greedy Decoding\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    # encoder()ç¼–ç å‡½æ•°: ä½¿ç”¨src_embedå¯¹srcåšå¤„ç†ï¼Œç„¶åå’Œsrc_maskä¸€èµ·ä¼ ç»™self.encoder\n",
    "    memory = model.encode(src=src, src_mask=src_mask)\n",
    "    # ysä»£è¡¨ç›®å‰å·²ç”Ÿæˆçš„åºåˆ—ï¼Œæœ€åˆä¸ºä»…åŒ…å«ä¸€ä¸ªèµ·å§‹ç¬¦çš„åºåˆ—ï¼Œä¸æ–­å°†é¢„æµ‹ç»“æœè¿½åŠ åˆ°åºåˆ—æœ€å\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len - 1):\n",
    "        # decoder()è§£ç å‡½æ•°: ä½¿ç”¨tgt_embedå¯¹tgtåšå¤„ç†ï¼Œç„¶åå’Œsrc_mask, tgt_mask, memoryä¸€èµ·ä¼ ç»™self.decoder\n",
    "        out = model.decode(memory=memory,\n",
    "                           src_mask=src_mask,\n",
    "                           tgt=ys,\n",
    "                           tgt_mask=subsequent_mask(size=ys.size(1)).type_as(src.data))\n",
    "        # generator: ç±»åˆ«ç”Ÿæˆå™¨å¯¹è±¡ -> linear+softmax\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        # cat(): å®ç°æ‹¼æ¥æ“ä½œ\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)],\n",
    "            dim=1)\n",
    "    return ys\n",
    "\n",
    "\n",
    "# 6.4 Training Example\n",
    "# def execute_example(fn, args=[]):\n",
    "#     if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "#         fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "\n",
    "def example_simple_model(device=None):\n",
    "    V = 11  # å­—å…¸çš„å¤§å°\n",
    "    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "    model = make_model(src_vocab=V, tgt_vocab=V, N=2)\n",
    "    if device == \"cuda\":\n",
    "        model.cuda()\n",
    "    model_size = model.src_embed[0].d_model  # 512\n",
    "\n",
    "    n_epochs = 40\n",
    "    n_batch_train_epoch = 30  # è®­ç»ƒæ—¶æ¯ä¸ªepochæ‰€éœ€æ‰¹æ¬¡å¤§å°\n",
    "    n_batch_val_epoch = 10  # éªŒè¯æ—¶æ¯ä¸ªepochæ‰€éœ€æ‰¹æ¬¡å¤§å°\n",
    "    batch_size = 40\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=0.5,\n",
    "                                 betas=(0.9, 0.98),\n",
    "                                 eps=1e-9)\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(step=step, model_size=model_size, factor=0.1, warmup=400)\n",
    "    )\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_compute = SimpleLossCompute(generator=model.generator,\n",
    "                                         criterion=criterion)\n",
    "\n",
    "        print(f\"\\n|   æ‰¹æ¬¡: {epoch}   |\")\n",
    "        print(\"*\" * 5 + \"è®­ç»ƒ\" + \"*\" * 5)\n",
    "        model.train()  # self.training=True\n",
    "\n",
    "        train_data_iter = data_gen(V=V, n_batches=n_batch_train_epoch,\n",
    "                                   batch_size=batch_size, device=device)\n",
    "        run_epoch(data_iter=train_data_iter,\n",
    "                  model=model,\n",
    "                  loss_compute=loss_compute,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=lr_scheduler,\n",
    "                  mode=\"train\",)\n",
    "\n",
    "        # -----------\n",
    "        print(\"*\" * 5 + \"éªŒè¯\" + \"*\" * 5)\n",
    "        model.eval()  # self.training=False\n",
    "\n",
    "        val_data_iter = data_gen(V=V, n_batches=n_batch_val_epoch,\n",
    "                                 batch_size=batch_size, device=device)\n",
    "        valid_mean_loss = run_epoch(data_iter=val_data_iter,\n",
    "                                    model=model,\n",
    "                                    loss_compute=loss_compute,\n",
    "                                    optimizer=DummyOptimizer(),  # None\n",
    "                                    scheduler=DummyScheduler(),  # None\n",
    "                                    mode=\"eval\",)[0]  # è¿”å›: total_loss / total_tokens\n",
    "        print(f\"|éªŒè¯æŸå¤±: {valid_mean_loss} |\")\n",
    "\n",
    "    model.eval()\n",
    "    torch.save(model, './example_1_copy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea675ff3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:40.560739Z",
     "iopub.status.busy": "2022-05-18T15:04:40.560202Z",
     "iopub.status.idle": "2022-05-18T15:04:40.563843Z",
     "shell.execute_reply": "2022-05-18T15:04:40.563069Z"
    },
    "papermill": {
     "duration": 0.025708,
     "end_time": "2022-05-18T15:04:40.565887",
     "exception": false,
     "start_time": "2022-05-18T15:04:40.540179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|   æ‰¹æ¬¡: 0   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   3.17 | Tokens / Sec:  2327.3 | Learning Rate: 5.5e-07\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 2.394113779067993 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 1   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   2.40 | Tokens / Sec:  9412.5 | Learning Rate: 8.8e-06\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 1.9547275304794312 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 2   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   2.21 | Tokens / Sec:  9569.1 | Learning Rate: 1.7e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 1.7825837135314941 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 3   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.97 | Tokens / Sec:  9915.8 | Learning Rate: 2.5e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 1.6604243516921997 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 4   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.90 | Tokens / Sec:  9883.9 | Learning Rate: 3.4e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 1.5723007917404175 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 5   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.72 | Tokens / Sec:  9371.5 | Learning Rate: 4.2e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 1.4360578060150146 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 6   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.64 | Tokens / Sec:  9481.5 | Learning Rate: 5.0e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 1.2682843208312988 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 7   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.47 | Tokens / Sec:  9443.6 | Learning Rate: 5.9e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.9558234214782715 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 8   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.29 | Tokens / Sec:  9301.9 | Learning Rate: 6.7e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.7557151913642883 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 9   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.20 | Tokens / Sec:  9342.2 | Learning Rate: 7.5e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.5053467154502869 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 10   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.93 | Tokens / Sec:  9864.1 | Learning Rate: 8.3e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.2739480435848236 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 11   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.64 | Tokens / Sec:  9942.4 | Learning Rate: 9.2e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.1763959378004074 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 12   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.50 | Tokens / Sec:  9836.1 | Learning Rate: 1.0e-04\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.11010845005512238 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 13   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.38 | Tokens / Sec:  9889.8 | Learning Rate: 1.1e-04\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.04243813827633858 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 14   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.26 | Tokens / Sec:  9201.6 | Learning Rate: 1.1e-04\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.06136145442724228 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 15   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.22 | Tokens / Sec:  9260.4 | Learning Rate: 1.0e-04\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.04074167460203171 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 16   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.15 | Tokens / Sec:  9413.2 | Learning Rate: 1.0e-04\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.017386775463819504 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 17   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.15 | Tokens / Sec:  9407.1 | Learning Rate: 9.8e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.023547537624835968 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 18   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.12 | Tokens / Sec:  9487.5 | Learning Rate: 9.5e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.014222418889403343 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 19   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.13 | Tokens / Sec:  9414.4 | Learning Rate: 9.2e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.008601798675954342 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 20   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.11 | Tokens / Sec:  9354.6 | Learning Rate: 9.0e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.00962837878614664 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 21   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.07 | Tokens / Sec:  9297.8 | Learning Rate: 8.8e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.0032831078860908747 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 22   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.07 | Tokens / Sec:  9115.8 | Learning Rate: 8.6e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.006906671449542046 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 23   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.09 | Tokens / Sec:  9201.5 | Learning Rate: 8.4e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.009479006752371788 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 24   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.03 | Tokens / Sec:  9775.9 | Learning Rate: 8.2e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.006316591054201126 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 25   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.10 | Tokens / Sec:  9843.4 | Learning Rate: 8.1e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.0020393519662320614 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 26   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.06 | Tokens / Sec:  9456.5 | Learning Rate: 7.9e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.00981949083507061 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 27   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.06 | Tokens / Sec:  9164.5 | Learning Rate: 7.8e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.004193373955786228 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 28   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.04 | Tokens / Sec:  9386.5 | Learning Rate: 7.6e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.006194210145622492 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 29   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.05 | Tokens / Sec:  9317.7 | Learning Rate: 7.5e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.0052854204550385475 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 30   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.09 | Tokens / Sec:  9246.6 | Learning Rate: 7.4e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.009405386634171009 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 31   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.08 | Tokens / Sec:  9269.6 | Learning Rate: 7.2e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.0021834897343069315 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 32   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.08 | Tokens / Sec:  9661.4 | Learning Rate: 7.1e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.0030930545181035995 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 33   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.07 | Tokens / Sec:  9883.6 | Learning Rate: 7.0e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.0010114101460203528 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 34   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.07 | Tokens / Sec:  9407.5 | Learning Rate: 6.9e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.0016180539969354868 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 35   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.05 | Tokens / Sec:  9505.2 | Learning Rate: 6.8e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.0005277844611555338 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 36   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.05 | Tokens / Sec:  9301.0 | Learning Rate: 6.7e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.0017933434573933482 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 37   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.02 | Tokens / Sec:  9893.1 | Learning Rate: 6.6e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.0012114470591768622 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 38   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9725.4 | Learning Rate: 6.5e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.0011449726298451424 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 39   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.08 | Tokens / Sec:  9365.2 | Learning Rate: 6.5e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.0007832083501853049 |\n"
     ]
    }
   ],
   "source": [
    "example_simple_model(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d7b9bb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:40.604458Z",
     "iopub.status.busy": "2022-05-18T15:04:40.604271Z",
     "iopub.status.idle": "2022-05-18T15:04:40.607289Z",
     "shell.execute_reply": "2022-05-18T15:04:40.606613Z"
    },
    "papermill": {
     "duration": 0.02323,
     "end_time": "2022-05-18T15:04:40.608947",
     "exception": false,
     "start_time": "2022-05-18T15:04:40.585717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  6,  3,  3,  5,  6,  5,  8,  8, 10]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('./example_1_copy.pth')\n",
    "src = torch.LongTensor([[1, 6, 3, 3, 5, 6, 5, 8, 8, 10]]).cuda()\n",
    "max_len = src.shape[1]\n",
    "src_mask = torch.ones(1, 1, max_len).cuda()\n",
    "print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eafd757",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:40.645433Z",
     "iopub.status.busy": "2022-05-18T15:04:40.645253Z",
     "iopub.status.idle": "2022-05-18T15:04:40.648569Z",
     "shell.execute_reply": "2022-05-18T15:04:40.647901Z"
    },
    "papermill": {
     "duration": 0.023354,
     "end_time": "2022-05-18T15:04:40.650128",
     "exception": false,
     "start_time": "2022-05-18T15:04:40.626774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4606a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:40.686998Z",
     "iopub.status.busy": "2022-05-18T15:04:40.686799Z",
     "iopub.status.idle": "2022-05-18T15:04:40.715107Z",
     "shell.execute_reply": "2022-05-18T15:04:40.714509Z"
    },
    "papermill": {
     "duration": 0.048612,
     "end_time": "2022-05-18T15:04:40.716688",
     "exception": false,
     "start_time": "2022-05-18T15:04:40.668076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. éšæœºç”Ÿæˆæ—¥æœŸ, å¹¶ä»¥è¾“å…¥æ ¼å¼å’Œç›®æ ‡æ ¼å¼æ˜¾ç¤º\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "\n",
    "def random_dates(n_dates):\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "\n",
    "    ordinals = np.random.randint(low=min_date, high=max_date + 1, size=n_dates)\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "\n",
    "    X = [MONTHS[date.month - 1] + \" \" + date.strftime(\"%d, %Y\") for date in dates]\n",
    "    y = [date.isoformat() for date in dates]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# 2. ç¡®å®šè¾“å…¥,ç›®æ ‡çš„è¯æ±‡(å­—ç¬¦)è¡¨\n",
    "input_chars = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
    "output_chars = \"0123456789-\"\n",
    "src_vocab = len(input_chars)\n",
    "tgt_vocab = len(output_chars)\n",
    "\n",
    "\n",
    "# 3. ç¼–å†™å‡½æ•°å°†å­—ç¬¦ä¸²è½¬åŒ–ä¸ºIDså½¢å¼\n",
    "def date_str_to_ids(date_str, chars_list):\n",
    "    return [chars_list.index(c) + 1 for c in date_str]\n",
    "\n",
    "\n",
    "# 4.å¤„ç†å¯å˜é•¿åº¦çš„åºåˆ—\n",
    "def prepare_date_strs(date_strs, chars=input_chars):\n",
    "    X_ids = [torch.tensor(date_str_to_ids(date, chars)).cuda() for date in date_strs]\n",
    "    X = pad_sequence(X_ids, batch_first=True, padding_value=0)\n",
    "    return X\n",
    "# def prepare_date_strs(date_strs, chars=input_chars):\n",
    "#     X_ids = [torch.tensor(date_str_to_ids(date, chars)) for date in date_strs]\n",
    "#     X = pad_sequence(X_ids, batch_first=True, padding_value=0)\n",
    "#     return X\n",
    "\n",
    "\n",
    "# 5. æ‰¹é‡å’Œæ©ç  Batched and Masking\n",
    "class Batch:\n",
    "    def __init__(self, src, tgt, pad=0):\n",
    "        \"\"\"\n",
    "        :param pad: é»˜è®¤0 è¡¨ç¤º<blank>\n",
    "        \"\"\"\n",
    "        self.src = src\n",
    "        # å°†ä¸ä»¤ç‰ŒåŒ¹é…çš„ä½ç½®è¡¨ç¤ºä¸ºFalse, å¦åˆ™ä¸ºTrue\n",
    "        # å¹¶åœ¨å€’æ•°ç¬¬äºŒä¸ªç»´åº¦åé¢æ·»åŠ ä¸€ç»´åº¦\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1]  # Decoderçš„è¾“å…¥ï¼Œå³é™¤å»æœ€åä¸€ä¸ªç»“æŸtokençš„éƒ¨åˆ†\n",
    "            self.tgt_y = tgt[:, 1:]  # Decoderçš„æœŸæœ›è¾“å…¥ï¼Œå³é™¤å»é¦–ä¸ªä¸€ä¸ªèµ·å§‹tokençš„éƒ¨åˆ†\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()  # æ‰€æœ‰Trueçš„è¯å…ƒæ•°é‡\n",
    "\n",
    "    @staticmethod\n",
    "    # staticmethod è¿”å›å‡½æ•°çš„é™æ€æ–¹æ³• å¯ä»¥ä¸å®ä¾‹åŒ–å³å¯è°ƒç”¨æ–¹æ³•\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"\"\"\n",
    "        pad å’Œ future words å‡åœ¨maskä¸­ç”¨padè¡¨ç¤º\n",
    "        \"\"\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        sequence_len = tgt.size(-1)  # æˆ–æ˜¯batchä¸­æœ€é•¿æ—¶é—´æ­¥æ•°\n",
    "        tgt_mask = tgt_mask & subsequent_mask(size=sequence_len).type_as(\n",
    "            tgt_mask.data\n",
    "            # &:è¿›è¡Œä½è¿ç®—\n",
    "            # subsequent_mask()è¿”å›ç»´åº¦ä¸º(1, size, size)\n",
    "            # type_as():å°†æ•°æ®ç±»å‹è½¬æ¢ä¸ºtgt_maskçš„æ•°æ®ç±»å‹\n",
    "        )\n",
    "        return tgt_mask\n",
    "\n",
    "\n",
    "# 6. æ„å»ºæ•°æ®é›†\n",
    "\n",
    "sos_id = tgt_vocab + 1  # 11+1=12\n",
    "\n",
    "\n",
    "# def shift_output_sequences(y, device=None):\n",
    "#     if device == \"cuda\":\n",
    "#         sos_token = torch.Tensor(len(y), 1).fill_(sos_id).int().cuda()\n",
    "#         decoder = torch.cat((sos_token, y[:, :-1]), axis=1).cuda()\n",
    "#     else:\n",
    "#         sos_token = torch.Tensor(len(y), 1).fill_(sos_id).int()\n",
    "#         decoder = torch.cat((sos_token, y[:, :-1]), axis=1)\n",
    "#     return decoder\n",
    "def shift_output_sequences(y, device=None):\n",
    "    if device == \"cuda\":\n",
    "        sos_token = torch.Tensor(len(y), 1).fill_(sos_id).int().cuda()\n",
    "        decoder = torch.cat((sos_token, y), axis=1).cuda()\n",
    "    else:\n",
    "        sos_token = torch.Tensor(len(y), 1).fill_(sos_id).int()\n",
    "        decoder = torch.cat((sos_token, y), axis=1)\n",
    "    return decoder\n",
    "\n",
    "\n",
    "def create_dataset(n_dates, device=None):\n",
    "    X, y = random_dates(n_dates)\n",
    "    X_pre = prepare_date_strs(X, input_chars)\n",
    "    y_pre = prepare_date_strs(y, output_chars)\n",
    "    y_pre_shift = shift_output_sequences(y_pre, device=device)\n",
    "\n",
    "    #         X_pre[:, 0] = 1  # å°†åºåˆ—çš„ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ç½®ä¸º1(å³çº¦å®šçš„èµ·å§‹ç¬¦)\n",
    "    #     y_pre[:, 0] = 1\n",
    "    return X_pre, y_pre_shift\n",
    "\n",
    "\n",
    "def data_gen(n_batches, batch_size, device=None):\n",
    "    \"\"\"\n",
    "    <ç¼–ç å™¨-è§£ç å™¨æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä»»åŠ¡> éšæœºæ•°æ®ç”Ÿæˆå™¨\n",
    "    :param batch_size: æ‰¹æ¬¡å¤§å°\n",
    "    :param n_batches: éœ€è¦ç”Ÿæˆçš„æ‰¹æ¬¡æ•°é‡\n",
    "    \"\"\"\n",
    "    for i in range(n_batches):\n",
    "        X_pre, y_pre = create_dataset(batch_size,device=device)\n",
    "        # data = torch.randint(2, V, size=(batch_size, s_len))\n",
    "        # .batch()\n",
    "        # è¿”å›ä¸€ä¸ªæ–°çš„tensorï¼Œä»å½“å‰è®¡ç®—å›¾ä¸­åˆ†ç¦»ä¸‹æ¥çš„ï¼Œä½†æ˜¯ä»æŒ‡å‘åŸå˜é‡çš„å­˜æ”¾ä½ç½®\n",
    "        # ä¸åŒä¹‹å¤„åªæ˜¯requires_gradä¸ºfalseï¼Œå¾—åˆ°çš„è¿™ä¸ªtensoræ°¸è¿œä¸éœ€è¦è®¡ç®—å…¶æ¢¯åº¦ï¼Œä¸å…·æœ‰gradã€‚\n",
    "        # requires_grad é»˜è®¤ä¸ºFalse\n",
    "        src = X_pre.requires_grad_(False).clone().detach()\n",
    "        tgt = y_pre.requires_grad_(False).clone().detach()\n",
    "        if device == \"cuda\":\n",
    "            src = src.cuda()\n",
    "            tgt = tgt.cuda()\n",
    "        yield Batch(src=src, tgt=tgt, pad=0)\n",
    "\n",
    "\n",
    "# 7. è®­ç»ƒè¯„ä¼°æ¨¡å‹\n",
    "def example_simple_model(device=None):\n",
    "    # V = 11  # å­—å…¸çš„å¤§å°\n",
    "    criterion = LabelSmoothing(size=tgt_vocab + 2, padding_idx=0, smoothing=0.0)\n",
    "    model = make_model(src_vocab=src_vocab + 1, tgt_vocab=tgt_vocab + 2, N=2)\n",
    "    if device == \"cuda\":\n",
    "        model.cuda()\n",
    "    model_size = model.src_embed[0].d_model  # 512\n",
    "\n",
    "    n_epochs = 20\n",
    "    n_batch_train_epoch = 200  # è®­ç»ƒæ—¶æ¯ä¸ªepochæ‰€éœ€æ‰¹æ¬¡å¤§å°\n",
    "    n_batch_val_epoch = 50  # éªŒè¯æ—¶æ¯ä¸ªepochæ‰€éœ€æ‰¹æ¬¡å¤§å°\n",
    "    batch_size = 100\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=0.5,\n",
    "                                 betas=(0.9, 0.98),\n",
    "                                 eps=1e-9)\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step=step, model_size=model_size, factor=0.1, warmup=600))\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        loss_compute = SimpleLossCompute(generator=model.generator,\n",
    "                                         criterion=criterion)\n",
    "\n",
    "        print(f\"\\n|   æ‰¹æ¬¡: {epoch}   |\")\n",
    "        print(\"*\" * 5 + \"è®­ç»ƒ\" + \"*\" * 5)\n",
    "        model.train()  # self.training=True\n",
    "\n",
    "        train_data_iter = data_gen(n_batches=n_batch_train_epoch,\n",
    "                                   batch_size=batch_size,\n",
    "                                   device=device)\n",
    "        run_epoch(data_iter=train_data_iter,\n",
    "                  model=model,\n",
    "                  loss_compute=loss_compute,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=lr_scheduler,\n",
    "                  mode=\"train\")\n",
    "\n",
    "        # -----------\n",
    "        print(\"*\" * 5 + \"éªŒè¯\" + \"*\" * 5)\n",
    "        model.eval()  # self.training=False\n",
    "\n",
    "        val_data_iter = data_gen(n_batches=n_batch_val_epoch,\n",
    "                                 batch_size=batch_size,\n",
    "                                 device=device)\n",
    "        valid_mean_loss = run_epoch(\n",
    "            data_iter=val_data_iter,\n",
    "            model=model,\n",
    "            loss_compute=loss_compute,\n",
    "            optimizer=DummyOptimizer(),  # None\n",
    "            scheduler=DummyScheduler(),  # None\n",
    "            mode=\"eval\")[0]  # è¿”å›: total_loss / total_tokens\n",
    "        print(f\"|éªŒè¯æŸå¤±: {valid_mean_loss} |\")\n",
    "\n",
    "    model.eval()\n",
    "    torch.save(model, './example_2_date.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d1d53f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:40.753557Z",
     "iopub.status.busy": "2022-05-18T15:04:40.753373Z",
     "iopub.status.idle": "2022-05-18T15:08:09.901642Z",
     "shell.execute_reply": "2022-05-18T15:08:09.900533Z"
    },
    "papermill": {
     "duration": 209.169764,
     "end_time": "2022-05-18T15:08:09.904190",
     "exception": false,
     "start_time": "2022-05-18T15:04:40.734426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|   æ‰¹æ¬¡: 0   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   3.15 | Tokens / Sec:  9014.5 | Learning Rate: 3.0e-07\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   2.39 | Tokens / Sec:  7208.7 | Learning Rate: 6.3e-06\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   2.02 | Tokens / Sec:  7394.0 | Learning Rate: 1.2e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   1.67 | Tokens / Sec:  7564.0 | Learning Rate: 1.8e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   1.36 | Tokens / Sec:  7263.0 | Learning Rate: 2.4e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.9065974950790405 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 1   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.14 | Tokens / Sec:  9374.3 | Learning Rate: 3.0e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.98 | Tokens / Sec:  7221.5 | Learning Rate: 3.6e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.83 | Tokens / Sec:  7419.8 | Learning Rate: 4.2e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.73 | Tokens / Sec:  7439.3 | Learning Rate: 4.8e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.62 | Tokens / Sec:  7390.4 | Learning Rate: 5.4e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.3712591826915741 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 2   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.53 | Tokens / Sec:  9824.6 | Learning Rate: 6.0e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.48 | Tokens / Sec:  7475.0 | Learning Rate: 6.6e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.37 | Tokens / Sec:  7220.8 | Learning Rate: 7.2e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.24 | Tokens / Sec:  7223.8 | Learning Rate: 7.8e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.22 | Tokens / Sec:  7183.2 | Learning Rate: 8.4e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.04182703047990799 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 3   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.15 | Tokens / Sec:  9221.8 | Learning Rate: 9.0e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.14 | Tokens / Sec:  7261.3 | Learning Rate: 8.7e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.10 | Tokens / Sec:  7263.5 | Learning Rate: 8.5e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.08 | Tokens / Sec:  7375.3 | Learning Rate: 8.2e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.07 | Tokens / Sec:  7217.4 | Learning Rate: 8.0e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.011830809526145458 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 4   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.06 | Tokens / Sec:  9243.4 | Learning Rate: 7.8e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.07 | Tokens / Sec:  7136.7 | Learning Rate: 7.6e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.05 | Tokens / Sec:  7152.0 | Learning Rate: 7.4e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.04 | Tokens / Sec:  7256.3 | Learning Rate: 7.3e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.06 | Tokens / Sec:  7155.7 | Learning Rate: 7.1e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.0012394256191328168 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 5   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.03 | Tokens / Sec:  9338.9 | Learning Rate: 7.0e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.03 | Tokens / Sec:  7177.2 | Learning Rate: 6.8e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.03 | Tokens / Sec:  7291.8 | Learning Rate: 6.7e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.04 | Tokens / Sec:  7250.2 | Learning Rate: 6.6e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.03 | Tokens / Sec:  7578.7 | Learning Rate: 6.5e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.00025529967388138175 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 6   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.02 | Tokens / Sec:  9882.7 | Learning Rate: 6.4e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.03 | Tokens / Sec:  7276.8 | Learning Rate: 6.3e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.03 | Tokens / Sec:  7243.4 | Learning Rate: 6.2e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.02 | Tokens / Sec:  7443.2 | Learning Rate: 6.1e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7587.3 | Learning Rate: 6.0e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 7.481365173589438e-05 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 7   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9157.2 | Learning Rate: 5.9e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.01 | Tokens / Sec:  7572.5 | Learning Rate: 5.8e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.01 | Tokens / Sec:  7453.4 | Learning Rate: 5.7e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.02 | Tokens / Sec:  7173.7 | Learning Rate: 5.7e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7244.1 | Learning Rate: 5.6e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.0001459354389226064 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 8   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9207.2 | Learning Rate: 5.5e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.01 | Tokens / Sec:  7154.1 | Learning Rate: 5.5e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.02 | Tokens / Sec:  7140.3 | Learning Rate: 5.4e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.01 | Tokens / Sec:  7378.0 | Learning Rate: 5.3e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7244.5 | Learning Rate: 5.3e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 7.281082798726857e-05 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 9   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9937.2 | Learning Rate: 5.2e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.02 | Tokens / Sec:  7198.4 | Learning Rate: 5.1e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.01 | Tokens / Sec:  7159.6 | Learning Rate: 5.1e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.01 | Tokens / Sec:  7167.4 | Learning Rate: 5.0e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7193.7 | Learning Rate: 5.0e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 0.0012970819370821118 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 10   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.03 | Tokens / Sec:  9247.0 | Learning Rate: 4.9e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.01 | Tokens / Sec:  7182.7 | Learning Rate: 4.9e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.00 | Tokens / Sec:  7471.1 | Learning Rate: 4.8e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7474.7 | Learning Rate: 4.8e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7181.5 | Learning Rate: 4.8e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 3.976562584284693e-05 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 11   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9311.4 | Learning Rate: 4.7e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.00 | Tokens / Sec:  7224.6 | Learning Rate: 4.7e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.02 | Tokens / Sec:  7388.1 | Learning Rate: 4.6e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.01 | Tokens / Sec:  7563.5 | Learning Rate: 4.6e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.00 | Tokens / Sec:  7252.2 | Learning Rate: 4.5e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 5.729972690460272e-05 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 12   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9314.0 | Learning Rate: 4.5e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.01 | Tokens / Sec:  7194.8 | Learning Rate: 4.5e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.01 | Tokens / Sec:  7207.4 | Learning Rate: 4.4e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.01 | Tokens / Sec:  7191.3 | Learning Rate: 4.4e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.00 | Tokens / Sec:  7182.0 | Learning Rate: 4.4e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 2.7599184249993414e-05 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 13   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9299.7 | Learning Rate: 4.3e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.00 | Tokens / Sec:  7150.6 | Learning Rate: 4.3e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.00 | Tokens / Sec:  7148.6 | Learning Rate: 4.3e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7350.1 | Learning Rate: 4.2e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7151.9 | Learning Rate: 4.2e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 3.36340963258408e-05 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 14   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9927.8 | Learning Rate: 4.2e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.00 | Tokens / Sec:  7275.8 | Learning Rate: 4.1e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.00 | Tokens / Sec:  7266.8 | Learning Rate: 4.1e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7150.6 | Learning Rate: 4.1e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7349.0 | Learning Rate: 4.1e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 2.267956551804673e-05 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 15   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.00 | Tokens / Sec:  9243.4 | Learning Rate: 4.0e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.00 | Tokens / Sec:  7252.6 | Learning Rate: 4.0e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.00 | Tokens / Sec:  7176.7 | Learning Rate: 4.0e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7351.7 | Learning Rate: 4.0e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.00 | Tokens / Sec:  7167.0 | Learning Rate: 3.9e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 1.996382707147859e-05 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 16   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.00 | Tokens / Sec:  9903.0 | Learning Rate: 3.9e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.00 | Tokens / Sec:  7357.6 | Learning Rate: 3.9e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.00 | Tokens / Sec:  7284.3 | Learning Rate: 3.9e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7276.9 | Learning Rate: 3.8e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.00 | Tokens / Sec:  7690.6 | Learning Rate: 3.8e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 1.9218403394916095e-05 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 17   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9349.5 | Learning Rate: 3.8e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.00 | Tokens / Sec:  7402.5 | Learning Rate: 3.8e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.00 | Tokens / Sec:  7244.3 | Learning Rate: 3.7e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7171.0 | Learning Rate: 3.7e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7263.0 | Learning Rate: 3.7e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 1.3489638149621896e-05 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 18   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9276.8 | Learning Rate: 3.7e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.00 | Tokens / Sec:  7164.0 | Learning Rate: 3.7e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.01 | Tokens / Sec:  7169.3 | Learning Rate: 3.6e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7144.2 | Learning Rate: 3.6e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.00 | Tokens / Sec:  7438.6 | Learning Rate: 3.6e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 1.4737389392394107e-05 |\n",
      "\n",
      "|   æ‰¹æ¬¡: 19   |\n",
      "*****è®­ç»ƒ*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.00 | Tokens / Sec:  9186.4 | Learning Rate: 3.6e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.01 | Tokens / Sec:  7138.3 | Learning Rate: 3.6e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.01 | Tokens / Sec:  7139.2 | Learning Rate: 3.5e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7349.9 | Learning Rate: 3.5e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.00 | Tokens / Sec:  7331.7 | Learning Rate: 3.5e-05\n",
      "*****éªŒè¯*****\n",
      "|éªŒè¯æŸå¤±: 1.458849055779865e-05 |\n"
     ]
    }
   ],
   "source": [
    "example_simple_model(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0132d27b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:08:10.025999Z",
     "iopub.status.busy": "2022-05-18T15:08:10.025448Z",
     "iopub.status.idle": "2022-05-18T15:08:10.033155Z",
     "shell.execute_reply": "2022-05-18T15:08:10.032337Z"
    },
    "papermill": {
     "duration": 0.067273,
     "end_time": "2022-05-18T15:08:10.035208",
     "exception": false,
     "start_time": "2022-05-18T15:08:09.967935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    # encoder()ç¼–ç å‡½æ•°: ä½¿ç”¨src_embedå¯¹srcåšå¤„ç†ï¼Œç„¶åå’Œsrc_maskä¸€èµ·ä¼ ç»™self.encoder\n",
    "    memory = model.encode(src=src, src_mask=src_mask)\n",
    "    # ysä»£è¡¨ç›®å‰å·²ç”Ÿæˆçš„åºåˆ—ï¼Œæœ€åˆä¸ºä»…åŒ…å«ä¸€ä¸ªèµ·å§‹ç¬¦çš„åºåˆ—ï¼Œä¸æ–­å°†é¢„æµ‹ç»“æœè¿½åŠ åˆ°åºåˆ—æœ€å\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data).cuda()\n",
    "    for i in range(max_len-1):\n",
    "        # decoder()è§£ç å‡½æ•°: ä½¿ç”¨tgt_embedå¯¹tgtåšå¤„ç†ï¼Œç„¶åå’Œsrc_mask, tgt_mask, memoryä¸€èµ·ä¼ ç»™self.decoder\n",
    "        out = model.decode(memory=memory,\n",
    "                           src_mask=src_mask,\n",
    "                           tgt=ys,\n",
    "                           tgt_mask=subsequent_mask(size=ys.size(1)).type_as(src.data))\n",
    "        # generator: ç±»åˆ«ç”Ÿæˆå™¨å¯¹è±¡ -> linear+softmax\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        # cat(): å®ç°æ‹¼æ¥æ“ä½œ\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)],\n",
    "            dim=1).cuda()\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4b68ab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:08:10.146684Z",
     "iopub.status.busy": "2022-05-18T15:08:10.145996Z",
     "iopub.status.idle": "2022-05-18T15:08:10.154723Z",
     "shell.execute_reply": "2022-05-18T15:08:10.154012Z"
    },
    "papermill": {
     "duration": 0.066351,
     "end_time": "2022-05-18T15:08:10.156364",
     "exception": false,
     "start_time": "2022-05-18T15:08:10.090013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 8. ç¼–å†™å‡½æ•°å°†IDsè½¬åŒ–ä¸ºå­—ç¬¦ä¸²å½¢å¼\n",
    "def ids_to_date_strs(ids, chars_list):\n",
    "    return [\n",
    "        \"\".join([(\" \" + chars_list)[index] for index in sequence])\n",
    "        for sequence in ids\n",
    "    ]\n",
    "\n",
    "\n",
    "# 9. é¢„å¤„ç†åºåˆ— å¼ºåˆ¶è¿›è¡Œ0å¡«å……è‡³length==18(max)\n",
    "max_input_length = 18\n",
    "\n",
    "\n",
    "def prepare_date_strs_padded(date_strs):\n",
    "    X = prepare_date_strs(date_strs, input_chars)\n",
    "    pd = (0, max_input_length - X.shape[1], 0, 0)\n",
    "\n",
    "    if X.shape[1] < max_input_length:\n",
    "        X = F.pad(X, pd, 'constant', 0)\n",
    "    return X\n",
    "\n",
    "# 10. ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹ é¢„æµ‹æ—¥æœŸå­—ç¬¦ä¸²å‡½æ•°\n",
    "def pred_date_strs(model, date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    y_pred_ids = tf.fill(dims=(len(X), 1), value=1)  # åˆå§‹ä½ç½®:<sos>\n",
    "    src_mask = torch.ones(1, 1, 18).cuda()\n",
    "\n",
    "#     # for index in range(max_output_length):\n",
    "#     #     pad_size = max_output_length - y_pred_ids.shape[1]  # 1: 10-1\n",
    "#     #     X_decoder = tf.pad(y_pred_ids, [[0, 0], [0, pad_size]])\n",
    "#     #     # è®¡ç®—ç›®æ ‡å­—ç¬¦è¡¨çš„å­—ç¬¦æ¦‚ç‡ å¹¶è¾“å‡ºæœ€å¤§çš„ids\n",
    "#     #     y_probas_next = model.predict([X, X_decoder])[:, index:index + 1]\n",
    "#     #     y_pred_next = tf.argmax(y_probas_next, axis=-1, output_type=tf.int32)\n",
    "#     #     # å¾ªç¯å°†é¢„æµ‹å­—ç¬¦ä¸Šä¸€ä¸ªå­—ç¬¦ä¸²è¿›è¡Œæ‹¼æ¥\n",
    "#     #     y_pred_ids = tf.concat([y_pred_ids, y_pred_next], axis=1)\n",
    "    ys = greedy_decode(model, src=X, src_mask=src_mask, max_len=tgt_vocab, start_symbol=sos_id)\n",
    "\n",
    "#     # æ’é™¤<sos>\n",
    "    y_pred_str = ids_to_date_strs(ys[:, 1:], output_chars)\n",
    "\n",
    "    return y_pred_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daf7e02e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:08:10.267679Z",
     "iopub.status.busy": "2022-05-18T15:08:10.267421Z",
     "iopub.status.idle": "2022-05-18T15:08:10.322580Z",
     "shell.execute_reply": "2022-05-18T15:08:10.321866Z"
    },
    "papermill": {
     "duration": 0.113045,
     "end_time": "2022-05-18T15:08:10.324472",
     "exception": false,
     "start_time": "2022-05-18T15:08:10.211427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.load('./example_2_date.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f6bdb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:08:10.438527Z",
     "iopub.status.busy": "2022-05-18T15:08:10.438302Z",
     "iopub.status.idle": "2022-05-18T15:08:10.782756Z",
     "shell.execute_reply": "2022-05-18T15:08:10.781965Z"
    },
    "papermill": {
     "duration": 0.402319,
     "end_time": "2022-05-18T15:08:10.784530",
     "exception": false,
     "start_time": "2022-05-18T15:08:10.382211",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 15:08:10.450909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.451964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.452621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.454577: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-18 15:08:10.454858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.455538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.456162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.457945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.458614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.459226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.460681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14045 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2021-05-15']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_date_strs(model, [\"May 15, 2021\"])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 239.5596,
   "end_time": "2022-05-18T15:08:13.840894",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-18T15:04:14.281294",
   "version": "2.3.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
