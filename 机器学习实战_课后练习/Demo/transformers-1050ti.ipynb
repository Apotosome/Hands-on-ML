{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e02517c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:22.472925Z",
     "iopub.status.busy": "2022-05-18T15:04:22.472367Z",
     "iopub.status.idle": "2022-05-18T15:04:32.974461Z",
     "shell.execute_reply": "2022-05-18T15:04:32.973252Z"
    },
    "papermill": {
     "duration": 10.523276,
     "end_time": "2022-05-18T15:04:32.977092",
     "exception": false,
     "start_time": "2022-05-18T15:04:22.453816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /home/sora/anaconda3/envs/DL/lib/python3.10/site-packages (1.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4050046",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:33.014783Z",
     "iopub.status.busy": "2022-05-18T15:04:33.014554Z",
     "iopub.status.idle": "2022-05-18T15:04:33.714413Z",
     "shell.execute_reply": "2022-05-18T15:04:33.712861Z"
    },
    "papermill": {
     "duration": 0.720368,
     "end_time": "2022-05-18T15:04:33.716486",
     "exception": false,
     "start_time": "2022-05-18T15:04:32.996118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 25 16:40:16 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.76       Driver Version: 515.76       CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 40%   36C    P8    N/A /  75W |    223MiB /  4096MiB |     25%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      3023      G   /usr/lib/xorg/Xorg                111MiB |\r\n",
      "|    0   N/A  N/A      3149    C+G   ...ome-remote-desktop-daemon       59MiB |\r\n",
      "|    0   N/A  N/A      3186      G   /usr/bin/gnome-shell               37MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da16be2a",
   "metadata": {
    "papermill": {
     "duration": 0.017858,
     "end_time": "2022-05-18T15:04:33.753838",
     "exception": false,
     "start_time": "2022-05-18T15:04:33.735980",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8645bc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:33.791302Z",
     "iopub.status.busy": "2022-05-18T15:04:33.790619Z",
     "iopub.status.idle": "2022-05-18T15:04:35.637634Z",
     "shell.execute_reply": "2022-05-18T15:04:35.636994Z"
    },
    "papermill": {
     "duration": 1.867772,
     "end_time": "2022-05-18T15:04:35.639447",
     "exception": false,
     "start_time": "2022-05-18T15:04:33.771675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cuda'), device(type='cuda', index=1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')\n",
    "# torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687d0891",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:35.677358Z",
     "iopub.status.busy": "2022-05-18T15:04:35.677152Z",
     "iopub.status.idle": "2022-05-18T15:04:40.416757Z",
     "shell.execute_reply": "2022-05-18T15:04:40.415945Z"
    },
    "papermill": {
     "duration": 4.761301,
     "end_time": "2022-05-18T15:04:40.418785",
     "exception": false,
     "start_time": "2022-05-18T15:04:35.657484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1 导入必备的库\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import altair as alt\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "# 3 Model Architecture\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        \"\"\"\n",
    "        初始化函数\n",
    "        :param encoder: 编码器对象\n",
    "        :param decoder: 解码器对象\n",
    "        :param src_embed: 源数据嵌入函数\n",
    "        :param tgt_embed: 目标数据嵌入函数\n",
    "        :param generator: 类别生成器对象\n",
    "        \"\"\"\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        将src, src_mask传入编码函数，得到结果后与src_mask, tgt和tgt_mask一同传给解码函数\n",
    "        :param src: 源数据\n",
    "        :param tgt: 目标数据\n",
    "        :param src_mask: 源数据掩码张量\n",
    "        :param tgt_mask: 目标数据掩码张量\n",
    "        \"\"\"\n",
    "        memory = self.encode(src, src_mask)\n",
    "        res = self.decode(memory, src_mask, tgt, tgt_mask)\n",
    "        return res\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        编码函数，使用src_embed对source做处理，然后和src_mask一起传给self.encoder\n",
    "        \"\"\"\n",
    "        source_embeddings = self.src_embed(src)\n",
    "        return self.encoder(source_embeddings, src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        \"\"\"\n",
    "        解码函数，使用tgt_embed对target做处理，然后和src_mask,tgt_mask,memory一起传给self.decoder\n",
    "        \"\"\"\n",
    "        target_embeddings = self.tgt_embed(tgt)\n",
    "        return self.decoder(target_embeddings, memory, src_mask, tgt_mask)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    将线性层和softmax计算层一起实现， 把类的名字叫做Generator，生成器类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        初始化函数\n",
    "        :param d_model: 嵌入的维度\n",
    "        :param vocab: vocab.size -> 词表的大小\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(in_features=d_model, out_features=vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        输入是上一层的输出张量x\n",
    "        使用上一步得到的self.proj对x进行线性变化, 然后使用F中已经实现的log_softmax进行softmax处理。\n",
    "        \"\"\"\n",
    "        softmax = F.log_softmax(self.proj(x), dim=-1)\n",
    "        return softmax\n",
    "\n",
    "\n",
    "# 3.1 Encoder and Decoder Stacks\n",
    "\n",
    "# 3.1.1 Encoder\n",
    "def clone(mudule, N):\n",
    "    \"\"\"\n",
    "    用于克隆多份结构\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(mudule) for _ in range(N)])\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clone(layer, N)  # 实现简单的克隆，在叠加在一起\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        将输入（和掩码）依次通过每一层。\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "# 3.1.2 Layer Normalization and Residual Connections\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        初始化函数\n",
    "        :param feature_size: 词嵌入的维度\n",
    "        :param eps: 防止分母为0，默认是1e-6\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # 使用nn.parameter封装，代表他们是模型的参数\n",
    "        self.gamma = nn.Parameter(torch.ones(feature_size))  # 缩放参数向量 初始化为1张量\n",
    "        self.beta = nn.Parameter(torch.zeros(feature_size))  # 平移参数向量 初始化为0张量\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1. 对输入变量x求其最后一个维度,即词嵌入维度的均值，并保持输出维度与输入维度一致\n",
    "        2. 求最后一个维度的标准差，进行规范化：用x减去均值除以标准差\n",
    "        3. 对结果乘以我们的缩放参数gamma, *表示点乘，加上位移参beta\n",
    "        :param x: 来自上一层的输出\n",
    "        \"\"\"\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return (x - mean) / (std + self.eps) * self.gamma + self.beta  ##########\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    SublayerConnection类:实现子层连接结构.𝑥表示上一层添加了残差连接的输出，这一层添加了残差连接的输出需要将  𝑥  执行层级归一化，\n",
    "    然后馈送到 Multi-Head Attention 层或全连接层，添加 Dropout 操作后可作为这一子层级的输出。最后将该子层的输出向量与输入向量相加得到下一层的输入。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        \"\"\"\n",
    "        :param size: 𝑑𝑚𝑜𝑑𝑒𝑙=512\n",
    "        :param dropout: 丢弃参数\n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # 原方案：先将x执行层级归一化\n",
    "        # sublayer_out = sublayer(self.norm(x))\n",
    "        # return x + self.dropout(sublayer_out)\n",
    "        # 改进版本：取出norm 加快收敛速度\n",
    "        sublayer_out = sublayer(x)\n",
    "        sublayer_out = self.dropout(sublayer_out)\n",
    "        return x + self.norm(sublayer_out)\n",
    "\n",
    "\n",
    "# 3.1.3 Encoder Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attention, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clone(mudule=SublayerConnection(size, dropout), N=2)  # 两次的跳过连接\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        第一个子层包括一个多头自注意力层和规范化层以及一个残差连接\n",
    "        第二个子层包括一个前馈全连接层和规范化层以及一个残差连接\n",
    "        \"\"\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))  # 输入 Query、Key 和 Value 都为 x 就表示自注意力。\n",
    "        z = self.sublayer[1](x, self.feed_forward)\n",
    "        return z\n",
    "\n",
    "\n",
    "# 3.1.4 Decoder\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        \"\"\"\n",
    "        :param layer: 解码器层layer\n",
    "        :param N: 解码器层的个数N\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clone(layer, N)  # 实现简单的克隆，在叠加在一起\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "# 3.1.5 Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attention, src_attn, feed_forward, dropout):\n",
    "        \"\"\"\n",
    "        :param self_attention: 多头自注意力对象，该注意力机制需要Q=K=V\n",
    "        :param src_attn: 多头注意力对象，这里Q!=K=V\n",
    "        :param dropout: dropout置0比率\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = self_attention\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clone(mudule=SublayerConnection(size, dropout), N=3)  # 三次的跳过连接\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        :param x: 上一层的输入\n",
    "        :param memory: 来自编码器层的语义存储变量\n",
    "        :param src_mask: 源数据掩码张量\n",
    "        :param tgt_mask: 目标数据掩码张量\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        将x传入第一个子层结构，第一个子层结构的输入分别是x和self-attn函数，因为是自注意力机制，所以Q,K,V都是x，\n",
    "        最后一个参数时目标数据掩码张量，这时要对目标数据进行遮掩，因为此时模型可能还没有生成任何目标数据。\n",
    "        比如在解码器准备生成第一个字符或词汇时，我们其实已经传入了第一个字符以便计算损失，但是我们不希望在生成第一个字符时模型能利用这个信息，\n",
    "        因此我们会将其遮掩，同样生成第二个字符或词汇时，模型只能使用第一个字符或词汇信息，第二个字符以及之后的信息都不允许被模型使用。\n",
    "        \"\"\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))  # 输入 Query、Key 和 Value 都为 x 就表示自注意力。\n",
    "        \"\"\"\n",
    "        接着进入第二个子层，这个子层中常规的注意力机制，q是输入x;\n",
    "        k,v是编码层输出memory，同样也传入source_mask，但是进行源数据遮掩的原因并非是抑制信息泄露，而是遮蔽掉对结果没有意义的padding。\n",
    "        \"\"\"\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        \"\"\"\n",
    "        最后一个子层就是前馈全连接子层，经过它的处理后就可以返回结果，这就是我们的解码器结构\n",
    "        \"\"\"\n",
    "        z = self.sublayer[2](x, self.feed_forward)\n",
    "        return z\n",
    "\n",
    "\n",
    "# 3.1.6 Mask\n",
    "def subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    生成向后遮掩的掩码张量->形成一个三角矩阵\n",
    "    :param size: 掩码张量最后两个维度的大小, 最后两维形成一个方阵\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "\n",
    "    # 然后使用np.ones()向这个形状中添加1元素，np.triu()形成上三角阵\n",
    "    mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "\n",
    "    # 最后将numpy类型转化为torch中的tensor，内部做一个1- 的操作。这个其实是做了一个三角阵的反转，subsequent_mask中的每个元素都会被1减。\n",
    "    # 如果是0，subsequent_mask中的该位置由0变成1\n",
    "    # 如果是1，subsequent_mask中的该位置由1变成0\n",
    "    return torch.from_numpy(mask) == 0\n",
    "\n",
    "\n",
    "# 3.2 Attention\n",
    "\n",
    "# 3.2.1 Scaled Dot-Product Attention\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    实现了缩放点积注意力\n",
    "    1. 首先取query的最后一维的大小，对应词嵌入维度\n",
    "    2. 利用公式计算注意力分数scores, 这里面key是将最后两个维度进行转置 -> (句子长度维度,词(多头)向量维度)\n",
    "    3. 判断是否使用掩码张量\n",
    "    4. 对scores的最后一维进行softmax操作，获得最终的注意力张量\n",
    "    5. 判断是否使用dropout进行随机置0\n",
    "    6. 最后，将p_attn与value张量相乘获得最终的query注意力表示，同时返回注意力张量\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        # 将掩码张量和scores张量每个位置一一比较\n",
    "        # 如果掩码张量则对应的scores张量相同，则用-1e9来替换\n",
    "        scores = scores.masked_fill(mask == 0, value=-1e9)\n",
    "\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    attn = torch.matmul(p_attn, value)\n",
    "    return attn, p_attn\n",
    "\n",
    "\n",
    "# 3.2.2 Multi-Head Attention\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param n_heads: 注意力头数\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param dropout: 比率默认为0.1\n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        # 判断n_heads是否能被d_model整除 -> embedding_dim / n_heads\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_k = d_model // n_heads  # 512//8=64\n",
    "        self.h = n_heads  # 8\n",
    "\n",
    "        # 创建linear层，并且克隆4个 -> Q,K,V各一个，最后拼接的矩阵还需要一个\n",
    "        self.linear = clone(mudule=nn.Linear(d_model, d_model), N=4)  # 512*512\n",
    "        self.p_attn = None  # 代表最后得到的注意力张量\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        1. 从 d_model(512) --> h*d_k(8*64) 批量执行所有线性投影\n",
    "        2. 将注意力集中在所有投影向量上\n",
    "        3. Concat并最终应用到线性层\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # 拓展维度，表示多头中的第n头\n",
    "        n_batches = query.size(0)  # batch_size代表有多少条样本\n",
    "\n",
    "        \"\"\"\n",
    "        1. 首先利用zip将输入QKV与三个线性层组到一起，然后利用for循环，将输入QKV分别传到线性层中,\n",
    "           使用view()对线性变换的结构进行维度重塑，为每个头分割输入\n",
    "               多加了一个维度h代表头，这样就意味着每个头可以获得一部分词特征组成的句子\n",
    "               其中的-1代表自适应维度，即m句子长度维度，将自动计算这里的值\n",
    "           然后对第二维和第三维进行转置操作：\n",
    "               原因：为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，\n",
    "               从attention函数中可以看到，利用的是原始输入的倒数第一和第二维，这样我们就得到了每个头的输入\n",
    "        \"\"\"\n",
    "        query, key, value = [\n",
    "            lin(x).view(n_batches, -1, self.h, self.d_k).transpose(1, 2)  # -1 <-> self.h\n",
    "            for lin, x in zip(self.linear, (query, key, value))\n",
    "        ]\n",
    "        \"\"\"\n",
    "        2. 得到每个头的输入后，接下来就是将他们传入到attention中，\n",
    "           这里直接调用我们之前实现的attention函数，同时也将mask和dropout传入其中\n",
    "        \"\"\"\n",
    "        attn, self.p_attn = attention(query, key, value, mask, self.dropout)\n",
    "        \"\"\"\n",
    "        3. 通过多头注意力计算后，我们就得到了每个头计算结果组成的4维张量，我们需要将其转换为输入的形状以方便后续的计算，\n",
    "           因此这里开始进行第一步处理环节的逆操作，先对第二和第三维进行转置，\n",
    "           然后使用contiguous(): 能够让转置后的张量应用view()，否则将无法直接使用.\n",
    "           下一步就是使用view重塑形状，变成和输入形状相同。  \n",
    "           最后使用线性层列表中的最后一个线性变换得到最终的多头注意力结构的输出\n",
    "        \"\"\"\n",
    "        concat = attn.transpose(1, 2).contiguous().view(n_batches, -1, self.h * self.d_k)\n",
    "        x = self.linear[-1](concat)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# 3.3 Position-wise Feed-Forward Networks\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: 通过前馈全连接层后输入和输出的维度不变\n",
    "        :param d_ff: 内部维度：第二个线性层的输入维度和第一个线性层的输出\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        首先经过第一个线性层，然后使用F中的relu函数进行激活，\n",
    "        之后再使用dropout进行随机置0，最后通过第二个线性层w2，返回最终结果\n",
    "        \"\"\"\n",
    "        x = self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "        return x\n",
    "\n",
    "\n",
    "# 3.4 Embeddings and Softmax\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: 这里代表输入给模型的单词文本通过词表映射后的one-hot向量\n",
    "        :return: 将x传给self.lut并与根号下self.d_model相乘作为结果返回\n",
    "        \"\"\"\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "# 3.5 Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        \"\"\"\n",
    "        :param d_model: 词嵌入维度 这里是512维\n",
    "        :param dropout: 词嵌入维度\n",
    "        :param max_len: 每个句子的最大长度\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 使用与原公式等价的表示\n",
    "        # 目的是避免中间的数值计算结果超出float的范围\n",
    "        pos_embed = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # 0->4999 再插入一个维度(5000,1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )  # shape=[256]\n",
    "        # div_term 实现的是分母\n",
    "        # pe[:, 0::2] 表示第二个维度从 0 开始以间隔为 2 取值，即偶数。\n",
    "        pos_embed[:, ::2] = torch.sin(position * div_term)  # shape=[max_len, 256]\n",
    "        pos_embed[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pos_embed = pos_embed.unsqueeze(0)  # shape=[1, 500, 512]\n",
    "        self.register_buffer('pe', pos_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# 3.6 Full Model\n",
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"\"\"\n",
    "    构建模型\n",
    "    :param src_vocab: 输入词表大小\n",
    "    :param tgt_vocab: 目标词表大小\n",
    "    :param N: 编码器和解码器堆叠的基础模块个数\n",
    "    :param d_model: 词嵌入的维度\n",
    "    :param d_ff: 逐位置的前馈网络中的内部维度\n",
    "    :param h: 注意力头的个数\n",
    "    \"\"\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(n_heads=h, d_model=d_model, dropout=dropout)\n",
    "    ff = PositionwiseFeedForward(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
    "    position = PositionalEncoding(d_model=d_model, dropout=dropout)\n",
    "    # -------\n",
    "    encoderLayer = EncoderLayer(size=d_model, self_attention=c(attn), feed_forward=c(ff), dropout=dropout)\n",
    "    decoderLayer = DecoderLayer(size=d_model, self_attention=c(attn), src_attn=c(attn), feed_forward=c(ff),\n",
    "                                dropout=dropout)\n",
    "    srcEmbed = Embeddings(d_model=d_model, vocab=src_vocab)\n",
    "    tgtEmbed = Embeddings(d_model=d_model, vocab=tgt_vocab)\n",
    "    generator = Generator(d_model=d_model, vocab=tgt_vocab)\n",
    "    # -------\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        encoder=Encoder(layer=encoderLayer, N=N),\n",
    "        decoder=Decoder(layer=decoderLayer, N=N),\n",
    "        src_embed=nn.Sequential(srcEmbed, c(position)),\n",
    "        tgt_embed=nn.Sequential(tgtEmbed, c(position)),\n",
    "        generator=generator\n",
    "    )\n",
    "    # 初始化参数: 使用Glorot初始化: 1/𝑓𝑎𝑛_𝑎𝑣𝑔, 𝑓𝑎𝑛_𝑎𝑣𝑔=(𝑓𝑎𝑛_𝑖𝑛 +𝑓𝑎𝑛_𝑜𝑢𝑡)/2\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553fccd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:40.458861Z",
     "iopub.status.busy": "2022-05-18T15:04:40.458615Z",
     "iopub.status.idle": "2022-05-18T15:04:40.519247Z",
     "shell.execute_reply": "2022-05-18T15:04:40.518543Z"
    },
    "papermill": {
     "duration": 0.082664,
     "end_time": "2022-05-18T15:04:40.521494",
     "exception": false,
     "start_time": "2022-05-18T15:04:40.438830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5 Training\n",
    "# 5.1 Batched and Masking\n",
    "class Batch:\n",
    "    def __init__(self, src, tgt, pad=2):\n",
    "        \"\"\"\n",
    "        :param pad: 默认2 表示<blank>\n",
    "        \"\"\"\n",
    "        self.src = src\n",
    "        # 将与令牌匹配的位置表示为False, 否则为True\n",
    "        # 并在倒数第二个维度后面添加一维度\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1]  # Decoder的输入，即除去最后一个结束token的部分\n",
    "            self.tgt_y = tgt[:, 1:]  # Decoder的期望输入，即除去首个一个起始token的部分\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()  # 所有True的词元数量\n",
    "\n",
    "    @staticmethod\n",
    "    # staticmethod 返回函数的静态方法 可以不实例化即可调用方法\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"\"\"\n",
    "        pad 和 future words 均在mask中用pad表示\n",
    "        \"\"\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        sequence_len = tgt.size(-1)  # 或是batch中最长时间步数\n",
    "        tgt_mask = tgt_mask & subsequent_mask(size=sequence_len).type_as(\n",
    "            tgt_mask.data\n",
    "            # &:进行位运算\n",
    "            # subsequent_mask()返回维度为(1, size, size)\n",
    "            # type_as():将数据类型转换为tgt_mask的数据类型\n",
    "        )\n",
    "        return tgt_mask\n",
    "\n",
    "\n",
    "# 5.2 Training Loop\n",
    "class TrainState:\n",
    "    \"\"\"\n",
    "    跟踪处理的步骤、示例和标记的数量\n",
    "    \"\"\"\n",
    "    step: int = 0  # 当前epoch的步\n",
    "    accum_step: int = 0  # 梯度累积步数\n",
    "    samples: int = 0  # 使用的示例总数\n",
    "    tokens: int = 0  # 处理的tokens总数\n",
    "\n",
    "\n",
    "def run_epoch(data_iter, model, loss_compute,\n",
    "              optimizer, scheduler,\n",
    "              mode=\"train\", accum_iter=1,\n",
    "              train_state=TrainState(),\n",
    "              device=None):\n",
    "    \"\"\"\n",
    "    完成了一个epoch训练的所有工作\n",
    "    包括数据加载、模型推理、损失计算与方向传播，同时将训练过程信息进行打印\n",
    "    \"\"\"\n",
    "    # 训练单个epoch\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0  # 梯度累积步数\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # model是一个EncoderDecoder对象\n",
    "        # 前向传播将src, src_mask传入编码函数，得到结果后与src_mask, tgt和tgt_mask一同传给解码函数\n",
    "        out = model.forward(src=batch.src, tgt=batch.tgt, src_mask=batch.src_mask, tgt_mask=batch.tgt_mask)\n",
    "        # 梯度累加技术 loss_node = loss_node / accum_iter\n",
    "        # accum_iter:小批次数 默认是1 不使用梯度累加技术\n",
    "        loss, loss_node = loss_compute(x=out, y=batch.tgt_y, norm=batch.ntokens)  # 计算损失->SimpleLossCompute\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward()  # 反向传播->不进行梯度清零, 执行梯度累加的操作\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch.src.shape[0]\n",
    "            train_state.tokens += batch.ntokens\n",
    "            if i % accum_iter == 0:  # 梯度累加达到固定次数之后\n",
    "                optimizer.step()  # 更新参数\n",
    "                optimizer.zero_grad(set_to_none=True)  # 梯度清零\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]  # 获取学习率\n",
    "            elapsed = time.time() - start  # 计算40个迭代所需时间\n",
    "            print((\"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \" +\n",
    "                   \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\") %\n",
    "                  (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "\n",
    "        del loss\n",
    "        del loss_node\n",
    "    return total_loss / total_tokens, train_state\n",
    "\n",
    "\n",
    "# 5.2 Optimizer\n",
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    对于 Lambda LR 函数，我们必须将步骤默认为 1 避免零提升为负幂。\n",
    "    :param step: 时间步长\n",
    "    :param model_size: 模型维度\n",
    "    :param factor: 示例中为1\n",
    "    :param warmup: 预热迭代数\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "            model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )\n",
    "\n",
    "\n",
    "def example_learning_schedule():\n",
    "    \"\"\"\n",
    "    学习率调度示例: 在 opts 列表中有 3 个示例。\n",
    "    为每个示例运行 20000 个 epoch\n",
    "    学习率调度使用 自定义调整学习率LambdaLR\n",
    "    数据可视化工具: Altair\n",
    "    \"\"\"\n",
    "    opts = [\n",
    "        [512, 1, 4000],  # example 1\n",
    "        [512, 1, 8000],  # example 2\n",
    "        [256, 1, 4000],  # example 3\n",
    "    ]\n",
    "    dummy_model = torch.nn.Linear(1, 1)\n",
    "    learning_rates = []\n",
    "\n",
    "    for idx, example in enumerate(opts):\n",
    "        optimizer = torch.optim.Adam(dummy_model.parameters(),\n",
    "                                     lr=1,\n",
    "                                     betas=(0.9, 0.98),\n",
    "                                     eps=1e-9)\n",
    "        lr_scheduler = LambdaLR(\n",
    "            optimizer=optimizer,\n",
    "            lr_lambda=lambda step: rate(step, *example))\n",
    "        tmp = []\n",
    "        #  采取20000次的虚拟训练步骤，并保存每一步的学习率\n",
    "        for step in range(20000):\n",
    "            # optimizer.param_groups[0]：长度为6的字典，\n",
    "            # 包括[‘amsgrad’, ‘params’, ‘lr’, ‘betas’, ‘weight_decay’, ‘eps’]\n",
    "            tmp.append(optimizer.param_groups[0][\"lr\"])\n",
    "            optimizer.step()  # 更新参数\n",
    "            lr_scheduler.step()  # 更新参数\n",
    "        learning_rates.append(tmp)\n",
    "\n",
    "    learning_rates = torch.tensor(learning_rates)\n",
    "    # ----数据可视化----\n",
    "    # 使 altair 能够处理超过 5000 行\n",
    "    alt.data_transformers.disable_max_rows()\n",
    "\n",
    "    opts_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Learning Rate\": learning_rates[warmup_idx, :],\n",
    "                    \"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][\n",
    "                        warmup_idx\n",
    "                    ],\n",
    "                    \"step\": range(20000),\n",
    "                }\n",
    "            )\n",
    "            for warmup_idx in [0, 1, 2]\n",
    "        ]\n",
    "    )\n",
    "    return (\n",
    "        alt.Chart(opts_data)\n",
    "            .mark_line()\n",
    "            .properties(width=600)\n",
    "            .encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\")\n",
    "            .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "# 5.3 Regularization\n",
    "\n",
    "# 5.3.2 Label Smoothing\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())\n",
    "\n",
    "\n",
    "RUN_EXAMPLES = True\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def example_label_smoothing():\n",
    "    crit = LabelSmoothing(5, 0, 0.4)\n",
    "    predict = torch.FloatTensor(\n",
    "        [\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "        ]\n",
    "    )\n",
    "    crit(x=predict.log(), target=torch.LongTensor([2, 1, 0, 3, 3]))\n",
    "    LS_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"target distribution\": crit.true_dist[x, y].flatten(),\n",
    "                    \"columns\": y,\n",
    "                    \"rows\": x,\n",
    "                }\n",
    "            )\n",
    "            for y in range(5)\n",
    "            for x in range(5)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(LS_data)\n",
    "            .mark_rect(color=\"Blue\", opacity=1)\n",
    "            .properties(height=200, width=200)\n",
    "            .encode(\n",
    "            alt.X(\"columns:O\", title=None),\n",
    "            alt.Y(\"rows:O\", title=None),\n",
    "            alt.Color(\n",
    "                \"target distribution:Q\", scale=alt.Scale(scheme=\"viridis\")\n",
    "            ),\n",
    "        )\n",
    "            .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "def loss(x, crit):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])\n",
    "    return crit(predict.log(), torch.LongTensor([1])).data\n",
    "\n",
    "\n",
    "def penalization_visualization():\n",
    "    crit = LabelSmoothing(5, 0, 0.1)\n",
    "    loss_data = pd.DataFrame({\n",
    "        \"Loss\": [loss(x, crit) for x in range(1, 100)],\n",
    "        \"Steps\": list(range(99)),\n",
    "    }).astype(\"float\")\n",
    "\n",
    "    return (alt.Chart(loss_data).mark_line().properties(width=350).encode(\n",
    "        x=\"Steps\",\n",
    "        y=\"Loss\",\n",
    "    ).interactive())\n",
    "\n",
    "\n",
    "# 6 A First Example\n",
    "\n",
    "# 6.1 Synthetic Data\n",
    "def data_gen(V, n_batches, batch_size, s_len=10, device=None):\n",
    "    \"\"\"\n",
    "    <编码器-解码器数据复制任务> 随机数据生成器\n",
    "    :param device: 是否使用GPU加速\n",
    "    :param V: 词典数量，取值范围[0, V-1]，约定0作为特殊符号使用代表padding\n",
    "    :param batch_size: 批次大小\n",
    "    :param n_batches: 需要生成的批次数量\n",
    "    :param s_len: 生成的序列数据的长度\n",
    "    \"\"\"\n",
    "    for i in range(n_batches):\n",
    "        src_data = torch.randint(2, V, size=(batch_size, s_len))\n",
    "        # 约定输出为输入除去序列第一个元素，即向后平移一位进行输出，同时输出数据要在第一个时间步添加一个起始符\n",
    "        tgt_data = src_data.clone()\n",
    "        tgt_data[:, 0] = 1  # 将序列的第一个时间步置为1(即约定的起始符)\n",
    "        # .batch()\n",
    "        # 返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置\n",
    "        # 不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad。\n",
    "        # requires_grad 默认为False\n",
    "        src = src_data.requires_grad_(False).clone().detach()\n",
    "        tgt = tgt_data.requires_grad_(False).clone().detach()\n",
    "        if device == \"cuda\":\n",
    "            src = src.cuda()\n",
    "            tgt = tgt.cuda()\n",
    "        yield Batch(src=src, tgt=tgt, pad=0)\n",
    "\n",
    "\n",
    "# 6.2 Loss Computation\n",
    "class SimpleLossCompute:\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion  # 使用标签平滑\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        \"\"\"\n",
    "        :param x: decoder输出的结果\n",
    "        :param y: 标签数据\n",
    "        :param norm: loss的归一化系数，用batch中所有有效token数即可\n",
    "        \"\"\"\n",
    "        x = self.generator(x)\n",
    "        # contiguous():\n",
    "        # 1. 由于torch.view等方法操作需要连续的Tensor\n",
    "        # 2. 出于性能考虑 使用该方法后会重新据开辟一块内存空间保证数是在逻辑顺序和内存中是一致的\n",
    "        x_ = x.contiguous().view(-1, x.size(-1))\n",
    "        y_ = y.contiguous().view(-1)\n",
    "        loss = self.criterion(x_, y_)\n",
    "        sloss = (loss / norm)\n",
    "\n",
    "        return sloss.data * norm, loss\n",
    "\n",
    "\n",
    "# 6.3 Greedy Decoding\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    # encoder()编码函数: 使用src_embed对src做处理，然后和src_mask一起传给self.encoder\n",
    "    memory = model.encode(src=src, src_mask=src_mask)\n",
    "    # ys代表目前已生成的序列，最初为仅包含一个起始符的序列，不断将预测结果追加到序列最后\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len - 1):\n",
    "        # decoder()解码函数: 使用tgt_embed对tgt做处理，然后和src_mask, tgt_mask, memory一起传给self.decoder\n",
    "        out = model.decode(memory=memory,\n",
    "                           src_mask=src_mask,\n",
    "                           tgt=ys,\n",
    "                           tgt_mask=subsequent_mask(size=ys.size(1)).type_as(src.data))\n",
    "        # generator: 类别生成器对象 -> linear+softmax\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        # cat(): 实现拼接操作\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)],\n",
    "            dim=1)\n",
    "    return ys\n",
    "\n",
    "\n",
    "# 6.4 Training Example\n",
    "# def execute_example(fn, args=[]):\n",
    "#     if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "#         fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "\n",
    "def example_simple_model(device=None):\n",
    "    V = 11  # 字典的大小\n",
    "    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "    model = make_model(src_vocab=V, tgt_vocab=V, N=2)\n",
    "    if device == \"cuda\":\n",
    "        model.cuda()\n",
    "    model_size = model.src_embed[0].d_model  # 512\n",
    "\n",
    "    n_epochs = 40\n",
    "    n_batch_train_epoch = 30  # 训练时每个epoch所需批次大小\n",
    "    n_batch_val_epoch = 10  # 验证时每个epoch所需批次大小\n",
    "    batch_size = 40\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=0.5,\n",
    "                                 betas=(0.9, 0.98),\n",
    "                                 eps=1e-9)\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(step=step, model_size=model_size, factor=0.1, warmup=400)\n",
    "    )\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_compute = SimpleLossCompute(generator=model.generator,\n",
    "                                         criterion=criterion)\n",
    "\n",
    "        print(f\"\\n|   批次: {epoch}   |\")\n",
    "        print(\"*\" * 5 + \"训练\" + \"*\" * 5)\n",
    "        model.train()  # self.training=True\n",
    "\n",
    "        train_data_iter = data_gen(V=V, n_batches=n_batch_train_epoch,\n",
    "                                   batch_size=batch_size, device=device)\n",
    "        run_epoch(data_iter=train_data_iter,\n",
    "                  model=model,\n",
    "                  loss_compute=loss_compute,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=lr_scheduler,\n",
    "                  mode=\"train\",)\n",
    "\n",
    "        # -----------\n",
    "        print(\"*\" * 5 + \"验证\" + \"*\" * 5)\n",
    "        model.eval()  # self.training=False\n",
    "\n",
    "        val_data_iter = data_gen(V=V, n_batches=n_batch_val_epoch,\n",
    "                                 batch_size=batch_size, device=device)\n",
    "        valid_mean_loss = run_epoch(data_iter=val_data_iter,\n",
    "                                    model=model,\n",
    "                                    loss_compute=loss_compute,\n",
    "                                    optimizer=DummyOptimizer(),  # None\n",
    "                                    scheduler=DummyScheduler(),  # None\n",
    "                                    mode=\"eval\",)[0]  # 返回: total_loss / total_tokens\n",
    "        print(f\"|验证损失: {valid_mean_loss} |\")\n",
    "\n",
    "    model.eval()\n",
    "    torch.save(model, './example_1_copy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea675ff3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:40.560739Z",
     "iopub.status.busy": "2022-05-18T15:04:40.560202Z",
     "iopub.status.idle": "2022-05-18T15:04:40.563843Z",
     "shell.execute_reply": "2022-05-18T15:04:40.563069Z"
    },
    "papermill": {
     "duration": 0.025708,
     "end_time": "2022-05-18T15:04:40.565887",
     "exception": false,
     "start_time": "2022-05-18T15:04:40.540179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|   批次: 0   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   3.17 | Tokens / Sec:  2327.3 | Learning Rate: 5.5e-07\n",
      "*****验证*****\n",
      "|验证损失: 2.394113779067993 |\n",
      "\n",
      "|   批次: 1   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   2.40 | Tokens / Sec:  9412.5 | Learning Rate: 8.8e-06\n",
      "*****验证*****\n",
      "|验证损失: 1.9547275304794312 |\n",
      "\n",
      "|   批次: 2   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   2.21 | Tokens / Sec:  9569.1 | Learning Rate: 1.7e-05\n",
      "*****验证*****\n",
      "|验证损失: 1.7825837135314941 |\n",
      "\n",
      "|   批次: 3   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.97 | Tokens / Sec:  9915.8 | Learning Rate: 2.5e-05\n",
      "*****验证*****\n",
      "|验证损失: 1.6604243516921997 |\n",
      "\n",
      "|   批次: 4   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.90 | Tokens / Sec:  9883.9 | Learning Rate: 3.4e-05\n",
      "*****验证*****\n",
      "|验证损失: 1.5723007917404175 |\n",
      "\n",
      "|   批次: 5   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.72 | Tokens / Sec:  9371.5 | Learning Rate: 4.2e-05\n",
      "*****验证*****\n",
      "|验证损失: 1.4360578060150146 |\n",
      "\n",
      "|   批次: 6   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.64 | Tokens / Sec:  9481.5 | Learning Rate: 5.0e-05\n",
      "*****验证*****\n",
      "|验证损失: 1.2682843208312988 |\n",
      "\n",
      "|   批次: 7   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.47 | Tokens / Sec:  9443.6 | Learning Rate: 5.9e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.9558234214782715 |\n",
      "\n",
      "|   批次: 8   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.29 | Tokens / Sec:  9301.9 | Learning Rate: 6.7e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.7557151913642883 |\n",
      "\n",
      "|   批次: 9   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.20 | Tokens / Sec:  9342.2 | Learning Rate: 7.5e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.5053467154502869 |\n",
      "\n",
      "|   批次: 10   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.93 | Tokens / Sec:  9864.1 | Learning Rate: 8.3e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.2739480435848236 |\n",
      "\n",
      "|   批次: 11   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.64 | Tokens / Sec:  9942.4 | Learning Rate: 9.2e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.1763959378004074 |\n",
      "\n",
      "|   批次: 12   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.50 | Tokens / Sec:  9836.1 | Learning Rate: 1.0e-04\n",
      "*****验证*****\n",
      "|验证损失: 0.11010845005512238 |\n",
      "\n",
      "|   批次: 13   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.38 | Tokens / Sec:  9889.8 | Learning Rate: 1.1e-04\n",
      "*****验证*****\n",
      "|验证损失: 0.04243813827633858 |\n",
      "\n",
      "|   批次: 14   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.26 | Tokens / Sec:  9201.6 | Learning Rate: 1.1e-04\n",
      "*****验证*****\n",
      "|验证损失: 0.06136145442724228 |\n",
      "\n",
      "|   批次: 15   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.22 | Tokens / Sec:  9260.4 | Learning Rate: 1.0e-04\n",
      "*****验证*****\n",
      "|验证损失: 0.04074167460203171 |\n",
      "\n",
      "|   批次: 16   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.15 | Tokens / Sec:  9413.2 | Learning Rate: 1.0e-04\n",
      "*****验证*****\n",
      "|验证损失: 0.017386775463819504 |\n",
      "\n",
      "|   批次: 17   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.15 | Tokens / Sec:  9407.1 | Learning Rate: 9.8e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.023547537624835968 |\n",
      "\n",
      "|   批次: 18   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.12 | Tokens / Sec:  9487.5 | Learning Rate: 9.5e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.014222418889403343 |\n",
      "\n",
      "|   批次: 19   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.13 | Tokens / Sec:  9414.4 | Learning Rate: 9.2e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.008601798675954342 |\n",
      "\n",
      "|   批次: 20   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.11 | Tokens / Sec:  9354.6 | Learning Rate: 9.0e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.00962837878614664 |\n",
      "\n",
      "|   批次: 21   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.07 | Tokens / Sec:  9297.8 | Learning Rate: 8.8e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.0032831078860908747 |\n",
      "\n",
      "|   批次: 22   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.07 | Tokens / Sec:  9115.8 | Learning Rate: 8.6e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.006906671449542046 |\n",
      "\n",
      "|   批次: 23   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.09 | Tokens / Sec:  9201.5 | Learning Rate: 8.4e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.009479006752371788 |\n",
      "\n",
      "|   批次: 24   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.03 | Tokens / Sec:  9775.9 | Learning Rate: 8.2e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.006316591054201126 |\n",
      "\n",
      "|   批次: 25   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.10 | Tokens / Sec:  9843.4 | Learning Rate: 8.1e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.0020393519662320614 |\n",
      "\n",
      "|   批次: 26   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.06 | Tokens / Sec:  9456.5 | Learning Rate: 7.9e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.00981949083507061 |\n",
      "\n",
      "|   批次: 27   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.06 | Tokens / Sec:  9164.5 | Learning Rate: 7.8e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.004193373955786228 |\n",
      "\n",
      "|   批次: 28   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.04 | Tokens / Sec:  9386.5 | Learning Rate: 7.6e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.006194210145622492 |\n",
      "\n",
      "|   批次: 29   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.05 | Tokens / Sec:  9317.7 | Learning Rate: 7.5e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.0052854204550385475 |\n",
      "\n",
      "|   批次: 30   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.09 | Tokens / Sec:  9246.6 | Learning Rate: 7.4e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.009405386634171009 |\n",
      "\n",
      "|   批次: 31   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.08 | Tokens / Sec:  9269.6 | Learning Rate: 7.2e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.0021834897343069315 |\n",
      "\n",
      "|   批次: 32   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.08 | Tokens / Sec:  9661.4 | Learning Rate: 7.1e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.0030930545181035995 |\n",
      "\n",
      "|   批次: 33   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.07 | Tokens / Sec:  9883.6 | Learning Rate: 7.0e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.0010114101460203528 |\n",
      "\n",
      "|   批次: 34   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.07 | Tokens / Sec:  9407.5 | Learning Rate: 6.9e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.0016180539969354868 |\n",
      "\n",
      "|   批次: 35   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.05 | Tokens / Sec:  9505.2 | Learning Rate: 6.8e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.0005277844611555338 |\n",
      "\n",
      "|   批次: 36   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.05 | Tokens / Sec:  9301.0 | Learning Rate: 6.7e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.0017933434573933482 |\n",
      "\n",
      "|   批次: 37   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.02 | Tokens / Sec:  9893.1 | Learning Rate: 6.6e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.0012114470591768622 |\n",
      "\n",
      "|   批次: 38   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9725.4 | Learning Rate: 6.5e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.0011449726298451424 |\n",
      "\n",
      "|   批次: 39   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.08 | Tokens / Sec:  9365.2 | Learning Rate: 6.5e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.0007832083501853049 |\n"
     ]
    }
   ],
   "source": [
    "example_simple_model(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d7b9bb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:40.604458Z",
     "iopub.status.busy": "2022-05-18T15:04:40.604271Z",
     "iopub.status.idle": "2022-05-18T15:04:40.607289Z",
     "shell.execute_reply": "2022-05-18T15:04:40.606613Z"
    },
    "papermill": {
     "duration": 0.02323,
     "end_time": "2022-05-18T15:04:40.608947",
     "exception": false,
     "start_time": "2022-05-18T15:04:40.585717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  6,  3,  3,  5,  6,  5,  8,  8, 10]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('./example_1_copy.pth')\n",
    "src = torch.LongTensor([[1, 6, 3, 3, 5, 6, 5, 8, 8, 10]]).cuda()\n",
    "max_len = src.shape[1]\n",
    "src_mask = torch.ones(1, 1, max_len).cuda()\n",
    "print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eafd757",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:40.645433Z",
     "iopub.status.busy": "2022-05-18T15:04:40.645253Z",
     "iopub.status.idle": "2022-05-18T15:04:40.648569Z",
     "shell.execute_reply": "2022-05-18T15:04:40.647901Z"
    },
    "papermill": {
     "duration": 0.023354,
     "end_time": "2022-05-18T15:04:40.650128",
     "exception": false,
     "start_time": "2022-05-18T15:04:40.626774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4606a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:40.686998Z",
     "iopub.status.busy": "2022-05-18T15:04:40.686799Z",
     "iopub.status.idle": "2022-05-18T15:04:40.715107Z",
     "shell.execute_reply": "2022-05-18T15:04:40.714509Z"
    },
    "papermill": {
     "duration": 0.048612,
     "end_time": "2022-05-18T15:04:40.716688",
     "exception": false,
     "start_time": "2022-05-18T15:04:40.668076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. 随机生成日期, 并以输入格式和目标格式显示\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "\n",
    "def random_dates(n_dates):\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "\n",
    "    ordinals = np.random.randint(low=min_date, high=max_date + 1, size=n_dates)\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "\n",
    "    X = [MONTHS[date.month - 1] + \" \" + date.strftime(\"%d, %Y\") for date in dates]\n",
    "    y = [date.isoformat() for date in dates]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# 2. 确定输入,目标的词汇(字符)表\n",
    "input_chars = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
    "output_chars = \"0123456789-\"\n",
    "src_vocab = len(input_chars)\n",
    "tgt_vocab = len(output_chars)\n",
    "\n",
    "\n",
    "# 3. 编写函数将字符串转化为IDs形式\n",
    "def date_str_to_ids(date_str, chars_list):\n",
    "    return [chars_list.index(c) + 1 for c in date_str]\n",
    "\n",
    "\n",
    "# 4.处理可变长度的序列\n",
    "def prepare_date_strs(date_strs, chars=input_chars):\n",
    "    X_ids = [torch.tensor(date_str_to_ids(date, chars)).cuda() for date in date_strs]\n",
    "    X = pad_sequence(X_ids, batch_first=True, padding_value=0)\n",
    "    return X\n",
    "# def prepare_date_strs(date_strs, chars=input_chars):\n",
    "#     X_ids = [torch.tensor(date_str_to_ids(date, chars)) for date in date_strs]\n",
    "#     X = pad_sequence(X_ids, batch_first=True, padding_value=0)\n",
    "#     return X\n",
    "\n",
    "\n",
    "# 5. 批量和掩码 Batched and Masking\n",
    "class Batch:\n",
    "    def __init__(self, src, tgt, pad=0):\n",
    "        \"\"\"\n",
    "        :param pad: 默认0 表示<blank>\n",
    "        \"\"\"\n",
    "        self.src = src\n",
    "        # 将与令牌匹配的位置表示为False, 否则为True\n",
    "        # 并在倒数第二个维度后面添加一维度\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1]  # Decoder的输入，即除去最后一个结束token的部分\n",
    "            self.tgt_y = tgt[:, 1:]  # Decoder的期望输入，即除去首个一个起始token的部分\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()  # 所有True的词元数量\n",
    "\n",
    "    @staticmethod\n",
    "    # staticmethod 返回函数的静态方法 可以不实例化即可调用方法\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"\"\"\n",
    "        pad 和 future words 均在mask中用pad表示\n",
    "        \"\"\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        sequence_len = tgt.size(-1)  # 或是batch中最长时间步数\n",
    "        tgt_mask = tgt_mask & subsequent_mask(size=sequence_len).type_as(\n",
    "            tgt_mask.data\n",
    "            # &:进行位运算\n",
    "            # subsequent_mask()返回维度为(1, size, size)\n",
    "            # type_as():将数据类型转换为tgt_mask的数据类型\n",
    "        )\n",
    "        return tgt_mask\n",
    "\n",
    "\n",
    "# 6. 构建数据集\n",
    "\n",
    "sos_id = tgt_vocab + 1  # 11+1=12\n",
    "\n",
    "\n",
    "# def shift_output_sequences(y, device=None):\n",
    "#     if device == \"cuda\":\n",
    "#         sos_token = torch.Tensor(len(y), 1).fill_(sos_id).int().cuda()\n",
    "#         decoder = torch.cat((sos_token, y[:, :-1]), axis=1).cuda()\n",
    "#     else:\n",
    "#         sos_token = torch.Tensor(len(y), 1).fill_(sos_id).int()\n",
    "#         decoder = torch.cat((sos_token, y[:, :-1]), axis=1)\n",
    "#     return decoder\n",
    "def shift_output_sequences(y, device=None):\n",
    "    if device == \"cuda\":\n",
    "        sos_token = torch.Tensor(len(y), 1).fill_(sos_id).int().cuda()\n",
    "        decoder = torch.cat((sos_token, y), axis=1).cuda()\n",
    "    else:\n",
    "        sos_token = torch.Tensor(len(y), 1).fill_(sos_id).int()\n",
    "        decoder = torch.cat((sos_token, y), axis=1)\n",
    "    return decoder\n",
    "\n",
    "\n",
    "def create_dataset(n_dates, device=None):\n",
    "    X, y = random_dates(n_dates)\n",
    "    X_pre = prepare_date_strs(X, input_chars)\n",
    "    y_pre = prepare_date_strs(y, output_chars)\n",
    "    y_pre_shift = shift_output_sequences(y_pre, device=device)\n",
    "\n",
    "    #         X_pre[:, 0] = 1  # 将序列的第一个时间步置为1(即约定的起始符)\n",
    "    #     y_pre[:, 0] = 1\n",
    "    return X_pre, y_pre_shift\n",
    "\n",
    "\n",
    "def data_gen(n_batches, batch_size, device=None):\n",
    "    \"\"\"\n",
    "    <编码器-解码器日期字符串转换任务> 随机数据生成器\n",
    "    :param batch_size: 批次大小\n",
    "    :param n_batches: 需要生成的批次数量\n",
    "    \"\"\"\n",
    "    for i in range(n_batches):\n",
    "        X_pre, y_pre = create_dataset(batch_size,device=device)\n",
    "        # data = torch.randint(2, V, size=(batch_size, s_len))\n",
    "        # .batch()\n",
    "        # 返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置\n",
    "        # 不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad。\n",
    "        # requires_grad 默认为False\n",
    "        src = X_pre.requires_grad_(False).clone().detach()\n",
    "        tgt = y_pre.requires_grad_(False).clone().detach()\n",
    "        if device == \"cuda\":\n",
    "            src = src.cuda()\n",
    "            tgt = tgt.cuda()\n",
    "        yield Batch(src=src, tgt=tgt, pad=0)\n",
    "\n",
    "\n",
    "# 7. 训练评估模型\n",
    "def example_simple_model(device=None):\n",
    "    # V = 11  # 字典的大小\n",
    "    criterion = LabelSmoothing(size=tgt_vocab + 2, padding_idx=0, smoothing=0.0)\n",
    "    model = make_model(src_vocab=src_vocab + 1, tgt_vocab=tgt_vocab + 2, N=2)\n",
    "    if device == \"cuda\":\n",
    "        model.cuda()\n",
    "    model_size = model.src_embed[0].d_model  # 512\n",
    "\n",
    "    n_epochs = 20\n",
    "    n_batch_train_epoch = 200  # 训练时每个epoch所需批次大小\n",
    "    n_batch_val_epoch = 50  # 验证时每个epoch所需批次大小\n",
    "    batch_size = 100\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=0.5,\n",
    "                                 betas=(0.9, 0.98),\n",
    "                                 eps=1e-9)\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step=step, model_size=model_size, factor=0.1, warmup=600))\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        loss_compute = SimpleLossCompute(generator=model.generator,\n",
    "                                         criterion=criterion)\n",
    "\n",
    "        print(f\"\\n|   批次: {epoch}   |\")\n",
    "        print(\"*\" * 5 + \"训练\" + \"*\" * 5)\n",
    "        model.train()  # self.training=True\n",
    "\n",
    "        train_data_iter = data_gen(n_batches=n_batch_train_epoch,\n",
    "                                   batch_size=batch_size,\n",
    "                                   device=device)\n",
    "        run_epoch(data_iter=train_data_iter,\n",
    "                  model=model,\n",
    "                  loss_compute=loss_compute,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=lr_scheduler,\n",
    "                  mode=\"train\")\n",
    "\n",
    "        # -----------\n",
    "        print(\"*\" * 5 + \"验证\" + \"*\" * 5)\n",
    "        model.eval()  # self.training=False\n",
    "\n",
    "        val_data_iter = data_gen(n_batches=n_batch_val_epoch,\n",
    "                                 batch_size=batch_size,\n",
    "                                 device=device)\n",
    "        valid_mean_loss = run_epoch(\n",
    "            data_iter=val_data_iter,\n",
    "            model=model,\n",
    "            loss_compute=loss_compute,\n",
    "            optimizer=DummyOptimizer(),  # None\n",
    "            scheduler=DummyScheduler(),  # None\n",
    "            mode=\"eval\")[0]  # 返回: total_loss / total_tokens\n",
    "        print(f\"|验证损失: {valid_mean_loss} |\")\n",
    "\n",
    "    model.eval()\n",
    "    torch.save(model, './example_2_date.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d1d53f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:04:40.753557Z",
     "iopub.status.busy": "2022-05-18T15:04:40.753373Z",
     "iopub.status.idle": "2022-05-18T15:08:09.901642Z",
     "shell.execute_reply": "2022-05-18T15:08:09.900533Z"
    },
    "papermill": {
     "duration": 209.169764,
     "end_time": "2022-05-18T15:08:09.904190",
     "exception": false,
     "start_time": "2022-05-18T15:04:40.734426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|   批次: 0   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   3.15 | Tokens / Sec:  9014.5 | Learning Rate: 3.0e-07\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   2.39 | Tokens / Sec:  7208.7 | Learning Rate: 6.3e-06\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   2.02 | Tokens / Sec:  7394.0 | Learning Rate: 1.2e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   1.67 | Tokens / Sec:  7564.0 | Learning Rate: 1.8e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   1.36 | Tokens / Sec:  7263.0 | Learning Rate: 2.4e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.9065974950790405 |\n",
      "\n",
      "|   批次: 1   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.14 | Tokens / Sec:  9374.3 | Learning Rate: 3.0e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.98 | Tokens / Sec:  7221.5 | Learning Rate: 3.6e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.83 | Tokens / Sec:  7419.8 | Learning Rate: 4.2e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.73 | Tokens / Sec:  7439.3 | Learning Rate: 4.8e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.62 | Tokens / Sec:  7390.4 | Learning Rate: 5.4e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.3712591826915741 |\n",
      "\n",
      "|   批次: 2   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.53 | Tokens / Sec:  9824.6 | Learning Rate: 6.0e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.48 | Tokens / Sec:  7475.0 | Learning Rate: 6.6e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.37 | Tokens / Sec:  7220.8 | Learning Rate: 7.2e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.24 | Tokens / Sec:  7223.8 | Learning Rate: 7.8e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.22 | Tokens / Sec:  7183.2 | Learning Rate: 8.4e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.04182703047990799 |\n",
      "\n",
      "|   批次: 3   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.15 | Tokens / Sec:  9221.8 | Learning Rate: 9.0e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.14 | Tokens / Sec:  7261.3 | Learning Rate: 8.7e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.10 | Tokens / Sec:  7263.5 | Learning Rate: 8.5e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.08 | Tokens / Sec:  7375.3 | Learning Rate: 8.2e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.07 | Tokens / Sec:  7217.4 | Learning Rate: 8.0e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.011830809526145458 |\n",
      "\n",
      "|   批次: 4   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.06 | Tokens / Sec:  9243.4 | Learning Rate: 7.8e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.07 | Tokens / Sec:  7136.7 | Learning Rate: 7.6e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.05 | Tokens / Sec:  7152.0 | Learning Rate: 7.4e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.04 | Tokens / Sec:  7256.3 | Learning Rate: 7.3e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.06 | Tokens / Sec:  7155.7 | Learning Rate: 7.1e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.0012394256191328168 |\n",
      "\n",
      "|   批次: 5   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.03 | Tokens / Sec:  9338.9 | Learning Rate: 7.0e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.03 | Tokens / Sec:  7177.2 | Learning Rate: 6.8e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.03 | Tokens / Sec:  7291.8 | Learning Rate: 6.7e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.04 | Tokens / Sec:  7250.2 | Learning Rate: 6.6e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.03 | Tokens / Sec:  7578.7 | Learning Rate: 6.5e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.00025529967388138175 |\n",
      "\n",
      "|   批次: 6   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.02 | Tokens / Sec:  9882.7 | Learning Rate: 6.4e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.03 | Tokens / Sec:  7276.8 | Learning Rate: 6.3e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.03 | Tokens / Sec:  7243.4 | Learning Rate: 6.2e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.02 | Tokens / Sec:  7443.2 | Learning Rate: 6.1e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7587.3 | Learning Rate: 6.0e-05\n",
      "*****验证*****\n",
      "|验证损失: 7.481365173589438e-05 |\n",
      "\n",
      "|   批次: 7   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9157.2 | Learning Rate: 5.9e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.01 | Tokens / Sec:  7572.5 | Learning Rate: 5.8e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.01 | Tokens / Sec:  7453.4 | Learning Rate: 5.7e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.02 | Tokens / Sec:  7173.7 | Learning Rate: 5.7e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7244.1 | Learning Rate: 5.6e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.0001459354389226064 |\n",
      "\n",
      "|   批次: 8   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9207.2 | Learning Rate: 5.5e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.01 | Tokens / Sec:  7154.1 | Learning Rate: 5.5e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.02 | Tokens / Sec:  7140.3 | Learning Rate: 5.4e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.01 | Tokens / Sec:  7378.0 | Learning Rate: 5.3e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7244.5 | Learning Rate: 5.3e-05\n",
      "*****验证*****\n",
      "|验证损失: 7.281082798726857e-05 |\n",
      "\n",
      "|   批次: 9   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9937.2 | Learning Rate: 5.2e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.02 | Tokens / Sec:  7198.4 | Learning Rate: 5.1e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.01 | Tokens / Sec:  7159.6 | Learning Rate: 5.1e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.01 | Tokens / Sec:  7167.4 | Learning Rate: 5.0e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7193.7 | Learning Rate: 5.0e-05\n",
      "*****验证*****\n",
      "|验证损失: 0.0012970819370821118 |\n",
      "\n",
      "|   批次: 10   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.03 | Tokens / Sec:  9247.0 | Learning Rate: 4.9e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.01 | Tokens / Sec:  7182.7 | Learning Rate: 4.9e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.00 | Tokens / Sec:  7471.1 | Learning Rate: 4.8e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7474.7 | Learning Rate: 4.8e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7181.5 | Learning Rate: 4.8e-05\n",
      "*****验证*****\n",
      "|验证损失: 3.976562584284693e-05 |\n",
      "\n",
      "|   批次: 11   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9311.4 | Learning Rate: 4.7e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.00 | Tokens / Sec:  7224.6 | Learning Rate: 4.7e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.02 | Tokens / Sec:  7388.1 | Learning Rate: 4.6e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.01 | Tokens / Sec:  7563.5 | Learning Rate: 4.6e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.00 | Tokens / Sec:  7252.2 | Learning Rate: 4.5e-05\n",
      "*****验证*****\n",
      "|验证损失: 5.729972690460272e-05 |\n",
      "\n",
      "|   批次: 12   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9314.0 | Learning Rate: 4.5e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.01 | Tokens / Sec:  7194.8 | Learning Rate: 4.5e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.01 | Tokens / Sec:  7207.4 | Learning Rate: 4.4e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.01 | Tokens / Sec:  7191.3 | Learning Rate: 4.4e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.00 | Tokens / Sec:  7182.0 | Learning Rate: 4.4e-05\n",
      "*****验证*****\n",
      "|验证损失: 2.7599184249993414e-05 |\n",
      "\n",
      "|   批次: 13   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9299.7 | Learning Rate: 4.3e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.00 | Tokens / Sec:  7150.6 | Learning Rate: 4.3e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.00 | Tokens / Sec:  7148.6 | Learning Rate: 4.3e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7350.1 | Learning Rate: 4.2e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7151.9 | Learning Rate: 4.2e-05\n",
      "*****验证*****\n",
      "|验证损失: 3.36340963258408e-05 |\n",
      "\n",
      "|   批次: 14   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9927.8 | Learning Rate: 4.2e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.00 | Tokens / Sec:  7275.8 | Learning Rate: 4.1e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.00 | Tokens / Sec:  7266.8 | Learning Rate: 4.1e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7150.6 | Learning Rate: 4.1e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7349.0 | Learning Rate: 4.1e-05\n",
      "*****验证*****\n",
      "|验证损失: 2.267956551804673e-05 |\n",
      "\n",
      "|   批次: 15   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.00 | Tokens / Sec:  9243.4 | Learning Rate: 4.0e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.00 | Tokens / Sec:  7252.6 | Learning Rate: 4.0e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.00 | Tokens / Sec:  7176.7 | Learning Rate: 4.0e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7351.7 | Learning Rate: 4.0e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.00 | Tokens / Sec:  7167.0 | Learning Rate: 3.9e-05\n",
      "*****验证*****\n",
      "|验证损失: 1.996382707147859e-05 |\n",
      "\n",
      "|   批次: 16   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.00 | Tokens / Sec:  9903.0 | Learning Rate: 3.9e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.00 | Tokens / Sec:  7357.6 | Learning Rate: 3.9e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.00 | Tokens / Sec:  7284.3 | Learning Rate: 3.9e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7276.9 | Learning Rate: 3.8e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.00 | Tokens / Sec:  7690.6 | Learning Rate: 3.8e-05\n",
      "*****验证*****\n",
      "|验证损失: 1.9218403394916095e-05 |\n",
      "\n",
      "|   批次: 17   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9349.5 | Learning Rate: 3.8e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.00 | Tokens / Sec:  7402.5 | Learning Rate: 3.8e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.00 | Tokens / Sec:  7244.3 | Learning Rate: 3.7e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7171.0 | Learning Rate: 3.7e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.01 | Tokens / Sec:  7263.0 | Learning Rate: 3.7e-05\n",
      "*****验证*****\n",
      "|验证损失: 1.3489638149621896e-05 |\n",
      "\n",
      "|   批次: 18   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  9276.8 | Learning Rate: 3.7e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.00 | Tokens / Sec:  7164.0 | Learning Rate: 3.7e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.01 | Tokens / Sec:  7169.3 | Learning Rate: 3.6e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7144.2 | Learning Rate: 3.6e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.00 | Tokens / Sec:  7438.6 | Learning Rate: 3.6e-05\n",
      "*****验证*****\n",
      "|验证损失: 1.4737389392394107e-05 |\n",
      "\n",
      "|   批次: 19   |\n",
      "*****训练*****\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.00 | Tokens / Sec:  9186.4 | Learning Rate: 3.6e-05\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   0.01 | Tokens / Sec:  7138.3 | Learning Rate: 3.6e-05\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   0.01 | Tokens / Sec:  7139.2 | Learning Rate: 3.5e-05\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   0.00 | Tokens / Sec:  7349.9 | Learning Rate: 3.5e-05\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   0.00 | Tokens / Sec:  7331.7 | Learning Rate: 3.5e-05\n",
      "*****验证*****\n",
      "|验证损失: 1.458849055779865e-05 |\n"
     ]
    }
   ],
   "source": [
    "example_simple_model(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0132d27b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:08:10.025999Z",
     "iopub.status.busy": "2022-05-18T15:08:10.025448Z",
     "iopub.status.idle": "2022-05-18T15:08:10.033155Z",
     "shell.execute_reply": "2022-05-18T15:08:10.032337Z"
    },
    "papermill": {
     "duration": 0.067273,
     "end_time": "2022-05-18T15:08:10.035208",
     "exception": false,
     "start_time": "2022-05-18T15:08:09.967935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    # encoder()编码函数: 使用src_embed对src做处理，然后和src_mask一起传给self.encoder\n",
    "    memory = model.encode(src=src, src_mask=src_mask)\n",
    "    # ys代表目前已生成的序列，最初为仅包含一个起始符的序列，不断将预测结果追加到序列最后\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data).cuda()\n",
    "    for i in range(max_len-1):\n",
    "        # decoder()解码函数: 使用tgt_embed对tgt做处理，然后和src_mask, tgt_mask, memory一起传给self.decoder\n",
    "        out = model.decode(memory=memory,\n",
    "                           src_mask=src_mask,\n",
    "                           tgt=ys,\n",
    "                           tgt_mask=subsequent_mask(size=ys.size(1)).type_as(src.data))\n",
    "        # generator: 类别生成器对象 -> linear+softmax\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        # cat(): 实现拼接操作\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)],\n",
    "            dim=1).cuda()\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4b68ab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:08:10.146684Z",
     "iopub.status.busy": "2022-05-18T15:08:10.145996Z",
     "iopub.status.idle": "2022-05-18T15:08:10.154723Z",
     "shell.execute_reply": "2022-05-18T15:08:10.154012Z"
    },
    "papermill": {
     "duration": 0.066351,
     "end_time": "2022-05-18T15:08:10.156364",
     "exception": false,
     "start_time": "2022-05-18T15:08:10.090013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 8. 编写函数将IDs转化为字符串形式\n",
    "def ids_to_date_strs(ids, chars_list):\n",
    "    return [\n",
    "        \"\".join([(\" \" + chars_list)[index] for index in sequence])\n",
    "        for sequence in ids\n",
    "    ]\n",
    "\n",
    "\n",
    "# 9. 预处理序列 强制进行0填充至length==18(max)\n",
    "max_input_length = 18\n",
    "\n",
    "\n",
    "def prepare_date_strs_padded(date_strs):\n",
    "    X = prepare_date_strs(date_strs, input_chars)\n",
    "    pd = (0, max_input_length - X.shape[1], 0, 0)\n",
    "\n",
    "    if X.shape[1] < max_input_length:\n",
    "        X = F.pad(X, pd, 'constant', 0)\n",
    "    return X\n",
    "\n",
    "# 10. 使用模型进行预测 预测日期字符串函数\n",
    "def pred_date_strs(model, date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    y_pred_ids = tf.fill(dims=(len(X), 1), value=1)  # 初始位置:<sos>\n",
    "    src_mask = torch.ones(1, 1, 18).cuda()\n",
    "\n",
    "#     # for index in range(max_output_length):\n",
    "#     #     pad_size = max_output_length - y_pred_ids.shape[1]  # 1: 10-1\n",
    "#     #     X_decoder = tf.pad(y_pred_ids, [[0, 0], [0, pad_size]])\n",
    "#     #     # 计算目标字符表的字符概率 并输出最大的ids\n",
    "#     #     y_probas_next = model.predict([X, X_decoder])[:, index:index + 1]\n",
    "#     #     y_pred_next = tf.argmax(y_probas_next, axis=-1, output_type=tf.int32)\n",
    "#     #     # 循环将预测字符上一个字符串进行拼接\n",
    "#     #     y_pred_ids = tf.concat([y_pred_ids, y_pred_next], axis=1)\n",
    "    ys = greedy_decode(model, src=X, src_mask=src_mask, max_len=tgt_vocab, start_symbol=sos_id)\n",
    "\n",
    "#     # 排除<sos>\n",
    "    y_pred_str = ids_to_date_strs(ys[:, 1:], output_chars)\n",
    "\n",
    "    return y_pred_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daf7e02e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:08:10.267679Z",
     "iopub.status.busy": "2022-05-18T15:08:10.267421Z",
     "iopub.status.idle": "2022-05-18T15:08:10.322580Z",
     "shell.execute_reply": "2022-05-18T15:08:10.321866Z"
    },
    "papermill": {
     "duration": 0.113045,
     "end_time": "2022-05-18T15:08:10.324472",
     "exception": false,
     "start_time": "2022-05-18T15:08:10.211427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.load('./example_2_date.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f6bdb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T15:08:10.438527Z",
     "iopub.status.busy": "2022-05-18T15:08:10.438302Z",
     "iopub.status.idle": "2022-05-18T15:08:10.782756Z",
     "shell.execute_reply": "2022-05-18T15:08:10.781965Z"
    },
    "papermill": {
     "duration": 0.402319,
     "end_time": "2022-05-18T15:08:10.784530",
     "exception": false,
     "start_time": "2022-05-18T15:08:10.382211",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 15:08:10.450909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.451964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.452621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.454577: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-18 15:08:10.454858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.455538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.456162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.457945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.458614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.459226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-18 15:08:10.460681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14045 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2021-05-15']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_date_strs(model, [\"May 15, 2021\"])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 239.5596,
   "end_time": "2022-05-18T15:08:13.840894",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-18T15:04:14.281294",
   "version": "2.3.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
