{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c3f6219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:01.627808Z",
     "iopub.status.busy": "2022-04-11T17:18:01.627048Z",
     "iopub.status.idle": "2022-04-11T17:18:07.607156Z",
     "shell.execute_reply": "2022-04-11T17:18:07.606142Z",
     "shell.execute_reply.started": "2022-04-11T17:16:23.078876Z"
    },
    "papermill": {
     "duration": 6.030298,
     "end_time": "2022-04-11T17:18:07.607329",
     "exception": false,
     "start_time": "2022-04-11T17:18:01.577031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import time\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def initialization(seed=42):\n",
    "    keras.backend.clear_session()\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20640b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:07.693772Z",
     "iopub.status.busy": "2022-04-11T17:18:07.692857Z",
     "iopub.status.idle": "2022-04-11T17:18:07.695303Z",
     "shell.execute_reply": "2022-04-11T17:18:07.694707Z",
     "shell.execute_reply.started": "2022-04-11T17:16:28.979454Z"
    },
    "papermill": {
     "duration": 0.046469,
     "end_time": "2022-04-11T17:18:07.695424",
     "exception": false,
     "start_time": "2022-04-11T17:18:07.648955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import font_manager\n",
    "my_font = font_manager.FontProperties(fname='./Fonts/SourceHanSerifSC-Medium.otf', size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288dbab2",
   "metadata": {
    "heading_collapsed": true,
    "papermill": {
     "duration": 0.038166,
     "end_time": "2022-04-11T17:18:07.773412",
     "exception": false,
     "start_time": "2022-04-11T17:18:07.735246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 使用Character RNN生成莎士比亚风格的文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472d5c1b",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.036835,
     "end_time": "2022-04-11T17:18:07.847757",
     "exception": false,
     "start_time": "2022-04-11T17:18:07.810922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 创建训练数据集 Creating the Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e81fe6",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.037577,
     "end_time": "2022-04-11T17:18:07.924142",
     "exception": false,
     "start_time": "2022-04-11T17:18:07.886565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. 使用`Keras`的`get_file()`函数，从项目中下载所有莎士比亚的作品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1207a91e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:08.004513Z",
     "iopub.status.busy": "2022-04-11T17:18:08.003640Z",
     "iopub.status.idle": "2022-04-11T17:18:08.590269Z",
     "shell.execute_reply": "2022-04-11T17:18:08.590704Z",
     "shell.execute_reply.started": "2022-04-11T17:16:28.986024Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.629265,
     "end_time": "2022-04-11T17:18:08.590874",
     "exception": false,
     "start_time": "2022-04-11T17:18:07.961609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52207d51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:08.673262Z",
     "iopub.status.busy": "2022-04-11T17:18:08.672593Z",
     "iopub.status.idle": "2022-04-11T17:18:08.675380Z",
     "shell.execute_reply": "2022-04-11T17:18:08.675786Z",
     "shell.execute_reply.started": "2022-04-11T17:16:29.850123Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.045289,
     "end_time": "2022-04-11T17:18:08.675945",
     "exception": false,
     "start_time": "2022-04-11T17:18:08.630656",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb1f7707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:08.755741Z",
     "iopub.status.busy": "2022-04-11T17:18:08.754979Z",
     "iopub.status.idle": "2022-04-11T17:18:08.771581Z",
     "shell.execute_reply": "2022-04-11T17:18:08.772043Z",
     "shell.execute_reply.started": "2022-04-11T17:16:29.857862Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.057756,
     "end_time": "2022-04-11T17:18:08.772161",
     "exception": false,
     "start_time": "2022-04-11T17:18:08.714405",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(sorted(set(shakespeare_text.lower())))   # 显示文本中出现的所有字符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66bdf6",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.038368,
     "end_time": "2022-04-11T17:18:08.849535",
     "exception": false,
     "start_time": "2022-04-11T17:18:08.811167",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "2. 将每个字符编码为一个整数。\n",
    "    - 创建一个自定义预处理层，\n",
    "    - 或使用`Keras`的`分词器Tokenizer`会更加简单。\n",
    "    \n",
    "> **tf.keras.preprocessing.text.Tokenizer**\n",
    "> ```python   \n",
    "tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',  \n",
    "    # filters过滤: 不包含 '\n",
    "    lower=True,   # 是否转化为小写\n",
    "    split=' ',\n",
    "    char_level=False,   # true: 每个字符都将被视为一个标记\n",
    "    oov_token=None,\n",
    "    document_count=0,\n",
    "    **kwargs\n",
    ")\n",
    "> ```   \n",
    "> \n",
    "> 默认情况下，所有标点符号都被删除，将文本转换为空格分隔的单词序列（单词可能包括 ' 字符）。然后将这些序列拆分为分词列表。然后它们将被索引或矢量化。\n",
    "0 是一个保留索引，不会分配给任何单词。\n",
    ">\n",
    "> `fit_on_texts(texts)` : 根据文本列表更新内部词汇表\n",
    ">\n",
    "> `fit_on_sequences(texts)` : 根据序列列表更新内部词汇表\n",
    ">\n",
    "> `get_config()` : 根据文本列表更新内部词汇表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43836f9d",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.038823,
     "end_time": "2022-04-11T17:18:08.927028",
     "exception": false,
     "start_time": "2022-04-11T17:18:08.888205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- 首先，将一个将`tokenizer`拟合到文本：`tokenizer`能从文本中发现所有的字符，并将所有字符映射到不同的字符ID，**映射从1开始**.\n",
    "- 设置`char_level=True`，以得到**字符级别的编码**，而不是默认的单词级别的编码。这个`tokenizer`默认**将所有文本转换成了小写**（如果不想这样，可以设置`lower=False`）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68268101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:09.037717Z",
     "iopub.status.busy": "2022-04-11T17:18:09.032675Z",
     "iopub.status.idle": "2022-04-11T17:18:10.379755Z",
     "shell.execute_reply": "2022-04-11T17:18:10.379221Z",
     "shell.execute_reply.started": "2022-04-11T17:16:29.882006Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 1.413763,
     "end_time": "2022-04-11T17:18:10.379898",
     "exception": false,
     "start_time": "2022-04-11T17:18:08.966135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2bd3c",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.038634,
     "end_time": "2022-04-11T17:18:10.458621",
     "exception": false,
     "start_time": "2022-04-11T17:18:10.419987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- 现在`tokenizer`可以将一整句（或句子列表）编码为`字符ID列表`，这可以告诉我们文本中有多少个独立的字符，以及总字符数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62adb170",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:10.541498Z",
     "iopub.status.busy": "2022-04-11T17:18:10.540900Z",
     "iopub.status.idle": "2022-04-11T17:18:10.543467Z",
     "shell.execute_reply": "2022-04-11T17:18:10.543869Z",
     "shell.execute_reply.started": "2022-04-11T17:16:31.291448Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.046605,
     "end_time": "2022-04-11T17:18:10.544003",
     "exception": false,
     "start_time": "2022-04-11T17:18:10.497398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['First'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d678bdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:10.626673Z",
     "iopub.status.busy": "2022-04-11T17:18:10.625762Z",
     "iopub.status.idle": "2022-04-11T17:18:10.629623Z",
     "shell.execute_reply": "2022-04-11T17:18:10.629220Z",
     "shell.execute_reply.started": "2022-04-11T17:16:31.300935Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.046906,
     "end_time": "2022-04-11T17:18:10.629725",
     "exception": false,
     "start_time": "2022-04-11T17:18:10.582819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])   # 不区分大小写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c063befb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:10.730627Z",
     "iopub.status.busy": "2022-04-11T17:18:10.728745Z",
     "iopub.status.idle": "2022-04-11T17:18:10.734370Z",
     "shell.execute_reply": "2022-04-11T17:18:10.733797Z",
     "shell.execute_reply.started": "2022-04-11T17:16:31.309684Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.053455,
     "end_time": "2022-04-11T17:18:10.734520",
     "exception": false,
     "start_time": "2022-04-11T17:18:10.681065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 1,\n",
       " 'e': 2,\n",
       " 't': 3,\n",
       " 'o': 4,\n",
       " 'a': 5,\n",
       " 'i': 6,\n",
       " 'h': 7,\n",
       " 's': 8,\n",
       " 'r': 9,\n",
       " 'n': 10,\n",
       " '\\n': 11,\n",
       " 'l': 12,\n",
       " 'd': 13,\n",
       " 'u': 14,\n",
       " 'm': 15,\n",
       " 'y': 16,\n",
       " 'w': 17,\n",
       " ',': 18,\n",
       " 'c': 19,\n",
       " 'f': 20,\n",
       " 'g': 21,\n",
       " 'b': 22,\n",
       " 'p': 23,\n",
       " ':': 24,\n",
       " 'k': 25,\n",
       " 'v': 26,\n",
       " '.': 27,\n",
       " \"'\": 28,\n",
       " ';': 29,\n",
       " '?': 30,\n",
       " '!': 31,\n",
       " '-': 32,\n",
       " 'j': 33,\n",
       " 'q': 34,\n",
       " 'x': 35,\n",
       " 'z': 36,\n",
       " '3': 37,\n",
       " '&': 38,\n",
       " '$': 39}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 列出字符索引\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed3d69f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:10.824995Z",
     "iopub.status.busy": "2022-04-11T17:18:10.824247Z",
     "iopub.status.idle": "2022-04-11T17:18:10.827412Z",
     "shell.execute_reply": "2022-04-11T17:18:10.826907Z",
     "shell.execute_reply.started": "2022-04-11T17:16:31.322307Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.048765,
     "end_time": "2022-04-11T17:18:10.827510",
     "exception": false,
     "start_time": "2022-04-11T17:18:10.778745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 字符索引数\n",
    "max_id = len(tokenizer.word_index)\n",
    "max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef5b5385",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:10.912735Z",
     "iopub.status.busy": "2022-04-11T17:18:10.912064Z",
     "iopub.status.idle": "2022-04-11T17:18:10.914722Z",
     "shell.execute_reply": "2022-04-11T17:18:10.915146Z",
     "shell.execute_reply.started": "2022-04-11T17:16:31.331264Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.047252,
     "end_time": "2022-04-11T17:18:10.915260",
     "exception": false,
     "start_time": "2022-04-11T17:18:10.868008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 总字符数\n",
    "dataset_size = tokenizer.document_count\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075ba8b",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.041051,
     "end_time": "2022-04-11T17:18:10.996865",
     "exception": false,
     "start_time": "2022-04-11T17:18:10.955814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- 现在对完整文本做编码，将每个字符都用ID来表示（减1**使ID从0到38**，而不是1到39）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7623c973",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:11.086789Z",
     "iopub.status.busy": "2022-04-11T17:18:11.086080Z",
     "iopub.status.idle": "2022-04-11T17:18:11.451240Z",
     "shell.execute_reply": "2022-04-11T17:18:11.450081Z",
     "shell.execute_reply.started": "2022-04-11T17:16:31.339425Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.41368,
     "end_time": "2022-04-11T17:18:11.451586",
     "exception": false,
     "start_time": "2022-04-11T17:18:11.037906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19,  5,  8, ..., 20, 26, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) -1\n",
    "encoded  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62200b63",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.040954,
     "end_time": "2022-04-11T17:18:11.537856",
     "exception": false,
     "start_time": "2022-04-11T17:18:11.496902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 如何区分序列数据集 How to Split a Sequential Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f7635",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.041438,
     "end_time": "2022-04-11T17:18:11.620949",
     "exception": false,
     "start_time": "2022-04-11T17:18:11.579511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "避免训练集、验证集、测试集发生重合非常重要。\n",
    "\n",
    "当处理时间序列时，\n",
    "- 通常按照时间切分,\n",
    "- 也可以按照其它维度来切分，可以得到更长的时间周期进行训练。如果训练集中的数据存在高度关联性, 则测试集的意义就不大，泛化误差会存在偏移。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5f9f3d",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.041051,
     "end_time": "2022-04-11T17:18:11.703553",
     "exception": false,
     "start_time": "2022-04-11T17:18:11.662502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "在莎士比亚案例中, 可以取90%的文本作为训练集，5%作为验证集，5%作为测试集。在这三个数据之间留出空隙，以避免段落重叠也是非常好的主意。\n",
    "\n",
    "创建`tf.data.Dataset`, 可以从数据集中一个个返回字符."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5151d26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:13.916986Z",
     "iopub.status.busy": "2022-04-11T17:18:11.789817Z",
     "iopub.status.idle": "2022-04-11T17:18:14.295567Z",
     "shell.execute_reply": "2022-04-11T17:18:14.295134Z",
     "shell.execute_reply.started": "2022-04-11T17:16:31.726484Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 2.550903,
     "end_time": "2022-04-11T17:18:14.295700",
     "exception": false,
     "start_time": "2022-04-11T17:18:11.744797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90 // 100     # train_size=1003854\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e700ebcf",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.04208,
     "end_time": "2022-04-11T17:18:14.379593",
     "exception": false,
     "start_time": "2022-04-11T17:18:14.337513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 将序列数据集切分成多个窗口 Chopping the Sequential Dataset into Multiple Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc74a3b2",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.04154,
     "end_time": "2022-04-11T17:18:14.463119",
     "exception": false,
     "start_time": "2022-04-11T17:18:14.421579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e764c",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.04142,
     "end_time": "2022-04-11T17:18:14.546415",
     "exception": false,
     "start_time": "2022-04-11T17:18:14.504995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. 让我们将序列 0 到 14 拆分为长度为 5 的窗口，每个窗口移动 2\n",
    "\n",
    "        [0, 1, 2, 3, 4], [2, 3, 4, 5, 6], [4, 5, 6, 7, 8], ....\n",
    "    \n",
    "2. 然后将它们打乱，并将它们拆分为`inputs`（前 4 步）和`targets`（后 4 步）\n",
    "\n",
    "        [2, 3, 4, 5, 6] 将被拆分为 [[2, 3, 4, 5], [3, 4, 5, 6]])\n",
    "  \n",
    "3. 然后创建 3 个这样的`inputs`/`targets`对的批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42d9d2b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:14.636168Z",
     "iopub.status.busy": "2022-04-11T17:18:14.635428Z",
     "iopub.status.idle": "2022-04-11T17:18:14.641432Z",
     "shell.execute_reply": "2022-04-11T17:18:14.641820Z",
     "shell.execute_reply.started": "2022-04-11T17:16:33.892159Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.053443,
     "end_time": "2022-04-11T17:18:14.641993",
     "exception": false,
     "start_time": "2022-04-11T17:18:14.588550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "initialization(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf4a80",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.041914,
     "end_time": "2022-04-11T17:18:14.726301",
     "exception": false,
     "start_time": "2022-04-11T17:18:14.684387",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- 调节`n_steps`：用短输入序列训练`RNN`更为简单，但是因此`RNN`学不到任何长度超过`n_steps`的规律，所以`n_steps`不要太短。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "206a6066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:14.814912Z",
     "iopub.status.busy": "2022-04-11T17:18:14.814069Z",
     "iopub.status.idle": "2022-04-11T17:18:14.818499Z",
     "shell.execute_reply": "2022-04-11T17:18:14.818038Z",
     "shell.execute_reply.started": "2022-04-11T17:16:33.903525Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.050324,
     "end_time": "2022-04-11T17:18:14.818623",
     "exception": false,
     "start_time": "2022-04-11T17:18:14.768299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_steps = 5\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(tf.range(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc64528",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.062414,
     "end_time": "2022-04-11T17:18:14.923356",
     "exception": false,
     "start_time": "2022-04-11T17:18:14.860942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- 使用数据集的`window()`，将这个**长序列转化为许多小窗口文本**。每个实例都是完整文本的相对短的子字符串，`RNN`只在这些子字符串上展开。这被称为`截断沿时间反向传播truncated backpropagation through time`\n",
    "    >\n",
    "    > ```python\n",
    "    window(\n",
    "        size, \n",
    "        shift=None, \n",
    "        stride=1, \n",
    "        drop_remainder=False,   \n",
    "        name=None\n",
    "    )\n",
    "    > ```\n",
    "    > - `drop_remainder`:如果最后一个窗口的大小小于 `size`，是否应该删除最后一个窗口。\n",
    "    > - `shift`: 表示**窗口**在每次迭代中**移动**的输入元素的数量.\n",
    "    > - `stride`: 步幅,默认为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb7c72ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:15.025275Z",
     "iopub.status.busy": "2022-04-11T17:18:15.024722Z",
     "iopub.status.idle": "2022-04-11T17:18:15.082273Z",
     "shell.execute_reply": "2022-04-11T17:18:15.082852Z",
     "shell.execute_reply.started": "2022-04-11T17:16:33.911299Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.106581,
     "end_time": "2022-04-11T17:18:15.083044",
     "exception": false,
     "start_time": "2022-04-11T17:18:14.976463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[2, 3, 4, 5, 6]\n",
      "[4, 5, 6, 7, 8]\n",
      "[6, 7, 8, 9, 10]\n",
      "[8, 9, 10, 11, 12]\n",
      "[10, 11, 12, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = test_dataset.window(size=n_steps, \n",
    "                                   shift=2, \n",
    "                                   drop_remainder=True)\n",
    "\n",
    "for ds in test_dataset:\n",
    "    print( [elem.numpy() for elem in ds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e04844",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.041999,
     "end_time": "2022-04-11T17:18:15.168547",
     "exception": false,
     "start_time": "2022-04-11T17:18:15.126548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    迭代次数: ( len(text)-window_size )//shift +1 = (16-5)//2+1 = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ef99d",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.041625,
     "end_time": "2022-04-11T17:18:15.252431",
     "exception": false,
     "start_time": "2022-04-11T17:18:15.210806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- 使用`window()`,返回一个嵌套的数据,类似于`list of lists`. 当调用数据集方法处理（比如`shuffle`或做`batch`）每个窗口时，这样会很方便。\n",
    "\n",
    "    但是，不能直接使用嵌套数据集来训练，因为模型要的输入是`tensors`，不是`datasets`。因此，必须调用`flat_map()`方法：\n",
    "    > - `flat_map()`:它能将嵌套数据集转换成**展平**的数据集。\n",
    "    > \n",
    "    >   例: 假设 `{1, 2, 3}` 表示包含张量1、2、3的序列。如果将嵌套数据集 `{{1, 2}, {3, 4, 5, 6}}` 打平，就会得到` {1, 2, 3, 4, 5, 6}` 。\n",
    "    >\n",
    "    >\n",
    "    > - `flat_map()`方法可以接收函数作为参数，可以处理嵌套数据集的每个数据集。\n",
    "    >\n",
    "    >   例: 如果将函数 `lambda ds: ds.batch(2)` 传递给 `flat_map()` ，它能将 `{{1, 2}, {3, 4, 5, 6}}` 转变为` {[1, 2], [3, 4], [5, 6]}` ：这是一个张量大小为2的数据集。\n",
    "    >\n",
    "    >   每个窗口上调用了`batch(window_length)`：因为所有窗口都是这个长度，对于每个窗口，都能得到一个独立的张量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8a23ef8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:15.344002Z",
     "iopub.status.busy": "2022-04-11T17:18:15.343455Z",
     "iopub.status.idle": "2022-04-11T17:18:15.436747Z",
     "shell.execute_reply": "2022-04-11T17:18:15.436149Z",
     "shell.execute_reply.started": "2022-04-11T17:16:33.966329Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.142516,
     "end_time": "2022-04-11T17:18:15.436895",
     "exception": false,
     "start_time": "2022-04-11T17:18:15.294379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[2, 3, 4, 5, 6]\n",
      "[4, 5, 6, 7, 8]\n",
      "[6, 7, 8, 9, 10]\n",
      "[8, 9, 10, 11, 12]\n",
      "[10, 11, 12, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = test_dataset.flat_map(lambda windows: windows.batch(5))\n",
    "\n",
    "for ds in test_dataset:\n",
    "    print([elem.numpy() for elem in ds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c382f9",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.042586,
     "end_time": "2022-04-11T17:18:15.522208",
     "exception": false,
     "start_time": "2022-04-11T17:18:15.479622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- 现在的数据集包含连续的窗口，每个有5个字符。因为梯度下降在训练集中的实例`独立同分布`时的效果最好，需要`shuffle`这些窗口。然后我们可以对窗口做`batch`，分割`inputs`（前4个字符）和`targets`(除去第一个字符)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af8e650b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:15.613704Z",
     "iopub.status.busy": "2022-04-11T17:18:15.613060Z",
     "iopub.status.idle": "2022-04-11T17:18:15.653073Z",
     "shell.execute_reply": "2022-04-11T17:18:15.652558Z",
     "shell.execute_reply.started": "2022-04-11T17:16:34.098471Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.088301,
     "end_time": "2022-04-11T17:18:15.653196",
     "exception": false,
     "start_time": "2022-04-11T17:18:15.564895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.shuffle(10).map(lambda windows:\n",
    "                                            (windows[:-1], windows[1:]))\n",
    "test_dataset = test_dataset.batch(3).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7ee0147",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:15.744582Z",
     "iopub.status.busy": "2022-04-11T17:18:15.743794Z",
     "iopub.status.idle": "2022-04-11T17:18:15.777986Z",
     "shell.execute_reply": "2022-04-11T17:18:15.778444Z",
     "shell.execute_reply.started": "2022-04-11T17:16:34.156772Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.081648,
     "end_time": "2022-04-11T17:18:15.778591",
     "exception": false,
     "start_time": "2022-04-11T17:18:15.696943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Batch 0 \n",
      "X_batch\n",
      "[[6 7 8 9]\n",
      " [2 3 4 5]\n",
      " [4 5 6 7]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 7  8  9 10]\n",
      " [ 3  4  5  6]\n",
      " [ 5  6  7  8]]\n",
      "____________________ Batch 1 \n",
      "X_batch\n",
      "[[ 0  1  2  3]\n",
      " [ 8  9 10 11]\n",
      " [10 11 12 13]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 1  2  3  4]\n",
      " [ 9 10 11 12]\n",
      " [11 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "for index, (X_batch, Y_batch) in enumerate(test_dataset):\n",
    "    print(\"_\" * 20, \"Batch\", index, \"\\nX_batch\")\n",
    "    print(X_batch.numpy())\n",
    "    print(\"=\" * 5, \"\\nY_batch\")\n",
    "    print(Y_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d91e324",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.042825,
     "end_time": "2022-04-11T17:18:15.864566",
     "exception": false,
     "start_time": "2022-04-11T17:18:15.821741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 使用在莎士比亚数据集上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ddea2",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.043526,
     "end_time": "2022-04-11T17:18:15.951772",
     "exception": false,
     "start_time": "2022-04-11T17:18:15.908246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"./images/other/15-22.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53fa9393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:16.047084Z",
     "iopub.status.busy": "2022-04-11T17:18:16.045141Z",
     "iopub.status.idle": "2022-04-11T17:18:16.049960Z",
     "shell.execute_reply": "2022-04-11T17:18:16.049482Z",
     "shell.execute_reply.started": "2022-04-11T17:16:34.208754Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.053697,
     "end_time": "2022-04-11T17:18:16.050083",
     "exception": false,
     "start_time": "2022-04-11T17:18:15.996386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "initialization(42)\n",
    "\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1  # target = input 后面的 1 个字符\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49da8dd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:16.142934Z",
     "iopub.status.busy": "2022-04-11T17:18:16.142393Z",
     "iopub.status.idle": "2022-04-11T17:18:16.165685Z",
     "shell.execute_reply": "2022-04-11T17:18:16.165144Z",
     "shell.execute_reply.started": "2022-04-11T17:16:34.224919Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.072113,
     "end_time": "2022-04-11T17:18:16.165809",
     "exception": false,
     "start_time": "2022-04-11T17:18:16.093696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.window(size=window_length, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    train_size=1003854\n",
    "    迭代次数: ( len(text)-window_size ) // shift +1 \n",
    "           = ( 1003854 - 101 ) // 1 + 1 = 1003754\n",
    "    即: 共有1003754个窗口, 每个窗口大小为101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "419c4e45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:16.258428Z",
     "iopub.status.busy": "2022-04-11T17:18:16.257634Z",
     "iopub.status.idle": "2022-04-11T17:18:16.284682Z",
     "shell.execute_reply": "2022-04-11T17:18:16.284176Z",
     "shell.execute_reply.started": "2022-04-11T17:16:34.270313Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.075423,
     "end_time": "2022-04-11T17:18:16.284789",
     "exception": false,
     "start_time": "2022-04-11T17:18:16.209366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5bab93",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.042742,
     "end_time": "2022-04-11T17:18:16.370701",
     "exception": false,
     "start_time": "2022-04-11T17:18:16.327959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- 特征编码可以选择`独热编码`或`嵌入`。这里使用**独热编码**，因为独立字符不多."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21d2015c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:16.462829Z",
     "iopub.status.busy": "2022-04-11T17:18:16.462005Z",
     "iopub.status.idle": "2022-04-11T17:18:16.486208Z",
     "shell.execute_reply": "2022-04-11T17:18:16.485651Z",
     "shell.execute_reply.started": "2022-04-11T17:16:34.320869Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.072948,
     "end_time": "2022-04-11T17:18:16.486328",
     "exception": false,
     "start_time": "2022-04-11T17:18:16.413380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch, Y_batch:\n",
    "                      (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c3d132",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.042611,
     "end_time": "2022-04-11T17:18:16.574623",
     "exception": false,
     "start_time": "2022-04-11T17:18:16.532012",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- `prefetch()`实现预提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c54c5c89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:16.665116Z",
     "iopub.status.busy": "2022-04-11T17:18:16.664484Z",
     "iopub.status.idle": "2022-04-11T17:18:16.666983Z",
     "shell.execute_reply": "2022-04-11T17:18:16.667411Z",
     "shell.execute_reply.started": "2022-04-11T17:16:34.370388Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.049609,
     "end_time": "2022-04-11T17:18:16.667533",
     "exception": false,
     "start_time": "2022-04-11T17:18:16.617924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c27054d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:16.757414Z",
     "iopub.status.busy": "2022-04-11T17:18:16.756649Z",
     "iopub.status.idle": "2022-04-11T17:18:18.617491Z",
     "shell.execute_reply": "2022-04-11T17:18:18.618103Z",
     "shell.execute_reply.started": "2022-04-11T17:16:34.398527Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 1.908102,
     "end_time": "2022-04-11T17:18:18.618298",
     "exception": false,
     "start_time": "2022-04-11T17:18:16.710196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6e458",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.043275,
     "end_time": "2022-04-11T17:18:18.706560",
     "exception": false,
     "start_time": "2022-04-11T17:18:18.663285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 搭建并训练Char-RNN模型 Building and Training the Char-RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b8ca6",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.043715,
     "end_time": "2022-04-11T17:18:18.793391",
     "exception": false,
     "start_time": "2022-04-11T17:18:18.749676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "根据前面的100个字符预测下一个字符:\n",
    "\n",
    "- 使用一个`RNN`，含有两个`GRU`层，每个128个单元，每个单元对`输入dropout`和`隐藏态recurrent_dropout`的丢失率是20%。如果需要的话，后面可以微调这些超参数。\n",
    "\n",
    "- 输出层是一个时间分布的紧密层，有39个单元,即`max_id`，因为文本中有39个不同的字符，需要输出每个可能字符（在每个时间步）的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3604d6",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": 0.043302,
     "end_time": "2022-04-11T17:18:18.880650",
     "exception": false,
     "start_time": "2022-04-11T17:18:18.837348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> 注意：当使用以下参数的默认值时，`GRU` 类将只使用 `GPU`（如果有的话）：`activation`、`recurrent_activation`、`recurrent_dropout`、`unroll`、`use_bias` 和 `reset_after`。\n",
    ">\n",
    "> 详见: https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/keras/layers/GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0f54bea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:18.973978Z",
     "iopub.status.busy": "2022-04-11T17:18:18.973235Z",
     "iopub.status.idle": "2022-04-11T17:18:19.377184Z",
     "shell.execute_reply": "2022-04-11T17:18:19.376695Z",
     "shell.execute_reply.started": "2022-04-11T17:16:36.472563Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 0.452901,
     "end_time": "2022-04-11T17:18:19.377315",
     "exception": false,
     "start_time": "2022-04-11T17:18:18.924414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=[None, max_id],\n",
    "                     # dropout=0.2, recurrent_dropout=0.2\n",
    "                     dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True, dropout=0.2),\n",
    "    keras.layers.TimeDistributed(\n",
    "        keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54da25e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T17:18:19.475684Z",
     "iopub.status.busy": "2022-04-11T17:18:19.474835Z",
     "iopub.status.idle": "2022-04-11T19:43:52.847227Z",
     "shell.execute_reply": "2022-04-11T19:43:52.846627Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 8733.426147,
     "end_time": "2022-04-11T19:43:52.847389",
     "exception": false,
     "start_time": "2022-04-11T17:18:19.421242",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-11 17:18:24.544045: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31368/31368 [==============================] - 408s 13ms/step - loss: 1.6206\n",
      "Epoch 2/20\n",
      "31368/31368 [==============================] - 401s 13ms/step - loss: 1.5369\n",
      "Epoch 3/20\n",
      "31368/31368 [==============================] - 402s 13ms/step - loss: 1.5170\n",
      "Epoch 4/20\n",
      "31368/31368 [==============================] - 421s 13ms/step - loss: 1.5057\n",
      "Epoch 5/20\n",
      "31368/31368 [==============================] - 400s 13ms/step - loss: 1.4983\n",
      "Epoch 6/20\n",
      "31368/31368 [==============================] - 408s 13ms/step - loss: 1.4929\n",
      "Epoch 7/20\n",
      "31368/31368 [==============================] - 401s 13ms/step - loss: 1.4893\n",
      "Epoch 8/20\n",
      "31368/31368 [==============================] - 417s 13ms/step - loss: 1.4866\n",
      "Epoch 9/20\n",
      "31368/31368 [==============================] - 401s 13ms/step - loss: 1.4845\n",
      "Epoch 10/20\n",
      "31368/31368 [==============================] - 408s 13ms/step - loss: 1.4822\n",
      "Epoch 11/20\n",
      "31368/31368 [==============================] - 409s 13ms/step - loss: 1.4808\n",
      "Epoch 12/20\n",
      "31368/31368 [==============================] - 414s 13ms/step - loss: 1.4795\n",
      "Epoch 13/20\n",
      "31368/31368 [==============================] - 410s 13ms/step - loss: 1.4788\n",
      "Epoch 14/20\n",
      "31368/31368 [==============================] - 409s 13ms/step - loss: 1.4774\n",
      "Epoch 15/20\n",
      "31368/31368 [==============================] - 423s 13ms/step - loss: 1.4764\n",
      "Epoch 16/20\n",
      "31368/31368 [==============================] - 428s 14ms/step - loss: 1.4756\n",
      "Epoch 17/20\n",
      "31368/31368 [==============================] - 411s 13ms/step - loss: 1.4753\n",
      "Epoch 18/20\n",
      "31368/31368 [==============================] - 401s 13ms/step - loss: 1.4738\n",
      "Epoch 19/20\n",
      "31368/31368 [==============================] - 429s 14ms/step - loss: 1.4733\n",
      "Epoch 20/20\n",
      "31368/31368 [==============================] - 402s 13ms/step - loss: 1.4725\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29524a6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T19:45:24.611422Z",
     "iopub.status.busy": "2022-04-11T19:45:24.610764Z",
     "iopub.status.idle": "2022-04-11T19:45:24.648547Z",
     "shell.execute_reply": "2022-04-11T19:45:24.648062Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 46.125412,
     "end_time": "2022-04-11T19:45:24.648668",
     "exception": false,
     "start_time": "2022-04-11T19:44:38.523256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(\"./models/my_CharRNN_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c41dee9",
   "metadata": {
    "hidden": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./models/my_CharRNN_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 使用Char-RNN模型 Using the Char-RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 首先需要对数据进行预处理\n",
    "    \n",
    "    对文本进行编码，将每个字符都用ID来表示(从0开始), 并使用独热编码."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess(tests):\n",
    "    X = np.array(tokenizer.texts_to_sequences(tests)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 接下来可以使用这个模型,预测文本中的下一个字母."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_new = preprocess([\"I love yo\"])\n",
    "y_pred = np.argmax(model(X_new), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n h o v e   t o u'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印第一句话所有时间窗的最后一个字符\n",
    "tokenizer.sequences_to_texts(y_pred + 1)[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印最后的预测结果\n",
    "tokenizer.sequences_to_texts(y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 生成假莎士比亚文本 Generating Fake Shakespearean Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "要使用`Char-RNN`生成新文本，我们可以给模型输入一些文本，让模型预测出下一个字母，**将字母添加到文本的尾部，再将延长后的文本输入给模型，预测下一个字母**，以此类推。但在实际中，这会导致相同的单词不断重复。\n",
    "\n",
    "- 可以使用`tf.random.categorical()`，**随机挑选下一个字符**，概率等同于`估计概率`。\n",
    "\n",
    "    根据分类的`对数几率logits`，`categorical()`从分类索引中随机抽取样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 2, 1, 1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialization(42)\n",
    "\n",
    "tf.random.categorical(logits=[[np.log(0.1),\n",
    "                               np.log(0.7),\n",
    "                               np.log(0.1),]],\n",
    "                      num_samples=10).numpy()   # 为每个行切片绘制的独立样本数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "为了对生成文本的多样性更可控，我们可以用一个称为`temperature`的可调节的数来除以对数概率：`temperature`接近0，会利于高概率字符，而非常高的`temperature`将给所有字符的相等概率。\n",
    "\n",
    "- 下面的`next_char()`函数使用这个方法，来挑选添加进文本中的字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model(X_new)[0, -1:, :]  # 0: 表示第一段文本\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    char = tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialization(42)\n",
    "\n",
    "next_char(text=\"I love yo\", temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 重复调用`next_char()`, 并测试不同的`temperature`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "initialization(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there shall be worst.\n",
      "\n",
      "tranio:\n",
      "sir, i will come to \n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the matter fair beholding of a scolding of a scoldi\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th reou; prouds.'thom a gint! a play ard face? sftw\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "显然，当`temperature=1`时，我们的莎士比亚模型效果最好。\n",
    "\n",
    "为了生成更有信服力的文字，**可以尝试用更多`GRU层`、每层更多的神经元、更长的训练时间，添加正则**（例如，可以在`GRU层`中设置`recurrent_dropout=0.3`）。另外，模型不能学习长度超过`n_steps`（只有100个字符）的规律。你可以**使用更大的窗口**，但也会让训练更为困难，甚至`LSTM`和`GRU`单元也不能处理长序列。\n",
    "\n",
    "另外，还可以使用`有状态RNN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 有状态RNN  Stateful RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 原理及数据集划分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "在Keras调用`LSTM/GRU`的参数中，有一个`stateful`参数，默认是`False`，也就是`无状态模式stateless`，为`True`的话就是`有状态模式stateful`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    ">- `stateless LSTM`\n",
    ">  \n",
    ">  `stateless`就是与`stateful`相反了，就是sample之间没有前后状态传递的关系，输入samples后，默认就会shuffle，可以说是**每个sample独立**，适合输入一些没有关系的样本。\n",
    "\n",
    "> - `stateful LSTM`\n",
    ">    \n",
    ">    除了正常的单个sample内部时间步之间的状态互相传递外，sample之间的状态还能互相传递。但是需要确定`batch_size`大小，传递的时候是前一个`batch`的第$i$个sample最终输出状态，传递给后一个`batch`的第$i$个sample，作为其状态的初始化值。\n",
    ">     \n",
    ">         解放军第72集团军某旅500名抗洪子弟兵撤离安徽铜陵枞阳县，赶赴合肥庐江县继续抗洪。当地百姓冒着大雨追着驶出的车辆，递上自家的莲蓬、葡萄、熟鸡蛋等食物，送别子弟兵。\n",
    "    第一句话为一个sample，第二句话为第二个sample，每个sample内部的word为一个时间步，很明显前后两个sample是相互关联的。**如果设置`batch_size=1`,则第一句话最终输出状态(即最后一个word的状态输出)，就可以传递给第二句话作为其初始状态值**。\n",
    ">    \n",
    ">   - 优点：后面的语句有了更合理的初始化状态值，显然会加快网络的收敛，所以需要更小的网络、更少的训练时间。\n",
    ">   - 缺点：每一个`epoch`后，要重置一下状态，因为训练一遍了，状态不能循环使用，要从头开始。\n",
    ">   - 使用注意事项：**调用`fit()` 时指定 `shuffle = False`**，要保证sample之间的前后顺序；可以使用`model.reset_states()`来重置模型中所有层的状态，也可使用`layer.reset_states()`来重置指定有状态 RNN 层的状态。\n",
    "\n",
    "\n",
    "\n",
    "> 参考: https://www.cnblogs.com/gczr/p/13393883.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`stateful RNN`只在前一批次的序列离开，后一批次中的对应输入序列开始的情况下才有意义。\n",
    "\n",
    "- 首先,**使用序列且没有重叠的输入序列**（而不是用来训练无状态RNN时的打散和重叠的序列）。\n",
    "    - 当创建`Dataset`时，**调用`window()`必须使用`shift=n_steps`**（而不是`shift=1`）。\n",
    "            例: window_size=31, shfit=30, \n",
    "                train_x=[1,...,31] (31-1=30), train_y=[2,...,31], \n",
    "                next, train_x=[31,...,61], train_y=[32...62],\n",
    "    - 不能使用`shuffle()`方法。但是，准备`stateful RNN`数据集的批处理会麻烦些。事实上，如果调用`batch(32)`，32个连续的窗口会放到一个相同的批处理中，而下一个批处理不会再这些窗口的每个中断处继续。第一个批次含有窗口1到32，第二个批次批次含有窗口33到64，因此每个批次中的第一个窗口（第一批次的窗口1和第二批次的窗口33），它们是不连续的。\n",
    "  \n",
    "  \n",
    "- 最简单办法是**使用只包含单个窗口的\"批处理\"**：\n",
    "   <img src=\"./images/other/15-23.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "initialization(42)\n",
    "\n",
    "n_steps = 100 \n",
    "window_length = n_steps + 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(size=window_length,\n",
    "                         shift=n_steps,\n",
    "                         drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "dataset = dataset.repeat().batch(1)  # 注意不同与前文.batch(batch_size)\n",
    "dataset = dataset.map(lambda window:(window[:, :-1], window[:, 1:]))\n",
    "\n",
    "dataset = dataset.map(lambda X_batch, y_batch: (tf.one_hot(X_batch, max_id), y_batch))\n",
    "\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 39) (1, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in dataset.take(1): \n",
    "    print(X_batch.shape, y_batch.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 使用以下方法也可以**实现批处理**\n",
    "   - 将莎士比亚作品切分成32段等长的文本\n",
    "   - 每个文本创建一个连续输入的数据集\n",
    "   - 最后使用`tf.train.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))`来创建合适的连续批处理，其中批处理中的第$n$个输入序列恰好是上一个批处理中第$n$个输入序列结束位置的开始."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "encoded_parts = np.array_split(ary=encoded[:train_size],\n",
    "                               indices_or_sections=batch_size)\n",
    "# 划分为32段等长的文本\n",
    "# 每段31371个字符, 最后一段为31370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "datasets = []\n",
    "\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(size=window_length,\n",
    "                             shift=n_steps,\n",
    "                             drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window:window.batch(window_length))\n",
    "    datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    encoded_part_size=31371\n",
    "    迭代次数: ( len(text)-window_size ) // shift +1 \n",
    "           = ( 31371 - 101 ) // 100 + 1 = 313\n",
    "    即: 共有313个窗口, 每个窗口大小为101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (32, None), types: tf.int64>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.zip(\n",
    "    tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda window: (window[:, :-1], window[:, 1:]))\n",
    "dataset = dataset.map(lambda X_batch, y_batch:\n",
    "                      (tf.one_hot(X_batch, max_id), y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in dataset.take(1): \n",
    "    print(X_batch.shape, y_batch.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 搭建并训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. 搭建`stateful RNN`模型\n",
    "    - 创建每个循环层时需要**设置`stateful=True`**\n",
    "    - 有状态RNN需要知道批次大小（因为要为批次中的每个输入序列保留一个状态），所以要必须在第一层中设置`batch_input_shape`。\n",
    "        - 注意:可以不用指定第二个维度，因为输入可以有任意长度."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2,),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2. 在每个`epoch`之后, 需要先重设状态, 然后再返回到文本的开头。因此, 我们需要使用回调函数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EarlyStoppingCallback = keras.callbacks.EarlyStopping(patience=10,  # 至多10个轮次\n",
    "                                                      restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "3. 编译并训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be757cd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T11:42:17.862182Z",
     "iopub.status.busy": "2022-04-13T11:42:17.861128Z",
     "iopub.status.idle": "2022-04-13T11:55:49.680288Z",
     "shell.execute_reply": "2022-04-13T11:55:49.681008Z",
     "shell.execute_reply.started": "2022-04-13T11:39:34.952502Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 811.882395,
     "end_time": "2022-04-13T11:55:49.681196",
     "exception": false,
     "start_time": "2022-04-13T11:42:17.798801",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 11:42:22.105494: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 11s 19ms/step - loss: 2.6204\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 2.2427\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 2.1129\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 2.0384\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.9869\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.9511\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.9225\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.8983\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.8815\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.8663\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.8505\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.8422\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.8310\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.8235\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.8156\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.8072\n",
      "Epoch 17/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7996\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7970\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.7899\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 1.7817\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7803\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 1.7766\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 1.7715\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7676\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.7646\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 1.7625\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 6s 21ms/step - loss: 1.7582\n",
      "Epoch 28/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7561\n",
      "Epoch 29/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7526\n",
      "Epoch 30/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 1.7517\n",
      "Epoch 31/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.7480\n",
      "Epoch 32/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7462\n",
      "Epoch 33/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.7417\n",
      "Epoch 34/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 1.7417\n",
      "Epoch 35/100\n",
      "313/313 [==============================] - 7s 24ms/step - loss: 1.7385\n",
      "Epoch 36/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7389\n",
      "Epoch 37/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7354\n",
      "Epoch 38/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.7318\n",
      "Epoch 39/100\n",
      "313/313 [==============================] - 8s 24ms/step - loss: 1.7309\n",
      "Epoch 40/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7306\n",
      "Epoch 41/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7280\n",
      "Epoch 42/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7253\n",
      "Epoch 43/100\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 1.7232\n",
      "Epoch 44/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7220\n",
      "Epoch 45/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 1.7221\n",
      "Epoch 46/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 1.7184\n",
      "Epoch 47/100\n",
      "313/313 [==============================] - 8s 24ms/step - loss: 1.7184\n",
      "Epoch 48/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7181\n",
      "Epoch 49/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7143\n",
      "Epoch 50/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7136\n",
      "Epoch 51/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7141\n",
      "Epoch 52/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7127\n",
      "Epoch 53/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7106\n",
      "Epoch 54/100\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 1.7106\n",
      "Epoch 55/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.7086\n",
      "Epoch 56/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7068\n",
      "Epoch 57/100\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 1.7072\n",
      "Epoch 58/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 1.7053\n",
      "Epoch 59/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7058\n",
      "Epoch 60/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7035\n",
      "Epoch 61/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7038\n",
      "Epoch 62/100\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 1.7006\n",
      "Epoch 63/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7010\n",
      "Epoch 64/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6999\n",
      "Epoch 65/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7000\n",
      "Epoch 66/100\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 1.7000\n",
      "Epoch 67/100\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 1.6990\n",
      "Epoch 68/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6969\n",
      "Epoch 69/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6970\n",
      "Epoch 70/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6972\n",
      "Epoch 71/100\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 1.6970\n",
      "Epoch 72/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.6953\n",
      "Epoch 73/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6944\n",
      "Epoch 74/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6915\n",
      "Epoch 75/100\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 1.6921\n",
      "Epoch 76/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6929\n",
      "Epoch 77/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6912\n",
      "Epoch 78/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6903\n",
      "Epoch 79/100\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 1.6908\n",
      "Epoch 80/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6893\n",
      "Epoch 81/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6873\n",
      "Epoch 82/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6863\n",
      "Epoch 83/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 1.6857\n",
      "Epoch 84/100\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 1.6873\n",
      "Epoch 85/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6861\n",
      "Epoch 86/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6845\n",
      "Epoch 87/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6844\n",
      "Epoch 88/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6835\n",
      "Epoch 89/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6838\n",
      "Epoch 90/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6839\n",
      "Epoch 91/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6822\n",
      "Epoch 92/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 1.6831\n",
      "Epoch 93/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6822\n",
      "Epoch 94/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.6807\n",
      "Epoch 95/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6805\n",
      "Epoch 96/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6807\n",
      "Epoch 97/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6796\n",
      "Epoch 98/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6784\n",
      "Epoch 99/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6806\n",
      "Epoch 100/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.6788\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, epochs=100,\n",
    "                    callbacks=[ResetStatesCallback(), EarlyStoppingCallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23c27573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T11:55:59.242152Z",
     "iopub.status.busy": "2022-04-13T11:55:59.234291Z",
     "iopub.status.idle": "2022-04-13T11:55:59.280227Z",
     "shell.execute_reply": "2022-04-13T11:55:59.279584Z",
     "shell.execute_reply.started": "2022-04-13T11:41:23.207726Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 4.884947,
     "end_time": "2022-04-13T11:55:59.280376",
     "exception": false,
     "start_time": "2022-04-13T11:55:54.395429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(\"./models/my_stateful_RNN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52982d7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T11:56:19.558930Z",
     "iopub.status.busy": "2022-04-13T11:56:19.557636Z",
     "iopub.status.idle": "2022-04-13T11:56:19.559948Z",
     "shell.execute_reply": "2022-04-13T11:56:19.560662Z",
     "shell.execute_reply.started": "2022-04-13T11:41:23.212345Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 5.669497,
     "end_time": "2022-04-13T11:56:19.560842",
     "exception": false,
     "start_time": "2022-04-13T11:56:13.891345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "train_loss = history_dict[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e15c963",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T11:56:29.083193Z",
     "iopub.status.busy": "2022-04-13T11:56:29.081980Z",
     "iopub.status.idle": "2022-04-13T11:56:29.345698Z",
     "shell.execute_reply": "2022-04-13T11:56:29.345160Z",
     "shell.execute_reply.started": "2022-04-13T11:41:23.214640Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 5.225427,
     "end_time": "2022-04-13T11:56:29.345866",
     "exception": false,
     "start_time": "2022-04-13T11:56:24.120439",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 100\n",
    "# figure 1\n",
    "plt.figure()\n",
    "plt.plot(range(epochs), train_loss, label='train_loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "4. 创建无状态副本模型\n",
    "\n",
    "    训练此模型后，只能使用它来预测**与训练期间使用的大小相同的批次**。为避免此限制，要**创建一个相同的无状态模型，并将有状态模型的权重复制到此模型**。 \n",
    "    \n",
    "    我们可以摆脱 dropout，因为它只在训练期间使用."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47626182",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T11:56:08.806325Z",
     "iopub.status.busy": "2022-04-13T11:56:08.805171Z",
     "iopub.status.idle": "2022-04-13T11:56:09.335240Z",
     "shell.execute_reply": "2022-04-13T11:56:09.334342Z",
     "shell.execute_reply.started": "2022-04-13T11:41:23.210025Z"
    },
    "hidden": true,
    "papermill": {
     "duration": 5.491895,
     "end_time": "2022-04-13T11:56:09.335385",
     "exception": false,
     "start_time": "2022-04-13T11:56:03.843490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./models/my_stateful_RNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    要设置权重，我们首先需要构建模型."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(\n",
    "        keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stateless_model.build(tf.TensorShape([None, None, max_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stateless_model.set_weights(model.get_weights())\n",
    "model = stateless_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "5. 测试字符级模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess(tests):\n",
    "    X = np.array(tokenizer.texts_to_sequences(tests)) - 1\n",
    "    return tf.one_hot(X, max_id)\n",
    "\n",
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model(X_new)[0, -1:, :]  # 0: 表示第一段文本\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    char = tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "    return char\n",
    "\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "initialization(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took;\n",
      "mist look'd ell help hot limisuadel.\n",
      "why do d\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情感分析 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`IMDb影评数据集`包含50000条英文影评，25000条用于训练，25000条用于测试，是从IMDb网站提取的，并带有影评标签，负（0）或正（1）。\n",
    "\n",
    "- 使用`keras`加载`IMDb影评数据集`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialization(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_text, y_text) = keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据集已经经过预处理:\n",
    "    1.  `X_train`由一个评论的列表构成，每条评论都是整数NumPy数组，其中每个整数代表一个词。\n",
    "    2. 所有标点符号都被删除， 然后将单词转换为小写字母，用空格隔开，最后用频次建立索引。\n",
    "    3. 整数0、1、2是特殊的\n",
    "        -  0:\\<pad>表示`填充token`\n",
    "        -  1:\\<sos>表示`序列开始(SSS)token`\n",
    "        -  2:\\<unk>表示`未知单词`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可视化评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1),\n",
       " ('and', 2),\n",
       " ('a', 3),\n",
       " ('of', 4),\n",
       " ('to', 5),\n",
       " ('is', 6),\n",
       " ('br', 7),\n",
       " ('in', 8),\n",
       " ('it', 9),\n",
       " ('i', 10)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()  # len(word_index):88584\n",
    "sorted(word_index.items(), key=lambda x: x[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{34704: 'fawn',\n",
       " 52009: 'tsukino',\n",
       " 52010: 'nunnery',\n",
       " 16819: 'sonja',\n",
       " 63954: 'vani',\n",
       " 1411: 'woods',\n",
       " 16118: 'spiders',\n",
       " 2348: 'hanging',\n",
       " 2292: 'woody',\n",
       " 52011: 'trawling',\n",
       " 52012: \"hold's\",\n",
       " 11310: 'comically',\n",
       " 40833: 'localized',\n",
       " 30571: 'disobeying',\n",
       " 52013: \"'royale\",\n",
       " 40834: \"harpo's\",\n",
       " 52014: 'canet',\n",
       " 19316: 'aileen',\n",
       " 52015: 'acurately',\n",
       " 52016: \"diplomat's\",\n",
       " 25245: 'rickman',\n",
       " 6749: 'arranged',\n",
       " 52017: 'rumbustious',\n",
       " 52018: 'familiarness',\n",
       " 52019: \"spider'\",\n",
       " 68807: 'hahahah',\n",
       " 52020: \"wood'\",\n",
       " 40836: 'transvestism',\n",
       " 34705: \"hangin'\",\n",
       " 2341: 'bringing',\n",
       " 40837: 'seamier',\n",
       " 34706: 'wooded',\n",
       " 52021: 'bravora',\n",
       " 16820: 'grueling',\n",
       " 1639: 'wooden',\n",
       " 16821: 'wednesday',\n",
       " 52022: \"'prix\",\n",
       " 34707: 'altagracia',\n",
       " 52023: 'circuitry',\n",
       " 11588: 'crotch',\n",
       " 57769: 'busybody',\n",
       " 52024: \"tart'n'tangy\",\n",
       " 14132: 'burgade',\n",
       " 52026: 'thrace',\n",
       " 11041: \"tom's\",\n",
       " 52028: 'snuggles',\n",
       " 29117: 'francesco',\n",
       " 52030: 'complainers',\n",
       " 52128: 'templarios',\n",
       " 40838: '272',\n",
       " 52031: '273',\n",
       " 52133: 'zaniacs',\n",
       " 34709: '275',\n",
       " 27634: 'consenting',\n",
       " 40839: 'snuggled',\n",
       " 15495: 'inanimate',\n",
       " 52033: 'uality',\n",
       " 11929: 'bronte',\n",
       " 4013: 'errors',\n",
       " 3233: 'dialogs',\n",
       " 52034: \"yomada's\",\n",
       " 34710: \"madman's\",\n",
       " 30588: 'dialoge',\n",
       " 52036: 'usenet',\n",
       " 40840: 'videodrome',\n",
       " 26341: \"kid'\",\n",
       " 52037: 'pawed',\n",
       " 30572: \"'girlfriend'\",\n",
       " 52038: \"'pleasure\",\n",
       " 52039: \"'reloaded'\",\n",
       " 40842: \"kazakos'\",\n",
       " 52040: 'rocque',\n",
       " 52041: 'mailings',\n",
       " 11930: 'brainwashed',\n",
       " 16822: 'mcanally',\n",
       " 52042: \"tom''\",\n",
       " 25246: 'kurupt',\n",
       " 21908: 'affiliated',\n",
       " 52043: 'babaganoosh',\n",
       " 40843: \"noe's\",\n",
       " 40844: 'quart',\n",
       " 362: 'kids',\n",
       " 5037: 'uplifting',\n",
       " 7096: 'controversy',\n",
       " 21909: 'kida',\n",
       " 23382: 'kidd',\n",
       " 52044: \"error'\",\n",
       " 52045: 'neurologist',\n",
       " 18513: 'spotty',\n",
       " 30573: 'cobblers',\n",
       " 9881: 'projection',\n",
       " 40845: 'fastforwarding',\n",
       " 52046: 'sters',\n",
       " 52047: \"eggar's\",\n",
       " 52048: 'etherything',\n",
       " 40846: 'gateshead',\n",
       " 34711: 'airball',\n",
       " 25247: 'unsinkable',\n",
       " 7183: 'stern',\n",
       " 52049: \"cervi's\",\n",
       " 40847: 'dnd',\n",
       " 11589: 'dna',\n",
       " 20601: 'insecurity',\n",
       " 52050: \"'reboot'\",\n",
       " 11040: 'trelkovsky',\n",
       " 52051: 'jaekel',\n",
       " 52052: 'sidebars',\n",
       " 52053: \"sforza's\",\n",
       " 17636: 'distortions',\n",
       " 52054: 'mutinies',\n",
       " 30605: 'sermons',\n",
       " 40849: '7ft',\n",
       " 52055: 'boobage',\n",
       " 52056: \"o'bannon's\",\n",
       " 23383: 'populations',\n",
       " 52057: 'chulak',\n",
       " 27636: 'mesmerize',\n",
       " 52058: 'quinnell',\n",
       " 10310: 'yahoo',\n",
       " 52060: 'meteorologist',\n",
       " 42580: 'beswick',\n",
       " 15496: 'boorman',\n",
       " 40850: 'voicework',\n",
       " 52061: \"ster'\",\n",
       " 22925: 'blustering',\n",
       " 52062: 'hj',\n",
       " 27637: 'intake',\n",
       " 5624: 'morally',\n",
       " 40852: 'jumbling',\n",
       " 52063: 'bowersock',\n",
       " 52064: \"'porky's'\",\n",
       " 16824: 'gershon',\n",
       " 40853: 'ludicrosity',\n",
       " 52065: 'coprophilia',\n",
       " 40854: 'expressively',\n",
       " 19503: \"india's\",\n",
       " 34713: \"post's\",\n",
       " 52066: 'wana',\n",
       " 5286: 'wang',\n",
       " 30574: 'wand',\n",
       " 25248: 'wane',\n",
       " 52324: 'edgeways',\n",
       " 34714: 'titanium',\n",
       " 40855: 'pinta',\n",
       " 181: 'want',\n",
       " 30575: 'pinto',\n",
       " 52068: 'whoopdedoodles',\n",
       " 21911: 'tchaikovsky',\n",
       " 2106: 'travel',\n",
       " 52069: \"'victory'\",\n",
       " 11931: 'copious',\n",
       " 22436: 'gouge',\n",
       " 52070: \"chapters'\",\n",
       " 6705: 'barbra',\n",
       " 30576: 'uselessness',\n",
       " 52071: \"wan'\",\n",
       " 27638: 'assimilated',\n",
       " 16119: 'petiot',\n",
       " 52072: 'most\\x85and',\n",
       " 3933: 'dinosaurs',\n",
       " 355: 'wrong',\n",
       " 52073: 'seda',\n",
       " 52074: 'stollen',\n",
       " 34715: 'sentencing',\n",
       " 40856: 'ouroboros',\n",
       " 40857: 'assimilates',\n",
       " 40858: 'colorfully',\n",
       " 27639: 'glenne',\n",
       " 52075: 'dongen',\n",
       " 4763: 'subplots',\n",
       " 52076: 'kiloton',\n",
       " 23384: 'chandon',\n",
       " 34716: \"effect'\",\n",
       " 27640: 'snugly',\n",
       " 40859: 'kuei',\n",
       " 9095: 'welcomed',\n",
       " 30074: 'dishonor',\n",
       " 52078: 'concurrence',\n",
       " 23385: 'stoicism',\n",
       " 14899: \"guys'\",\n",
       " 52080: \"beroemd'\",\n",
       " 6706: 'butcher',\n",
       " 40860: \"melfi's\",\n",
       " 30626: 'aargh',\n",
       " 20602: 'playhouse',\n",
       " 11311: 'wickedly',\n",
       " 1183: 'fit',\n",
       " 52081: 'labratory',\n",
       " 40862: 'lifeline',\n",
       " 1930: 'screaming',\n",
       " 4290: 'fix',\n",
       " 52082: 'cineliterate',\n",
       " 52083: 'fic',\n",
       " 52084: 'fia',\n",
       " 34717: 'fig',\n",
       " 52085: 'fmvs',\n",
       " 52086: 'fie',\n",
       " 52087: 'reentered',\n",
       " 30577: 'fin',\n",
       " 52088: 'doctresses',\n",
       " 52089: 'fil',\n",
       " 12609: 'zucker',\n",
       " 31934: 'ached',\n",
       " 52091: 'counsil',\n",
       " 52092: 'paterfamilias',\n",
       " 13888: 'songwriter',\n",
       " 34718: 'shivam',\n",
       " 9657: 'hurting',\n",
       " 302: 'effects',\n",
       " 52093: 'slauther',\n",
       " 52094: \"'flame'\",\n",
       " 52095: 'sommerset',\n",
       " 52096: 'interwhined',\n",
       " 27641: 'whacking',\n",
       " 52097: 'bartok',\n",
       " 8778: 'barton',\n",
       " 21912: 'frewer',\n",
       " 52098: \"fi'\",\n",
       " 6195: 'ingrid',\n",
       " 30578: 'stribor',\n",
       " 52099: 'approporiately',\n",
       " 52100: 'wobblyhand',\n",
       " 52101: 'tantalisingly',\n",
       " 52102: 'ankylosaurus',\n",
       " 17637: 'parasites',\n",
       " 52103: 'childen',\n",
       " 52104: \"jenkins'\",\n",
       " 52105: 'metafiction',\n",
       " 17638: 'golem',\n",
       " 40863: 'indiscretion',\n",
       " 23386: \"reeves'\",\n",
       " 57784: \"inamorata's\",\n",
       " 52107: 'brittannica',\n",
       " 7919: 'adapt',\n",
       " 30579: \"russo's\",\n",
       " 48249: 'guitarists',\n",
       " 10556: 'abbott',\n",
       " 40864: 'abbots',\n",
       " 17652: 'lanisha',\n",
       " 40866: 'magickal',\n",
       " 52108: 'mattter',\n",
       " 52109: \"'willy\",\n",
       " 34719: 'pumpkins',\n",
       " 52110: 'stuntpeople',\n",
       " 30580: 'estimate',\n",
       " 40867: 'ugghhh',\n",
       " 11312: 'gameplay',\n",
       " 52111: \"wern't\",\n",
       " 40868: \"n'sync\",\n",
       " 16120: 'sickeningly',\n",
       " 40869: 'chiara',\n",
       " 4014: 'disturbed',\n",
       " 40870: 'portmanteau',\n",
       " 52112: 'ineffectively',\n",
       " 82146: \"duchonvey's\",\n",
       " 37522: \"nasty'\",\n",
       " 1288: 'purpose',\n",
       " 52115: 'lazers',\n",
       " 28108: 'lightened',\n",
       " 52116: 'kaliganj',\n",
       " 52117: 'popularism',\n",
       " 18514: \"damme's\",\n",
       " 30581: 'stylistics',\n",
       " 52118: 'mindgaming',\n",
       " 46452: 'spoilerish',\n",
       " 52120: \"'corny'\",\n",
       " 34721: 'boerner',\n",
       " 6795: 'olds',\n",
       " 52121: 'bakelite',\n",
       " 27642: 'renovated',\n",
       " 27643: 'forrester',\n",
       " 52122: \"lumiere's\",\n",
       " 52027: 'gaskets',\n",
       " 887: 'needed',\n",
       " 34722: 'smight',\n",
       " 1300: 'master',\n",
       " 25908: \"edie's\",\n",
       " 40871: 'seeber',\n",
       " 52123: 'hiya',\n",
       " 52124: 'fuzziness',\n",
       " 14900: 'genesis',\n",
       " 12610: 'rewards',\n",
       " 30582: 'enthrall',\n",
       " 40872: \"'about\",\n",
       " 52125: \"recollection's\",\n",
       " 11042: 'mutilated',\n",
       " 52126: 'fatherlands',\n",
       " 52127: \"fischer's\",\n",
       " 5402: 'positively',\n",
       " 34708: '270',\n",
       " 34723: 'ahmed',\n",
       " 9839: 'zatoichi',\n",
       " 13889: 'bannister',\n",
       " 52130: 'anniversaries',\n",
       " 30583: \"helm's\",\n",
       " 52131: \"'work'\",\n",
       " 34724: 'exclaimed',\n",
       " 52132: \"'unfunny'\",\n",
       " 52032: '274',\n",
       " 547: 'feeling',\n",
       " 52134: \"wanda's\",\n",
       " 33269: 'dolan',\n",
       " 52136: '278',\n",
       " 52137: 'peacoat',\n",
       " 40873: 'brawny',\n",
       " 40874: 'mishra',\n",
       " 40875: 'worlders',\n",
       " 52138: 'protags',\n",
       " 52139: 'skullcap',\n",
       " 57599: 'dastagir',\n",
       " 5625: 'affairs',\n",
       " 7802: 'wholesome',\n",
       " 52140: 'hymen',\n",
       " 25249: 'paramedics',\n",
       " 52141: 'unpersons',\n",
       " 52142: 'heavyarms',\n",
       " 52143: 'affaire',\n",
       " 52144: 'coulisses',\n",
       " 40876: 'hymer',\n",
       " 52145: 'kremlin',\n",
       " 30584: 'shipments',\n",
       " 52146: 'pixilated',\n",
       " 30585: \"'00s\",\n",
       " 18515: 'diminishing',\n",
       " 1360: 'cinematic',\n",
       " 14901: 'resonates',\n",
       " 40877: 'simplify',\n",
       " 40878: \"nature'\",\n",
       " 40879: 'temptresses',\n",
       " 16825: 'reverence',\n",
       " 19505: 'resonated',\n",
       " 34725: 'dailey',\n",
       " 52147: '2\\x85',\n",
       " 27644: 'treize',\n",
       " 52148: 'majo',\n",
       " 21913: 'kiya',\n",
       " 52149: 'woolnough',\n",
       " 39800: 'thanatos',\n",
       " 35734: 'sandoval',\n",
       " 40882: 'dorama',\n",
       " 52150: \"o'shaughnessy\",\n",
       " 4991: 'tech',\n",
       " 32021: 'fugitives',\n",
       " 30586: 'teck',\n",
       " 76128: \"'e'\",\n",
       " 40884: 'doesn’t',\n",
       " 52152: 'purged',\n",
       " 660: 'saying',\n",
       " 41098: \"martians'\",\n",
       " 23421: 'norliss',\n",
       " 27645: 'dickey',\n",
       " 52155: 'dicker',\n",
       " 52156: \"'sependipity\",\n",
       " 8425: 'padded',\n",
       " 57795: 'ordell',\n",
       " 40885: \"sturges'\",\n",
       " 52157: 'independentcritics',\n",
       " 5748: 'tempted',\n",
       " 34727: \"atkinson's\",\n",
       " 25250: 'hounded',\n",
       " 52158: 'apace',\n",
       " 15497: 'clicked',\n",
       " 30587: \"'humor'\",\n",
       " 17180: \"martino's\",\n",
       " 52159: \"'supporting\",\n",
       " 52035: 'warmongering',\n",
       " 34728: \"zemeckis's\",\n",
       " 21914: 'lube',\n",
       " 52160: 'shocky',\n",
       " 7479: 'plate',\n",
       " 40886: 'plata',\n",
       " 40887: 'sturgess',\n",
       " 40888: \"nerds'\",\n",
       " 20603: 'plato',\n",
       " 34729: 'plath',\n",
       " 40889: 'platt',\n",
       " 52162: 'mcnab',\n",
       " 27646: 'clumsiness',\n",
       " 3902: 'altogether',\n",
       " 42587: 'massacring',\n",
       " 52163: 'bicenntinial',\n",
       " 40890: 'skaal',\n",
       " 14363: 'droning',\n",
       " 8779: 'lds',\n",
       " 21915: 'jaguar',\n",
       " 34730: \"cale's\",\n",
       " 1780: 'nicely',\n",
       " 4591: 'mummy',\n",
       " 18516: \"lot's\",\n",
       " 10089: 'patch',\n",
       " 50205: 'kerkhof',\n",
       " 52164: \"leader's\",\n",
       " 27647: \"'movie\",\n",
       " 52165: 'uncomfirmed',\n",
       " 40891: 'heirloom',\n",
       " 47363: 'wrangle',\n",
       " 52166: 'emotion\\x85',\n",
       " 52167: \"'stargate'\",\n",
       " 40892: 'pinoy',\n",
       " 40893: 'conchatta',\n",
       " 41131: 'broeke',\n",
       " 40894: 'advisedly',\n",
       " 17639: \"barker's\",\n",
       " 52169: 'descours',\n",
       " 775: 'lots',\n",
       " 9262: 'lotr',\n",
       " 9882: 'irs',\n",
       " 52170: 'lott',\n",
       " 40895: 'xvi',\n",
       " 34731: 'irk',\n",
       " 52171: 'irl',\n",
       " 6890: 'ira',\n",
       " 21916: 'belzer',\n",
       " 52172: 'irc',\n",
       " 27648: 'ire',\n",
       " 40896: 'requisites',\n",
       " 7696: 'discipline',\n",
       " 52964: 'lyoko',\n",
       " 11313: 'extend',\n",
       " 876: 'nature',\n",
       " 52173: \"'dickie'\",\n",
       " 40897: 'optimist',\n",
       " 30589: 'lapping',\n",
       " 3903: 'superficial',\n",
       " 52174: 'vestment',\n",
       " 2826: 'extent',\n",
       " 52175: 'tendons',\n",
       " 52176: \"heller's\",\n",
       " 52177: 'quagmires',\n",
       " 52178: 'miyako',\n",
       " 20604: 'moocow',\n",
       " 52179: \"coles'\",\n",
       " 40898: 'lookit',\n",
       " 52180: 'ravenously',\n",
       " 40899: 'levitating',\n",
       " 52181: 'perfunctorily',\n",
       " 30590: 'lookin',\n",
       " 40901: \"lot'\",\n",
       " 52182: 'lookie',\n",
       " 34873: 'fearlessly',\n",
       " 52184: 'libyan',\n",
       " 40902: 'fondles',\n",
       " 35717: 'gopher',\n",
       " 40904: 'wearying',\n",
       " 52185: \"nz's\",\n",
       " 27649: 'minuses',\n",
       " 52186: 'puposelessly',\n",
       " 52187: 'shandling',\n",
       " 31271: 'decapitates',\n",
       " 11932: 'humming',\n",
       " 40905: \"'nother\",\n",
       " 21917: 'smackdown',\n",
       " 30591: 'underdone',\n",
       " 40906: 'frf',\n",
       " 52188: 'triviality',\n",
       " 25251: 'fro',\n",
       " 8780: 'bothers',\n",
       " 52189: \"'kensington\",\n",
       " 76: 'much',\n",
       " 34733: 'muco',\n",
       " 22618: 'wiseguy',\n",
       " 27651: \"richie's\",\n",
       " 40907: 'tonino',\n",
       " 52190: 'unleavened',\n",
       " 11590: 'fry',\n",
       " 40908: \"'tv'\",\n",
       " 40909: 'toning',\n",
       " 14364: 'obese',\n",
       " 30592: 'sensationalized',\n",
       " 40910: 'spiv',\n",
       " 6262: 'spit',\n",
       " 7367: 'arkin',\n",
       " 21918: 'charleton',\n",
       " 16826: 'jeon',\n",
       " 21919: 'boardroom',\n",
       " 4992: 'doubts',\n",
       " 3087: 'spin',\n",
       " 53086: 'hepo',\n",
       " 27652: 'wildcat',\n",
       " 10587: 'venoms',\n",
       " 52194: 'misconstrues',\n",
       " 18517: 'mesmerising',\n",
       " 40911: 'misconstrued',\n",
       " 52195: 'rescinds',\n",
       " 52196: 'prostrate',\n",
       " 40912: 'majid',\n",
       " 16482: 'climbed',\n",
       " 34734: 'canoeing',\n",
       " 52198: 'majin',\n",
       " 57807: 'animie',\n",
       " 40913: 'sylke',\n",
       " 14902: 'conditioned',\n",
       " 40914: 'waddell',\n",
       " 52199: '3\\x85',\n",
       " 41191: 'hyperdrive',\n",
       " 34735: 'conditioner',\n",
       " 53156: 'bricklayer',\n",
       " 2579: 'hong',\n",
       " 52201: 'memoriam',\n",
       " 30595: 'inventively',\n",
       " 25252: \"levant's\",\n",
       " 20641: 'portobello',\n",
       " 52203: 'remand',\n",
       " 19507: 'mummified',\n",
       " 27653: 'honk',\n",
       " 19508: 'spews',\n",
       " 40915: 'visitations',\n",
       " 52204: 'mummifies',\n",
       " 25253: 'cavanaugh',\n",
       " 23388: 'zeon',\n",
       " 40916: \"jungle's\",\n",
       " 34736: 'viertel',\n",
       " 27654: 'frenchmen',\n",
       " 52205: 'torpedoes',\n",
       " 52206: 'schlessinger',\n",
       " 34737: 'torpedoed',\n",
       " 69879: 'blister',\n",
       " 52207: 'cinefest',\n",
       " 34738: 'furlough',\n",
       " 52208: 'mainsequence',\n",
       " 40917: 'mentors',\n",
       " 9097: 'academic',\n",
       " 20605: 'stillness',\n",
       " 40918: 'academia',\n",
       " 52209: 'lonelier',\n",
       " 52210: 'nibby',\n",
       " 52211: \"losers'\",\n",
       " 40919: 'cineastes',\n",
       " 4452: 'corporate',\n",
       " 40920: 'massaging',\n",
       " 30596: 'bellow',\n",
       " 19509: 'absurdities',\n",
       " 53244: 'expetations',\n",
       " 40921: 'nyfiken',\n",
       " 75641: 'mehras',\n",
       " 52212: 'lasse',\n",
       " 52213: 'visability',\n",
       " 33949: 'militarily',\n",
       " 52214: \"elder'\",\n",
       " 19026: 'gainsbourg',\n",
       " 20606: 'hah',\n",
       " 13423: 'hai',\n",
       " 34739: 'haj',\n",
       " 25254: 'hak',\n",
       " 4314: 'hal',\n",
       " 4895: 'ham',\n",
       " 53262: 'duffer',\n",
       " 52216: 'haa',\n",
       " 69: 'had',\n",
       " 11933: 'advancement',\n",
       " 16828: 'hag',\n",
       " 25255: \"hand'\",\n",
       " 13424: 'hay',\n",
       " 20607: 'mcnamara',\n",
       " 52217: \"mozart's\",\n",
       " 30734: 'duffel',\n",
       " 30597: 'haq',\n",
       " 13890: 'har',\n",
       " 47: 'has',\n",
       " 2404: 'hat',\n",
       " 40922: 'hav',\n",
       " 30598: 'haw',\n",
       " 52218: 'figtings',\n",
       " 15498: 'elders',\n",
       " 52219: 'underpanted',\n",
       " 52220: 'pninson',\n",
       " 27655: 'unequivocally',\n",
       " 23676: \"barbara's\",\n",
       " 52222: \"bello'\",\n",
       " 13000: 'indicative',\n",
       " 40923: 'yawnfest',\n",
       " 52223: 'hexploitation',\n",
       " 52224: \"loder's\",\n",
       " 27656: 'sleuthing',\n",
       " 32625: \"justin's\",\n",
       " 52225: \"'ball\",\n",
       " 52226: \"'summer\",\n",
       " 34938: \"'demons'\",\n",
       " 52228: \"mormon's\",\n",
       " 34740: \"laughton's\",\n",
       " 52229: 'debell',\n",
       " 39727: 'shipyard',\n",
       " 30600: 'unabashedly',\n",
       " 40404: 'disks',\n",
       " 2293: 'crowd',\n",
       " 10090: 'crowe',\n",
       " 56437: \"vancouver's\",\n",
       " 34741: 'mosques',\n",
       " 6630: 'crown',\n",
       " 52230: 'culpas',\n",
       " 27657: 'crows',\n",
       " 53347: 'surrell',\n",
       " 52232: 'flowless',\n",
       " 52233: 'sheirk',\n",
       " 40926: \"'three\",\n",
       " 52234: \"peterson'\",\n",
       " 52235: 'ooverall',\n",
       " 40927: 'perchance',\n",
       " 1324: 'bottom',\n",
       " 53366: 'chabert',\n",
       " 52236: 'sneha',\n",
       " 13891: 'inhuman',\n",
       " 52237: 'ichii',\n",
       " 52238: 'ursla',\n",
       " 30601: 'completly',\n",
       " 40928: 'moviedom',\n",
       " 52239: 'raddick',\n",
       " 51998: 'brundage',\n",
       " 40929: 'brigades',\n",
       " 1184: 'starring',\n",
       " 52240: \"'goal'\",\n",
       " 52241: 'caskets',\n",
       " 52242: 'willcock',\n",
       " 52243: \"threesome's\",\n",
       " 52244: \"mosque'\",\n",
       " 52245: \"cover's\",\n",
       " 17640: 'spaceships',\n",
       " 40930: 'anomalous',\n",
       " 27658: 'ptsd',\n",
       " 52246: 'shirdan',\n",
       " 21965: 'obscenity',\n",
       " 30602: 'lemmings',\n",
       " 30603: 'duccio',\n",
       " 52247: \"levene's\",\n",
       " 52248: \"'gorby'\",\n",
       " 25258: \"teenager's\",\n",
       " 5343: 'marshall',\n",
       " 9098: 'honeymoon',\n",
       " 3234: 'shoots',\n",
       " 12261: 'despised',\n",
       " 52249: 'okabasho',\n",
       " 8292: 'fabric',\n",
       " 18518: 'cannavale',\n",
       " 3540: 'raped',\n",
       " 52250: \"tutt's\",\n",
       " 17641: 'grasping',\n",
       " 18519: 'despises',\n",
       " 40931: \"thief's\",\n",
       " 8929: 'rapes',\n",
       " 52251: 'raper',\n",
       " 27659: \"eyre'\",\n",
       " 52252: 'walchek',\n",
       " 23389: \"elmo's\",\n",
       " 40932: 'perfumes',\n",
       " 21921: 'spurting',\n",
       " 52253: \"exposition'\\x85\",\n",
       " 52254: 'denoting',\n",
       " 34743: 'thesaurus',\n",
       " 40933: \"shoot'\",\n",
       " 49762: 'bonejack',\n",
       " 52256: 'simpsonian',\n",
       " 30604: 'hebetude',\n",
       " 34744: \"hallow's\",\n",
       " 52257: 'desperation\\x85',\n",
       " 34745: 'incinerator',\n",
       " 10311: 'congratulations',\n",
       " 52258: 'humbled',\n",
       " 5927: \"else's\",\n",
       " 40848: 'trelkovski',\n",
       " 52259: \"rape'\",\n",
       " 59389: \"'chapters'\",\n",
       " 52260: '1600s',\n",
       " 7256: 'martian',\n",
       " 25259: 'nicest',\n",
       " 52262: 'eyred',\n",
       " 9460: 'passenger',\n",
       " 6044: 'disgrace',\n",
       " 52263: 'moderne',\n",
       " 5123: 'barrymore',\n",
       " 52264: 'yankovich',\n",
       " 40934: 'moderns',\n",
       " 52265: 'studliest',\n",
       " 52266: 'bedsheet',\n",
       " 14903: 'decapitation',\n",
       " 52267: 'slurring',\n",
       " 52268: \"'nunsploitation'\",\n",
       " 34746: \"'character'\",\n",
       " 9883: 'cambodia',\n",
       " 52269: 'rebelious',\n",
       " 27660: 'pasadena',\n",
       " 40935: 'crowne',\n",
       " 52270: \"'bedchamber\",\n",
       " 52271: 'conjectural',\n",
       " 52272: 'appologize',\n",
       " 52273: 'halfassing',\n",
       " 57819: 'paycheque',\n",
       " 20609: 'palms',\n",
       " 52274: \"'islands\",\n",
       " 40936: 'hawked',\n",
       " 21922: 'palme',\n",
       " 40937: 'conservatively',\n",
       " 64010: 'larp',\n",
       " 5561: 'palma',\n",
       " 21923: 'smelling',\n",
       " 13001: 'aragorn',\n",
       " 52275: 'hawker',\n",
       " 52276: 'hawkes',\n",
       " 3978: 'explosions',\n",
       " 8062: 'loren',\n",
       " 52277: \"pyle's\",\n",
       " 6707: 'shootout',\n",
       " 18520: \"mike's\",\n",
       " 52278: \"driscoll's\",\n",
       " 40938: 'cogsworth',\n",
       " 52279: \"britian's\",\n",
       " 34747: 'childs',\n",
       " 52280: \"portrait's\",\n",
       " 3629: 'chain',\n",
       " 2500: 'whoever',\n",
       " 52281: 'puttered',\n",
       " 52282: 'childe',\n",
       " 52283: 'maywether',\n",
       " 3039: 'chair',\n",
       " 52284: \"rance's\",\n",
       " 34748: 'machu',\n",
       " 4520: 'ballet',\n",
       " 34749: 'grapples',\n",
       " 76155: 'summerize',\n",
       " 30606: 'freelance',\n",
       " 52286: \"andrea's\",\n",
       " 52287: '\\x91very',\n",
       " 45882: 'coolidge',\n",
       " 18521: 'mache',\n",
       " 52288: 'balled',\n",
       " 40940: 'grappled',\n",
       " 18522: 'macha',\n",
       " 21924: 'underlining',\n",
       " 5626: 'macho',\n",
       " 19510: 'oversight',\n",
       " 25260: 'machi',\n",
       " 11314: 'verbally',\n",
       " 21925: 'tenacious',\n",
       " 40941: 'windshields',\n",
       " 18560: 'paychecks',\n",
       " 3399: 'jerk',\n",
       " 11934: \"good'\",\n",
       " 34751: 'prancer',\n",
       " 21926: 'prances',\n",
       " 52289: 'olympus',\n",
       " 21927: 'lark',\n",
       " 10788: 'embark',\n",
       " 7368: 'gloomy',\n",
       " 52290: 'jehaan',\n",
       " 52291: 'turaqui',\n",
       " 20610: \"child'\",\n",
       " 2897: 'locked',\n",
       " 52292: 'pranced',\n",
       " 2591: 'exact',\n",
       " 52293: 'unattuned',\n",
       " 786: 'minute',\n",
       " 16121: 'skewed',\n",
       " 40943: 'hodgins',\n",
       " 34752: 'skewer',\n",
       " 52294: 'think\\x85',\n",
       " 38768: 'rosenstein',\n",
       " 52295: 'helmit',\n",
       " 34753: 'wrestlemanias',\n",
       " 16829: 'hindered',\n",
       " 30607: \"martha's\",\n",
       " 52296: 'cheree',\n",
       " 52297: \"pluckin'\",\n",
       " 40944: 'ogles',\n",
       " 11935: 'heavyweight',\n",
       " 82193: 'aada',\n",
       " 11315: 'chopping',\n",
       " 61537: 'strongboy',\n",
       " 41345: 'hegemonic',\n",
       " 40945: 'adorns',\n",
       " 41349: 'xxth',\n",
       " 34754: 'nobuhiro',\n",
       " 52301: 'capitães',\n",
       " 52302: 'kavogianni',\n",
       " 13425: 'antwerp',\n",
       " 6541: 'celebrated',\n",
       " 52303: 'roarke',\n",
       " 40946: 'baggins',\n",
       " 31273: 'cheeseburgers',\n",
       " 52304: 'matras',\n",
       " 52305: \"nineties'\",\n",
       " 52306: \"'craig'\",\n",
       " 13002: 'celebrates',\n",
       " 3386: 'unintentionally',\n",
       " 14365: 'drafted',\n",
       " 52307: 'climby',\n",
       " 52308: '303',\n",
       " 18523: 'oldies',\n",
       " 9099: 'climbs',\n",
       " 9658: 'honour',\n",
       " 34755: 'plucking',\n",
       " 30077: '305',\n",
       " 5517: 'address',\n",
       " 40947: 'menjou',\n",
       " 42595: \"'freak'\",\n",
       " 19511: 'dwindling',\n",
       " 9461: 'benson',\n",
       " 52310: 'white’s',\n",
       " 40948: 'shamelessness',\n",
       " 21928: 'impacted',\n",
       " 52311: 'upatz',\n",
       " 3843: 'cusack',\n",
       " 37570: \"flavia's\",\n",
       " 52312: 'effette',\n",
       " 34756: 'influx',\n",
       " 52313: 'boooooooo',\n",
       " 52314: 'dimitrova',\n",
       " 13426: 'houseman',\n",
       " 25262: 'bigas',\n",
       " 52315: 'boylen',\n",
       " 52316: 'phillipenes',\n",
       " 40949: 'fakery',\n",
       " 27661: \"grandpa's\",\n",
       " 27662: 'darnell',\n",
       " 19512: 'undergone',\n",
       " 52318: 'handbags',\n",
       " 21929: 'perished',\n",
       " 37781: 'pooped',\n",
       " 27663: 'vigour',\n",
       " 3630: 'opposed',\n",
       " 52319: 'etude',\n",
       " 11802: \"caine's\",\n",
       " 52320: 'doozers',\n",
       " 34757: 'photojournals',\n",
       " 52321: 'perishes',\n",
       " 34758: 'constrains',\n",
       " 40951: 'migenes',\n",
       " 30608: 'consoled',\n",
       " 16830: 'alastair',\n",
       " 52322: 'wvs',\n",
       " 52323: 'ooooooh',\n",
       " 34759: 'approving',\n",
       " 40952: 'consoles',\n",
       " 52067: 'disparagement',\n",
       " 52325: 'futureistic',\n",
       " 52326: 'rebounding',\n",
       " 52327: \"'date\",\n",
       " 52328: 'gregoire',\n",
       " 21930: 'rutherford',\n",
       " 34760: 'americanised',\n",
       " 82199: 'novikov',\n",
       " 1045: 'following',\n",
       " 34761: 'munroe',\n",
       " 52329: \"morita'\",\n",
       " 52330: 'christenssen',\n",
       " 23109: 'oatmeal',\n",
       " 25263: 'fossey',\n",
       " 40953: 'livered',\n",
       " 13003: 'listens',\n",
       " 76167: \"'marci\",\n",
       " 52333: \"otis's\",\n",
       " 23390: 'thanking',\n",
       " 16022: 'maude',\n",
       " 34762: 'extensions',\n",
       " 52335: 'ameteurish',\n",
       " 52336: \"commender's\",\n",
       " 27664: 'agricultural',\n",
       " 4521: 'convincingly',\n",
       " 17642: 'fueled',\n",
       " 54017: 'mahattan',\n",
       " 40955: \"paris's\",\n",
       " 52339: 'vulkan',\n",
       " 52340: 'stapes',\n",
       " 52341: 'odysessy',\n",
       " 12262: 'harmon',\n",
       " 4255: 'surfing',\n",
       " 23497: 'halloran',\n",
       " 49583: 'unbelieveably',\n",
       " 52342: \"'offed'\",\n",
       " 30610: 'quadrant',\n",
       " 19513: 'inhabiting',\n",
       " 34763: 'nebbish',\n",
       " 40956: 'forebears',\n",
       " 34764: 'skirmish',\n",
       " 52343: 'ocassionally',\n",
       " 52344: \"'resist\",\n",
       " 21931: 'impactful',\n",
       " 52345: 'spicier',\n",
       " 40957: 'touristy',\n",
       " 52346: \"'football'\",\n",
       " 40958: 'webpage',\n",
       " 52348: 'exurbia',\n",
       " 52349: 'jucier',\n",
       " 14904: 'professors',\n",
       " 34765: 'structuring',\n",
       " 30611: 'jig',\n",
       " 40959: 'overlord',\n",
       " 25264: 'disconnect',\n",
       " 82204: 'sniffle',\n",
       " 40960: 'slimeball',\n",
       " 40961: 'jia',\n",
       " 16831: 'milked',\n",
       " 40962: 'banjoes',\n",
       " 1240: 'jim',\n",
       " 52351: 'workforces',\n",
       " 52352: 'jip',\n",
       " 52353: 'rotweiller',\n",
       " 34766: 'mundaneness',\n",
       " 52354: \"'ninja'\",\n",
       " 11043: \"dead'\",\n",
       " 40963: \"cipriani's\",\n",
       " 20611: 'modestly',\n",
       " 52355: \"professor'\",\n",
       " 40964: 'shacked',\n",
       " 34767: 'bashful',\n",
       " 23391: 'sorter',\n",
       " 16123: 'overpowering',\n",
       " 18524: 'workmanlike',\n",
       " 27665: 'henpecked',\n",
       " 18525: 'sorted',\n",
       " 52357: \"jōb's\",\n",
       " 52358: \"'always\",\n",
       " 34768: \"'baptists\",\n",
       " 52359: 'dreamcatchers',\n",
       " 52360: \"'silence'\",\n",
       " 21932: 'hickory',\n",
       " 52361: 'fun\\x97yet',\n",
       " 52362: 'breakumentary',\n",
       " 15499: 'didn',\n",
       " 52363: 'didi',\n",
       " 52364: 'pealing',\n",
       " 40965: 'dispite',\n",
       " 25265: \"italy's\",\n",
       " 21933: 'instability',\n",
       " 6542: 'quarter',\n",
       " 12611: 'quartet',\n",
       " 52365: 'padmé',\n",
       " 52366: \"'bleedmedry\",\n",
       " 52367: 'pahalniuk',\n",
       " 52368: 'honduras',\n",
       " 10789: 'bursting',\n",
       " 41468: \"pablo's\",\n",
       " 52370: 'irremediably',\n",
       " 40966: 'presages',\n",
       " 57835: 'bowlegged',\n",
       " 65186: 'dalip',\n",
       " 6263: 'entering',\n",
       " 76175: 'newsradio',\n",
       " 54153: 'presaged',\n",
       " 27666: \"giallo's\",\n",
       " 40967: 'bouyant',\n",
       " 52371: 'amerterish',\n",
       " 18526: 'rajni',\n",
       " 30613: 'leeves',\n",
       " 34770: 'macauley',\n",
       " 615: 'seriously',\n",
       " 52372: 'sugercoma',\n",
       " 52373: 'grimstead',\n",
       " 52374: \"'fairy'\",\n",
       " 30614: 'zenda',\n",
       " 52375: \"'twins'\",\n",
       " 17643: 'realisation',\n",
       " 27667: 'highsmith',\n",
       " 7820: 'raunchy',\n",
       " 40968: 'incentives',\n",
       " 52377: 'flatson',\n",
       " 35100: 'snooker',\n",
       " 16832: 'crazies',\n",
       " 14905: 'crazier',\n",
       " 7097: 'grandma',\n",
       " 52378: 'napunsaktha',\n",
       " 30615: 'workmanship',\n",
       " 52379: 'reisner',\n",
       " 61309: \"sanford's\",\n",
       " 52380: '\\x91doña',\n",
       " 6111: 'modest',\n",
       " 19156: \"everything's\",\n",
       " 40969: 'hamer',\n",
       " 52382: \"couldn't'\",\n",
       " 13004: 'quibble',\n",
       " 52383: 'socking',\n",
       " 21934: 'tingler',\n",
       " 52384: 'gutman',\n",
       " 40970: 'lachlan',\n",
       " 52385: 'tableaus',\n",
       " 52386: 'headbanger',\n",
       " 2850: 'spoken',\n",
       " 34771: 'cerebrally',\n",
       " 23493: \"'road\",\n",
       " 21935: 'tableaux',\n",
       " 40971: \"proust's\",\n",
       " 40972: 'periodical',\n",
       " 52388: \"shoveller's\",\n",
       " 25266: 'tamara',\n",
       " 17644: 'affords',\n",
       " 3252: 'concert',\n",
       " 87958: \"yara's\",\n",
       " 52389: 'someome',\n",
       " 8427: 'lingering',\n",
       " 41514: \"abraham's\",\n",
       " 34772: 'beesley',\n",
       " 34773: 'cherbourg',\n",
       " 28627: 'kagan',\n",
       " 9100: 'snatch',\n",
       " 9263: \"miyazaki's\",\n",
       " 25267: 'absorbs',\n",
       " 40973: \"koltai's\",\n",
       " 64030: 'tingled',\n",
       " 19514: 'crossroads',\n",
       " 16124: 'rehab',\n",
       " 52392: 'falworth',\n",
       " 52393: 'sequals',\n",
       " ...}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_word = [word for word, id_ in word_index.items()]\n",
    "len(imdb_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_, token in enumerate([\"<pad>\", \"<sos>\", \"<unk>\"]):\n",
    "    id_to_word[id_] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在真实的项目中，必须要自己预处理文本。\n",
    "\n",
    "- 使用前面用过的`Tokenizer`\n",
    "    - 设置`char_level=False`(默认的).\n",
    "    - 当编码单词时，`Tokenizer`会过滤掉许多字符, 使用`filters`进行过滤.\n",
    "    - `Tokenizer`使用空格确定单词的边界, 对于非英语语言并不通用.\n",
    "- *Taku Kudo*提出的一种无监督学习技术，用一种独立于语言的方式在**子单词级别**对文本进行`分词tokenize`和`组词detokenize`。这使得模型可以推断出没有见识过的单词.\n",
    "                  smart √  -est √      →   smartest\n",
    "- *Rico Sennrich*创建子字编码的其他方法（例如，使用字节对编码）\n",
    "- `TF.Text 库`，该库实现了各种分词策略，包括 `Word Piece`（字节对编码的一种变体）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 从`tfds上**下载`IMDB`数据集**\n",
    "\n",
    "    详情: https://tensorflow.google.cn/datasets/catalog/imdb_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'unsupervised'])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "\n",
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = info.splits[\"train\"].num_examples\n",
    "test_size = info.splits[\"test\"].num_examples\n",
    "\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **可视化评论**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评论: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
      "情感标签: 0 = Nagative\n",
      "\n",
      "评论: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
      "情感标签: 0 = Nagative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print(\"评论:\", review.decode('utf-8')[:200], \"...\")\n",
    "        print(\"情感标签:\", label, \"= Position\" if label else \"= Nagative\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **预处理函数**\n",
    "    - **截断评论**: 每条评论只保留前300个字符: 加快训练速度, 对性能影响小.\n",
    "    - **替换标识**:\n",
    "        - 使用空格替换`<br />`\n",
    "            ```python\n",
    "            tf.strings.regex_replace(X_batch, b\"<br\\s*/?>\", b\" \")\n",
    "            ```\n",
    "        - 使用空格替换除了字母和引号外其他所有字符\n",
    "            ```python\n",
    "            tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "            ```\n",
    "    - **分词**: 按照**空格**进行分割, 返回一个`不规则ragged张量`\n",
    "    - **填充**: 将这个张量转换为`密集dense张量`, 使用**填充token**`<pad>` 填充所有评论, 使它们具有相同的长度 \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    # 截断评论\n",
    "    X_batch = tf.strings.substr(input=X_batch, pos=0, len=300)\n",
    "    # 替换标识\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    # 分词\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    # 填充  [1,2,3],[],[4,5] -> [1,2,3],[0,0,0],[4,5,0]\n",
    "    X_batch = X_batch.to_tensor(default_value=b\"<pad>\")\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 53), dtype=string, numpy=\n",
       " array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n",
       "         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
       "         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n",
       "         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n",
       "         b'their', b'worst', b'role', b'in', b'history', b'Even',\n",
       "         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
       "         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n",
       "         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
       "         b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>'],\n",
       "        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
       "         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n",
       "         b'to', b'a', b'combination', b'of', b'things', b'including',\n",
       "         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n",
       "         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
       "         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n",
       "         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
       "         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n",
       "         b'Cons']], dtype=object)>,\n",
       " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **构建词汇表**\n",
    "\n",
    "    遍历整个训练集, 应用`preprocess()`，并使用 `Counter` 来计算每个单词的出现次数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "\n",
    "for X_batch, y_batch in train_set:\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(review.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    查看最常见的三个词:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53893"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **截断词汇表**的前1 0000个单词, 以获取更好的性能."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocabulary_size]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<pad>', b'the', b'a']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncated_vocabulary[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **建立**词汇表的**索引**\n",
    "    \n",
    "    详见`第13章 使用TensorFlow加载和预处理数据(1) 3.2使用独热向量编码分类特征`\n",
    "    \n",
    "    >- 首先定义词汇表,这是所有可能类别的列表.然后创建相应索引的张量.\n",
    "    >- 为查找表创建一个初始化程序,将类别列表及其索引传递给它.\n",
    "    >- 创建查找表,并为其提供初始化程序并指定了词汇表外`out-of-vocabulary (oov)桶`的数量.如果我们査找词汇表中不存在的类别，则査找表将计算该类别的哈希并将这个未知类別分配给`oov桶`之中的一个。它们的素引从已知类别开始，因此在此示例中，`oov桶`的索引为10001到11000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "\n",
    "vocabulary_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "\n",
    "num_oov_buckets = 10000\n",
    "table = tf.lookup.StaticVocabularyTable(vocabulary_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 18053]])>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建并训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，可以创建最终的训练集。\n",
    "1. 首先对影评做批处理, 使用`preprocess()`将其转换为单词的短序列.\n",
    "2. 然后使用一个简单的`encode_words()`对这些单词进行编码, 最后预提取下一个批次."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  22   11   28 ...    0    0    0]\n",
      " [   6   21   70 ...    0    0    0]\n",
      " [4099 6881    1 ...    0    0    0]\n",
      " ...\n",
      " [  22   12  118 ...  331 1047    0]\n",
      " [1757 4101  451 ...    0    0    0]\n",
      " [3365 4392    6 ...    0    0    0]]\n",
      "\n",
      "[0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in train_set.take(1):\n",
    "    print(X_batch.numpy())\n",
    "    print()\n",
    "    print(y_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 构建模型\n",
    "    \n",
    "    使用`Keras`提供的`Embedding`层实现嵌入.\n",
    "    \n",
    "    将所有单词ID转换为`嵌入`.嵌入矩阵需要每个单词ID（`vocab_size` + `num_oov_buckets`）, 每个嵌入维度一列. 模型输入是2D张量, 形状为 `[批次大小, 时间步]` , 嵌入层的输出是一个3D张量, 形状为 `[批次大小, 时间步, 嵌入大小]` .输出层使用`sigmoid`来输出估计概率的单个神经元, 该概率反应了评论表达与电影相关的正面情绪.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialization(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128  # 嵌入维度\n",
    "row = vocabulary_size + num_oov_buckets  # 53893+10000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=row,\n",
    "                           output_dim=embed_size,\n",
    "                           mask_zero=True,\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),  # 仅返回最后一个时间步长的输出\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, None, 128)         99072     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,758,273\n",
      "Trainable params: 2,758,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-e1fc66270b61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     metrics=[\"accuracy\"])\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    763\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 764\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    980\u001b[0m                     \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                     \u001b[0moptional_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautograph_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m                     \u001b[0muser_requested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m                 ))\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m    847\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1283\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1284\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1285\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2831\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2832\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2833\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2835\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3606\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3607\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3608\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3610\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;31m# Run forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m       \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m       loss = self.compiled_loss(\n\u001b[1;32m    797\u001b[0m           y, y_pred, sample_weight, regularization_losses=self.losses)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    378\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \"\"\"\n\u001b[1;32m    420\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 421\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m       last_output, outputs, runtime, states = self._defun_gru_call(\n\u001b[0;32m--> 458\u001b[0;31m           inputs, initial_state, training, mask, row_lengths)\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36m_defun_gru_call\u001b[0;34m(self, inputs, initial_state, training, mask, sequence_lengths)\u001b[0m\n\u001b[1;32m    533\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         last_output, outputs, new_h, runtime = gru_with_backend_selection(\n\u001b[0;32m--> 535\u001b[0;31m             **normal_gru_kwargs)\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_h\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mgru_with_backend_selection\u001b[0;34m(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# grappler will kick in during session execution to optimize the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m     \u001b[0mlast_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefun_standard_gru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m     \u001b[0m_function_register\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_gpu_gru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlast_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36m_function_register\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1794\u001b[0m   \u001b[0mconcrete_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m   \u001b[0mconcrete_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1796\u001b[0;31m   \u001b[0mconcrete_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_gradient_functions_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1797\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36madd_gradient_functions_to_graph\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m   2097\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m     forward_function, backward_function = (\n\u001b[0;32m-> 2099\u001b[0;31m         self._delayed_rewrite_functions.forward_backward())\n\u001b[0m\u001b[1;32m   2100\u001b[0m     \u001b[0mforward_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[0mbackward_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_backward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m     \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_construct_forward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    739\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m           \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m           func_graph=backwards_graph)\n\u001b[0m\u001b[1;32m    742\u001b[0m       \u001b[0mbackwards_graph_captures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackwards_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternal_captures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m       captures_from_forward = [\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backprop_function\u001b[0;34m(*grad_ys)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mgrad_ys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m             src_graph=self._func_graph)\n\u001b[0m\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 682\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    683\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0;31m# If the gradients are supposed to be compiled separately, we give them a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 682\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    683\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py\u001b[0m in \u001b[0;36m_IfGrad\u001b[0;34m(op, *grads)\u001b[0m\n\u001b[1;32m    125\u001b[0m       true_graph, grads, util.unique_grad_fn_name(true_graph.name))\n\u001b[1;32m    126\u001b[0m   false_grad_graph = _create_grad_func(\n\u001b[0;32m--> 127\u001b[0;31m       false_graph, grads, util.unique_grad_fn_name(false_graph.name))\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0;31m# Replaces output None grads with zeros if at least one branch has non-None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py\u001b[0m in \u001b[0;36m_create_grad_func\u001b[0;34m(func_graph, grads, name)\u001b[0m\n\u001b[1;32m    394\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m       \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_grad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m       func_graph=_CondGradFuncGraph(name, func_graph))\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    393\u001b[0m   return func_graph_module.func_graph_from_py_func(\n\u001b[1;32m    394\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m       \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_grad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m       func_graph=_CondGradFuncGraph(name, func_graph))\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py\u001b[0m in \u001b[0;36m_grad_fn\u001b[0;34m(func_graph, grads)\u001b[0m\n\u001b[1;32m    384\u001b[0m   result = gradients_util._GradientsHelper(\n\u001b[1;32m    385\u001b[0m       \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m       src_graph=func_graph)\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 682\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    683\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0;31m# If the gradients are supposed to be compiled separately, we give them a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 682\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    683\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py\u001b[0m in \u001b[0;36m_WhileGrad\u001b[0;34m(op, *grads)\u001b[0m\n\u001b[1;32m    354\u001b[0m   body_grad_graph, args = _create_grad_func(\n\u001b[1;32m    355\u001b[0m       \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_none_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       util.unique_grad_fn_name(body_graph.name), op, maximum_iterations)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbody_grad_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhile_op_needs_rewrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py\u001b[0m in \u001b[0;36m_create_grad_func\u001b[0;34m(ys, xs, grads, cond_graph, body_graph, name, while_op, maximum_iterations)\u001b[0m\n\u001b[1;32m    651\u001b[0m       func_graph=_WhileBodyGradFuncGraph(name, cond_graph, body_graph,\n\u001b[1;32m    652\u001b[0m                                          \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhile_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m                                          body_graph_inputs, body_graph_outputs))\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m   \u001b[0;31m# Update the list of outputs with tensors corresponding to the captured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         if x is not None)\n\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m     \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m       \u001b[0;31m# Ignore switches (they're handled separately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Switch\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m       \u001b[0;31m# Make merges trigger all other computation which must run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2451\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m     \u001b[0;34m\"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2453\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationOpType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",  # 二元交叉熵\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 掩码遮蔽 Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该模型需要学习忽略掉`填充token`: 可在创建`Embedding层`添加`mask_zero=True`, 使得`填充token`被遮蔽, 因为其ID为0(频率最高).\n",
    "\n",
    "> 其中的原理，是嵌入层创建了一个等于`K.not_equal(inputs, 0)`的`掩码张量`：这是一个布尔张量，形状和输入相同，**只要词ID有0，它就等于False，否则为True。只要时间维度保留着, 模型将自动将这个`掩码张量`向前传递给所有层。**\n",
    ">\n",
    ">    - 在本示例中，尽管两个`GRU`都接收到了`掩码张量`，但第二个`GRU`层不返回序列, 只返回最后一个时间步，`掩码张量`不会传递到`Dense`层。\n",
    ">\n",
    "> 每个层处理掩码的方式不同，但通常只会忽略被遮掩的时间步长（即掩码为False的时间步长）。例如，当循环层碰到被掩蔽的时间步长时，就只是从前一时间步长复制输出而已。 如果`掩码`一直传递到输出（输出为序列的模型），则它也会作用到损失上，所以被掩码的时间步长不会贡献到损失上（它们的损失为0）.\n",
    ">\n",
    " `LSTM`和`GRU`层有基于`cuDNN`库的GUP优化的实现, 但是不支持掩码. 优化的实现还需要使用多个超参数默认值 具体详见`1.4`节\n",
    "\n",
    " 所有接收掩码的层必须支持掩码（否则会抛出异常）。包括所有的循环层、`TimeDistributed`层和其它层。所有支持掩码的层必须`supports_masking=True`。\n",
    "\n",
    " - 如果想实现自定义的支持掩码的层，应该给`call()`方法添加`mask`参数。另外，要在构造器中设定`self.supports_masking = True`。\n",
    " - 如果第一个层不是`Embedding层`，可以使用`keras.layers.Masking层`: 它设掩码为`K.any(K.not_equal(inputs, 0), axis=-1)`，意思是最后一维都是0的时间步，会被后续层遮挡。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **使用函数式API构建模型手动处理掩码**\n",
    "\n",
    " 使用掩码层和自动掩码传播最适合简单的`Sequential`模型,不适合复杂的模型,这种情况需要使用函数式API或子类API来显性的计算掩码并传递到适当的层."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialization(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "embed_size = 128  # 嵌入维度\n",
    "row = vocabulary_size + num_oov_buckets  # 53893+10000\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=[None]) \n",
    "mask = keras.layers.Lambda(lambda inputs:K.not_equal(inputs, 0))(inputs)\n",
    "Z = keras.layers.Embedding(input_dim=row, output_dim=embed_size)(inputs)\n",
    "Z = keras.layers.GRU(128, return_sequences=True)(Z, mask=mask)\n",
    "Z = keras.layers.GRU(128)(Z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(Z)\n",
    "model = keras.models.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 128)    2560000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, None)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (None, None, 128)    99072       embedding[0][0]                  \n",
      "                                                                 lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 128)          99072       gru[0][0]                        \n",
      "                                                                 lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            129         gru_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 2,758,273\n",
      "Trainable params: 2,758,273\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 60s 70ms/step - loss: 0.5305 - accuracy: 0.7276\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.3070 - accuracy: 0.8772\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.1514 - accuracy: 0.9472\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.0937 - accuracy: 0.9666\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.0543 - accuracy: 0.9815\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",  # 二元交叉熵\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/my_Masking_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化嵌入向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.首先, 我们将检索在训练期间学习的单词嵌入向量:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 128)\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"./models/my_Masking_model.h5\")\n",
    "\n",
    "e = model.layers[1]\n",
    "weight = e.get_weights()[0]\n",
    "print(weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将权重写入磁盘。要使用 `Embedding Projector`，我们将以制表符分隔的格式上传两个文件：\n",
    "    - 向量文件（包含嵌入向量）\n",
    "    - 一个元数据文件（包含单词）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('./Embedding/vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('./Embedding/meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(truncated_vocabulary):\n",
    "    word = word.decode(\"utf-8\")\n",
    "    vec = weight[num+1]   # 跳过<pad>填充\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_m.close()\n",
    "out_v.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 为了可视化嵌入向量，我们将它们上传到`Embedding Projector`: http://projector.tensorflow.org/\n",
    "    - 打开`Embedding Projector`\n",
    "    - 点击`Load data`\n",
    "    - 上传我们在上面创建的两个文件：`vecs.tsv`和`meta.tsv`\n",
    "    \n",
    " <img src=\"./images/other/15-24.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins import projector\n",
    "\n",
    "# 设置日志目录，以便 Tensorboard 知道在哪里查找文件。\n",
    "log_dir='./Logs/SentimentAnalysis_logs/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "# 将我们要分析的权重保存为变量。\n",
    "# 请注意，第一个 value 代表任何未知单词，它不在元数据中，这里我们将删除这个值。\n",
    "\n",
    "weights = tf.Variable(model.layers[1].get_weights()[0][1:])\n",
    "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "checkpoint.save(os.path.join(run_dir, \"embedding.ckpt\"))\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "# 张量的名称将以`/.ATTRIBUTES/VARIABLE _ VALUE`作为后缀。\n",
    "embedding.tensor_name = \"./Embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = './Embedding/meta.tsv'\n",
    "projector.visualize_embeddings(run_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6063 (pid 78671), started 0:01:02 ago. (Use '!kill 78671' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-37404501acefb58b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-37404501acefb58b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6063;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir='./Logs/SentimentAnalysis_logs/' --port=6061"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重用预训练的嵌入 Reusing Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用`TensorFlow Hub`项目, 在情感分析模型中使用`tf2-preview/nnlm-en-dim50`句子嵌入模块.\n",
    "\n",
    "项目地址: https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\n",
    "\n",
    "本模块是一个`句子编码器`：它把字符串作为输入，并把每个字符串编码为一个独立向量（这个例子中是50维度的矢量）。在内部，它将字符串解析（空格分隔），然后使用在大型语料库`Google News 7B`(该语料库一共有70亿个词)上预训练的的嵌入矩阵来嵌入每个词。然后计算所有词嵌入的平均值，结果是`句子嵌入`。我们接着可以添加两个简单的`Dense`层来创建一个出色的情感分析模型。\n",
    "\n",
    "默认，`hub.KerasLayer`是不可训练的，但创建时可以设定`trainable=True`，就可以针对自己的任务微调了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialization(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFHUB_CACHE_DIR = os.path.join(os.curdir, \"tfhub_cache\")\n",
    "# 将文件缓存目录添加到系统变量 避免每次系统清理之后再次下载\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # 将一维字符串张量中的一批句子作为输入\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                   dtype=tf.string,\n",
    "                   input_shape=[],\n",
    "                   output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5461 - accuracy: 0.7267\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.5130 - accuracy: 0.7493\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.5082 - accuracy: 0.7522\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5047 - accuracy: 0.7544\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5018 - accuracy: 0.7557\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets[\"train\"].batch(batch_size).prefetch(1)\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8994.080817,
   "end_time": "2022-04-11T19:47:47.130309",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-11T17:17:53.049492",
   "version": "2.3.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "目录",
   "title_sidebar": "目录",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "299.525px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
